{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 原理\n",
    "\n",
    "核心思路：基于语料库构建词的共现矩阵，然后基于共现矩阵和GloVe模型学习词向量。\n",
    "\n",
    "需要说明的是，GloVe并没有用到神经网络。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/2019-08-21_160302.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 模型思想\n",
    "\n",
    "假设我们已经得到了词向量，如果我们用词向量$v_i$、$v_j$、$v_k$，通过某种函数计算$ratio_{i,j,k}$，能够同样得到这样的规律的话，就意味着我们词向量与共现矩阵具有很好的一致性，也就说明我们的词向量中蕴含了共现矩阵中所蕴含的信息。\n",
    "\n",
    "设用词向量$v_i$、$v_j$、$v_k$计算$ratio_{i,j,k}$的函数为$g(v_i,v_j,v_k)$\n",
    "\n",
    "（我们先不去管具体的函数形式），那么应该满足：$\\dfrac{P_{i,k}}{P_{j,k}}=ratio_{i,j,k}=g(v_{i},v_{j},v_{k})$\n",
    "\n",
    "即：\n",
    "$\\dfrac{P_{i,k}}{P_{j,k}}=g(v_{i},v_{j},v_{k})$\n",
    "\n",
    "即二者应该尽可能地接近；\n",
    "很容易想到用二者的差方来作为代价函数：\n",
    "$J=\\sum_{i,j,k}^N(\\dfrac{P_{i,k}}{P_{j,k}}-g(v_{i},v_{j},v_{k}))^2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "但是仔细一看，模型中包含3个单词，这就意味着要在 N ∗ N ∗ N 的复杂度上进行计算，太复杂了，最好能再简单点。 \n",
    "现在我们来仔细思考$g(v_i,v_j,v_k)$，或许它能帮上忙。\n",
    "\n",
    "思路是这样的："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1、要考虑单词i和单词j之间的关系，那$g(v_i,v_j,v_k)$中大概要有这么一项吧：$v_i−v_j$；还算合理，在线性空间中考察两个向量的相似性，不失线性地考察，那么$v_i−v_j$大概是个合理的选择；"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2、$ratio_{i,j,k}$是个标量，那么$g(v_i,v_j,v_k)$最后应该是个标量啊，虽然其输入都是向量，那內积应该是合理的选择，于是应该有这么一项吧：$(v_{i}-v_{j})^Tv_{k}$。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3、然后作者又往$(v_{i}-v_{j})^Tv_{k}$的外面套了一层指数运算exp()，得到最终的$g(v_i,v_j,v_k)= exp((v_{i}-v_{j})^Tv_{k})$；最关键的第3步，为什么套了一层exp()？ 别急往下看。\n",
    "\n",
    "套上之后，我们的目标是让以下公式尽可能地成立：\n",
    "\n",
    "$\\dfrac{P_{i,k}}{P_{j,k}}=g(v_{i},v_{j},v_{k})$\n",
    "\n",
    "$=exp((v_{i}-v_{j})^Tv_{k})$\n",
    "\n",
    "$=exp(v_{i}^Tv_{k}-v_{j}^Tv_{k})$\n",
    "\n",
    "$=\\dfrac{exp(v_{i}^Tv_{k})}{exp(v_{j}^Tv_{k})}$\n",
    "\n",
    "然后就发现找到简化方法了：只需要让上式分子对应相等，分母对应相等，即： \n",
    "\n",
    "${P_{i,k}}={exp(v_{i}^Tv_{k})}并且{P_{j,k}}={exp(v_{j}^Tv_{k})}$\n",
    "\n",
    "然而分子分母形式相同，就可以把两者统一考虑了，即： \n",
    "\n",
    "${P_{i,j}}={exp(v_{i}^Tv_{j})}$\n",
    "\n",
    "原本我们的目标是：\n",
    "$\\dfrac{P_{i,k}}{P_{j,k}}=g(v_{i},v_{j},v_{k})$\n",
    "\n",
    "现在我们的目标是：${P_{i,j}}={exp(v_{i}^Tv_{j})}$\n",
    "\n",
    "两边取个对数： $log(P_{i,j})=v_{i}^Tv_{j}$\n",
    "\n",
    "那么代价函数就可以简化为： \n",
    "$J=\\sum_{i,j}^N(log(P_{i,j})-v_{i}^Tv_{j})^2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "现在只需要在 N ∗ N 的复杂度上进行计算，而不是N ∗ N ∗ N，现在关于为什么第3步中，外面套一层exp()就清楚了，正是因为套了一层exp()，才使得差形式变成商形式，进而等式两边分子分母对应相等，进而简化模型。\n",
    "然而，出了点问题。仔细看这两个式子： \n",
    "\n",
    "$log(P_{i,j})=v_{i}^Tv_{j}和log(P_{j,i})=v_{j}^Tv_{i}$\n",
    "\n",
    "$log(P_{i,j})$不等于$log(P_{j,i})$但是$v_{i}^Tv_{j}$等于$v_{j}^Tv_{i}$；即等式左侧不具有对称性，但是右侧具有对称性。 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "怎么办呢？？？ 首先将代价函数中的条件概率展开： $log(P_{i,j})=v_{i}^Tv_{j}$\n",
    "\n",
    "$log(X_{i,j})-log(X_{i})=v_{i}^Tv_{j}$\n",
    "\n",
    "即：$log(X_{i,j})=v_{i}^Tv_{j}+b_{i}+b_{j}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "即添了一个偏差项$b_j$，并将$log(X_i)$吸收到偏差项bi中。 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "于是代价函数就变成了： \n",
    "\n",
    "$J=\\sum_{i,j}^N(v_{i}^Tv_{j}+b_{i}+b_{j}-log(X_{i,j}))^2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "然后作者认为这样的处理存在一个弊端，即对于一个词，他的每一个共现词都享有相同的权重来决定该词的词向量，而这在常理上的理解是不合理的，因此，作者引入了一种带权的最小二乘法来解决这种问题，基于出现频率越高的词对权重应该越大的原则，在代价函数中添加权重项，于是代价函数进一步完善： \n",
    "\n",
    "$J=\\sum_{i,j}^Nf(X_{i,j})(v_{i}^Tv_{j}+b_{i}+b_{j}-log(X_{i,j}))^2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "具体权重函数应该是怎么样的呢？\n",
    "首先应该是非减的，其次当词频过高时，权重不应过分增大，作者通过实验确定权重函数为： "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/11525720-bc23fdaf16111b6b.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    作者经过实验得出，α取值为0.75能得到最好的模型效果。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### python实现GloVe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import codecs\n",
    "from collections import Counter\n",
    "from functools import partial\n",
    "from math import log\n",
    "import os.path\n",
    "import pickle\n",
    "from random import shuffle\n",
    "\n",
    "import numpy as np\n",
    "from scipy import sparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 第一步：创建词表\n",
    "def build_vocab(corpus):\n",
    "    \"\"\"\n",
    "    用 collectionos.Counter 创建词表\n",
    "    Build a vocabulary with word frequencies for an entire corpus.\n",
    "\n",
    "    Returns a dictionary `w -> (i, f)`, mapping word strings to pairs of\n",
    "    word ID and word corpus frequency.\n",
    "    \"\"\"\n",
    "    print(\"Building vocab from corpus\")\n",
    "\n",
    "    vocab = Counter()\n",
    "    for line in corpus:\n",
    "        tokens = line.strip().split()\n",
    "        vocab.update(tokens)\n",
    "\n",
    "    print(\"Done building vocab from corpus.\")\n",
    "\n",
    "    return {word: (i, freq) for i, (word, freq) in enumerate(vocab.items())}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 第二步：创建共现矩阵\n",
    "\n",
    "def build_cooccur(vocab, corpus, window_size=10, min_count=None):\n",
    "    \"\"\"\n",
    "    Build a word co-occurrence list for the given corpus.\n",
    "\n",
    "    This function is a tuple generator, where each element (representing\n",
    "    a cooccurrence pair) is of the form\n",
    "\n",
    "        (i_main, i_context, cooccurrence)\n",
    "\n",
    "    where `i_main` is the ID of the main word in the cooccurrence and\n",
    "    `i_context` is the ID of the context word, and `cooccurrence` is the\n",
    "    `X_{ij}` cooccurrence value as described in Pennington et al.\n",
    "    (2014).\n",
    "\n",
    "    If `min_count` is not `None`, cooccurrence pairs where either word\n",
    "    occurs in the corpus fewer than `min_count` times are ignored.\n",
    "    \"\"\"\n",
    "\n",
    "    vocab_size = len(vocab)\n",
    "    id2word = dict((i, word) for word, (i, _) in vocab.items())\n",
    "\n",
    "    # Collect cooccurrences internally as a sparse matrix for passable\n",
    "    # indexing speed; we'll convert into a list later\n",
    "    cooccurrences = sparse.lil_matrix((vocab_size, vocab_size),\n",
    "                                      dtype=np.float64)\n",
    "\n",
    "    for i, line in enumerate(corpus):\n",
    "        if i % 1000 == 0:\n",
    "            print(\"Building cooccurrence matrix: on line %i\", i)\n",
    "\n",
    "        tokens = line.strip().split()\n",
    "        token_ids = [vocab[word][0] for word in tokens]\n",
    "\n",
    "        for center_i, center_id in enumerate(token_ids):\n",
    "            # Collect all word IDs in left window of center word\n",
    "            context_ids = token_ids[max(0, center_i - window_size) : center_i]\n",
    "            contexts_len = len(context_ids)\n",
    "\n",
    "            for left_i, left_id in enumerate(context_ids):\n",
    "                # Distance from center word\n",
    "                distance = contexts_len - left_i\n",
    "\n",
    "                # Weight by inverse of distance between words\n",
    "                increment = 1.0 / float(distance)\n",
    "\n",
    "                # Build co-occurrence matrix symmetrically (pretend we\n",
    "                # are calculating right contexts as well)\n",
    "                cooccurrences[center_id, left_id] += increment\n",
    "                cooccurrences[left_id, center_id] += increment\n",
    "\n",
    "    # Now yield our tuple sequence (dig into the LiL-matrix internals to\n",
    "    # quickly iterate through all nonzero cells)\n",
    "    for i, (row, data) in enumerate(izip(cooccurrences.rows, cooccurrences.data)):\n",
    "        if min_count is not None and vocab[id2word[i]][1] < min_count:\n",
    "            continue\n",
    "\n",
    "        for data_idx, j in enumerate(row):\n",
    "            if min_count is not None and vocab[id2word[j]][1] < min_count:\n",
    "                continue\n",
    "\n",
    "            yield i, j, data[data_idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def save_model(W, path):\n",
    "    with open(path, 'wb') as vector_f:\n",
    "        pickle.dump(W, vector_f, protocol=2)\n",
    "\n",
    "    logger.info(\"Saved vectors to %s\", path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def run_iter(vocab, data, learning_rate=0.05, x_max=100, alpha=0.75):\n",
    "    \"\"\"\n",
    "    Run a single iteration of GloVe training using the given\n",
    "    cooccurrence data and the previously computed weight vectors /\n",
    "    biases and accompanying gradient histories.\n",
    "\n",
    "    `data` is a pre-fetched data / weights list where each element is of\n",
    "    the form\n",
    "\n",
    "        (v_main, v_context,\n",
    "         b_main, b_context,\n",
    "         gradsq_W_main, gradsq_W_context,\n",
    "         gradsq_b_main, gradsq_b_context,\n",
    "         cooccurrence)\n",
    "\n",
    "    as produced by the `train_glove` function. Each element in this\n",
    "    tuple is an `ndarray` view into the data structure which contains\n",
    "    it.\n",
    "\n",
    "    See the `train_glove` function for information on the shapes of `W`,\n",
    "    `biases`, `gradient_squared`, `gradient_squared_biases` and how they\n",
    "    should be initialized.\n",
    "\n",
    "    The parameters `x_max`, `alpha` define our weighting function when\n",
    "    computing the cost for two word pairs; see the GloVe paper for more\n",
    "    details.\n",
    "\n",
    "    Returns the cost associated with the given weight assignments and\n",
    "    updates the weights by online AdaGrad in place.\n",
    "    \"\"\"\n",
    "\n",
    "    global_cost = 0\n",
    "\n",
    "    # We want to iterate over data randomly so as not to unintentionally\n",
    "    # bias the word vector contents\n",
    "    shuffle(data)\n",
    "\n",
    "    for (v_main, v_context, b_main, b_context, gradsq_W_main, gradsq_W_context,\n",
    "         gradsq_b_main, gradsq_b_context, cooccurrence) in data:\n",
    "\n",
    "        weight = (cooccurrence / x_max) ** alpha if cooccurrence < x_max else 1\n",
    "\n",
    "        # Compute inner component of cost function, which is used in\n",
    "        # both overall cost calculation and in gradient calculation\n",
    "        #\n",
    "        #   $$ J' = w_i^Tw_j + b_i + b_j - log(X_{ij}) $$\n",
    "        cost_inner = (v_main.dot(v_context)\n",
    "                      + b_main[0] + b_context[0]\n",
    "                      - log(cooccurrence))\n",
    "\n",
    "        # Compute cost\n",
    "        #\n",
    "        #   $$ J = f(X_{ij}) (J')^2 $$\n",
    "        cost = weight * (cost_inner ** 2)\n",
    "\n",
    "        # Add weighted cost to the global cost tracker\n",
    "        global_cost += 0.5 * cost\n",
    "\n",
    "        # Compute gradients for word vector terms.\n",
    "        #\n",
    "        # NB: `main_word` is only a view into `W` (not a copy), so our\n",
    "        # modifications here will affect the global weight matrix;\n",
    "        # likewise for context_word, biases, etc.\n",
    "        grad_main = weight * cost_inner * v_context\n",
    "        grad_context = weight * cost_inner * v_main\n",
    "\n",
    "        # Compute gradients for bias terms\n",
    "        grad_bias_main = weight * cost_inner\n",
    "        grad_bias_context = weight * cost_inner\n",
    "\n",
    "        # Now perform adaptive updates\n",
    "        v_main -= (learning_rate * grad_main / np.sqrt(gradsq_W_main))\n",
    "        v_context -= (learning_rate * grad_context / np.sqrt(gradsq_W_context))\n",
    "\n",
    "        b_main -= (learning_rate * grad_bias_main / np.sqrt(gradsq_b_main))\n",
    "        b_context -= (learning_rate * grad_bias_context / np.sqrt(\n",
    "                gradsq_b_context))\n",
    "\n",
    "        # Update squared gradient sums\n",
    "        gradsq_W_main += np.square(grad_main)\n",
    "        gradsq_W_context += np.square(grad_context)\n",
    "        gradsq_b_main += grad_bias_main ** 2\n",
    "        gradsq_b_context += grad_bias_context ** 2\n",
    "\n",
    "    return global_cost\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_glove(vocab, cooccurrences, iter_callback=None, vector_size=100,\n",
    "                iterations=25, **kwargs):\n",
    "    \"\"\"\n",
    "    Train GloVe vectors on the given generator `cooccurrences`, where\n",
    "    each element is of the form\n",
    "\n",
    "        (word_i_id, word_j_id, x_ij)\n",
    "\n",
    "    where `x_ij` is a cooccurrence value $X_{ij}$ as presented in the\n",
    "    matrix defined by `build_cooccur` and the Pennington et al. (2014)\n",
    "    paper itself.\n",
    "\n",
    "    If `iter_callback` is not `None`, the provided function will be\n",
    "    called after each iteration with the learned `W` matrix so far.\n",
    "\n",
    "    Keyword arguments are passed on to the iteration step function\n",
    "    `run_iter`.\n",
    "\n",
    "    Returns the computed word vector matrix `W`.\n",
    "    \"\"\"\n",
    "\n",
    "    vocab_size = len(vocab)\n",
    "\n",
    "    # Word vector matrix. This matrix is (2V) * d, where N is the size\n",
    "    # of the corpus vocabulary and d is the dimensionality of the word\n",
    "    # vectors. All elements are initialized randomly in the range (-0.5,\n",
    "    # 0.5]. We build two word vectors for each word: one for the word as\n",
    "    # the main (center) word and one for the word as a context word.\n",
    "    #\n",
    "    # It is up to the client to decide what to do with the resulting two\n",
    "    # vectors. Pennington et al. (2014) suggest adding or averaging the\n",
    "    # two for each word, or discarding the context vectors.\n",
    "    W = (np.random.rand(vocab_size * 2, vector_size) - 0.5) / float(vector_size + 1)\n",
    "\n",
    "    # Bias terms, each associated with a single vector. An array of size\n",
    "    # $2V$, initialized randomly in the range (-0.5, 0.5].\n",
    "    biases = (np.random.rand(vocab_size * 2) - 0.5) / float(vector_size + 1)\n",
    "\n",
    "    # Training is done via adaptive gradient descent (AdaGrad). To make\n",
    "    # this work we need to store the sum of squares of all previous\n",
    "    # gradients.\n",
    "    #\n",
    "    # Like `W`, this matrix is (2V) * d.\n",
    "    #\n",
    "    # Initialize all squared gradient sums to 1 so that our initial\n",
    "    # adaptive learning rate is simply the global learning rate.\n",
    "    gradient_squared = np.ones((vocab_size * 2, vector_size),\n",
    "                               dtype=np.float64)\n",
    "\n",
    "    # Sum of squared gradients for the bias terms.\n",
    "    gradient_squared_biases = np.ones(vocab_size * 2, dtype=np.float64)\n",
    "\n",
    "    # Build a reusable list from the given cooccurrence generator,\n",
    "    # pre-fetching all necessary data.\n",
    "    #\n",
    "    # NB: These are all views into the actual data matrices, so updates\n",
    "    # to them will pass on to the real data structures\n",
    "    #\n",
    "    # (We even extract the single-element biases as slices so that we\n",
    "    # can use them as views)\n",
    "    data = [(W[i_main], W[i_context + vocab_size],\n",
    "             biases[i_main : i_main + 1],\n",
    "             biases[i_context + vocab_size : i_context + vocab_size + 1],\n",
    "             gradient_squared[i_main], gradient_squared[i_context + vocab_size],\n",
    "             gradient_squared_biases[i_main : i_main + 1],\n",
    "             gradient_squared_biases[i_context + vocab_size\n",
    "                                     : i_context + vocab_size + 1],\n",
    "             cooccurrence)\n",
    "            for i_main, i_context, cooccurrence in cooccurrences]\n",
    "\n",
    "    for i in range(iterations):\n",
    "        logger.info(\"\\tBeginning iteration %i..\", i)\n",
    "\n",
    "        cost = run_iter(vocab, data, **kwargs)\n",
    "\n",
    "        logger.info(\"\\t\\tDone (cost %f)\", cost)\n",
    "\n",
    "        if iter_callback is not None:\n",
    "            iter_callback(W)\n",
    "\n",
    "    return W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def main(arguments):\n",
    "    corpus = arguments.corpus\n",
    "\n",
    "    print(\"Fetching vocab..\")\n",
    "    vocab = get_or_build(arguments.vocab_path, build_vocab, corpus)\n",
    "    print(\"Vocab has %i elements.\\n\", len(vocab))\n",
    "\n",
    "    print(\"Fetching cooccurrence list..\")\n",
    "    corpus.seek(0)\n",
    "    cooccurrences = get_or_build(arguments.cooccur_path,\n",
    "                                 build_cooccur, vocab, corpus,\n",
    "                                 window_size=arguments.window_size,\n",
    "                                 min_count=arguments.min_count)\n",
    "    print(\"Cooccurrence list fetch complete (%i pairs).\\n\",\n",
    "                len(cooccurrences))\n",
    "\n",
    "    if arguments.save_often:\n",
    "        iter_callback = partial(save_model, path=arguments.vector_path)\n",
    "    else:\n",
    "        iter_callback = None\n",
    "\n",
    "    print(\"Beginning GloVe training..\")\n",
    "    W = train_glove(vocab, cooccurrences,\n",
    "                    iter_callback=iter_callback,\n",
    "                    vector_size=arguments.vector_size,\n",
    "                    iterations=arguments.iterations,\n",
    "                    learning_rate=arguments.learning_rate)\n",
    "\n",
    "    # TODO shave off bias values, do something with context vectors\n",
    "    save_model(W, arguments.vector_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_corpus = (\"\"\"human interface computer\n",
    "survey user computer system response time\n",
    "eps user interface system\n",
    "system human system eps\n",
    "user response time\n",
    "trees\n",
    "graph trees\n",
    "graph minors trees\n",
    "graph minors survey\n",
    "I like graph and stuff\n",
    "I like trees and stuff\n",
    "Sometimes I build a graph\n",
    "Sometimes I build trees\"\"\").split(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['human interface computer', 'survey user computer system response time', 'eps user interface system', 'system human system eps', 'user response time', 'trees', 'graph trees', 'graph minors trees', 'graph minors survey', 'I like graph and stuff', 'I like trees and stuff', 'Sometimes I build a graph', 'Sometimes I build trees']\n"
     ]
    }
   ],
   "source": [
    "print(test_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building vocab from corpus\n",
      "Done building vocab from corpus.\n"
     ]
    }
   ],
   "source": [
    "vocab = build_vocab(test_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'I': (12, 4),\n",
       " 'Sometimes': (16, 2),\n",
       " 'a': (18, 1),\n",
       " 'and': (14, 2),\n",
       " 'build': (17, 2),\n",
       " 'computer': (2, 2),\n",
       " 'eps': (8, 2),\n",
       " 'graph': (10, 5),\n",
       " 'human': (0, 2),\n",
       " 'interface': (1, 2),\n",
       " 'like': (13, 2),\n",
       " 'minors': (11, 2),\n",
       " 'response': (6, 2),\n",
       " 'stuff': (15, 2),\n",
       " 'survey': (3, 2),\n",
       " 'system': (5, 4),\n",
       " 'time': (7, 2),\n",
       " 'trees': (9, 5),\n",
       " 'user': (4, 3)}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "cooccur = build_cooccur(vocab, test_corpus, window_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'corpus' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-53-1fd19763941c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      7\u001b[0m                                   dtype=np.float64)\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mline\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;36m1000\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Building cooccurrence matrix: on line %i\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'corpus' is not defined"
     ]
    }
   ],
   "source": [
    "vocab_size = len(vocab)\n",
    "id2word = dict((i, word) for word, (i, _) in vocab.items())\n",
    "\n",
    "# Collect cooccurrences internally as a sparse matrix for passable\n",
    "# indexing speed; we'll convert into a list later\n",
    "cooccurrences = sparse.lil_matrix((vocab_size, vocab_size),\n",
    "                                  dtype=np.float64)\n",
    "\n",
    "for i, line in enumerate(test_corpus):\n",
    "    if i % 1000 == 0:\n",
    "        print(\"Building cooccurrence matrix: on line %i\", i)\n",
    "\n",
    "    tokens = line.strip().split()\n",
    "    token_ids = [vocab[word][0] for word in tokens]\n",
    "\n",
    "    for center_i, center_id in enumerate(token_ids):\n",
    "        # Collect all word IDs in left window of center word\n",
    "        context_ids = token_ids[max(0, center_i - window_size) : center_i]\n",
    "        contexts_len = len(context_ids)\n",
    "\n",
    "        for left_i, left_id in enumerate(context_ids):\n",
    "            # Distance from center word\n",
    "            distance = contexts_len - left_i\n",
    "\n",
    "            # Weight by inverse of distance between words\n",
    "            increment = 1.0 / float(distance)\n",
    "\n",
    "            # Build co-occurrence matrix symmetrically (pretend we\n",
    "            # are calculating right contexts as well)\n",
    "            cooccurrences[center_id, left_id] += increment\n",
    "            cooccurrences[left_id, center_id] += increment\n",
    "cooccurrences\n",
    "# for i, (row, data) in enumerate(izip(cooccurrences.rows, cooccurrences.data)):\n",
    "#     if min_count is not None and vocab[id2word[i]][1] < min_count:\n",
    "#         continue\n",
    "\n",
    "#     for data_idx, j in enumerate(row):\n",
    "#         if min_count is not None and vocab[id2word[j]][1] < min_count:\n",
    "#             continue\n",
    "\n",
    "#         yield i, j, data[data_idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
