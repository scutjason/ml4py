{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 词嵌入模型\n",
    "\n",
    "两种基本的模型，CBOW和Skip-Gram 模型。CBOW是给定上下文预测中心词，Skip-Gram是给定中心词预测上下文。\n",
    "\n",
    "\n",
    "#### Skip-Gram跳字模型\n",
    "\n",
    "假设文本序列是“the”“man”“loves”“his”“son”。以“loves”作为中心词，设背景窗口大小为2。如图10.1所示，跳字模型所关心的是，给定中心词“loves”，生成与它距离不超过2个词的背景词“the”“man”“his”“son”的条件概率，即\n",
    "\n",
    "![跳字模型关心给定中心词生成背景词的条件概率](./img/skip-gram.svg)\n",
    "\n",
    "\n",
    "在跳字模型中，每个词被表示成两个$d$维向量，用来计算条件概率。假设这个词在词典中索引为$i$，当它为中心词时向量表示为$\\boldsymbol{v}_i\\in\\mathbb{R}^d$，而为背景词时向量表示为$\\boldsymbol{u}_i\\in\\mathbb{R}^d$。设中心词$w_c$在词典中索引为$c$，背景词$w_o$在词典中索引为$o$，给定中心词生成背景词的条件概率可以通过对向量内积做softmax运算而得到：\n",
    "\n",
    "$$P(w_o \\mid w_c) = \\frac{\\text{exp}(\\boldsymbol{u}_o^\\top \\boldsymbol{v}_c)}{ \\sum_{i \\in \\mathcal{V}} \\text{exp}(\\boldsymbol{u}_i^\\top \\boldsymbol{v}_c)},$$\n",
    "\n",
    "其中词典索引集$\\mathcal{V} = \\{0, 1, \\ldots, |\\mathcal{V}|-1\\}$。假设给定一个长度为$T$的文本序列，设时间步$t$的词为$w^{(t)}$。假设给定中心词的情况下背景词的生成相互独立，当背景窗口大小为$m$时，跳字模型的似然函数即给定任一中心词生成所有背景词的概率\n",
    "\n",
    "$$ \\prod_{t=1}^{T} \\prod_{-m \\leq j \\leq m,\\ j \\neq 0} P(w^{(t+j)} \\mid w^{(t)}),$$\n",
    "\n",
    "这里小于1和大于$T$的时间步可以忽略。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 训练\n",
    "\n",
    "跳字模型的参数是每个词所对应的中心词向量和背景词向量。训练中我们通过最大化似然函数来学习模型参数，即最大似然估计。这等价于最小化以下损失函数：\n",
    "\n",
    "$$ - \\sum_{t=1}^{T} \\sum_{-m \\leq j \\leq m,\\ j \\neq 0} \\text{log}\\, P(w^{(t+j)} \\mid w^{(t)}).$$\n",
    "\n",
    "\n",
    "如果使用随机梯度下降，那么在每一次迭代里我们随机采样一个较短的子序列来计算有关该子序列的损失，然后计算梯度来更新模型参数。梯度计算的关键是条件概率的对数有关中心词向量和背景词向量的梯度。根据定义，首先看到\n",
    "\n",
    "\n",
    "$$\\log P(w_o \\mid w_c) =\n",
    "\\boldsymbol{u}_o^\\top \\boldsymbol{v}_c - \\log\\left(\\sum_{i \\in \\mathcal{V}} \\text{exp}(\\boldsymbol{u}_i^\\top \\boldsymbol{v}_c)\\right)$$\n",
    "\n",
    "通过微分，我们可以得到上式中$\\boldsymbol{v}_c$的梯度\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\frac{\\partial \\text{log}\\, P(w_o \\mid w_c)}{\\partial \\boldsymbol{v}_c} \n",
    "&= \\boldsymbol{u}_o - \\frac{\\sum_{j \\in \\mathcal{V}} \\exp(\\boldsymbol{u}_j^\\top \\boldsymbol{v}_c)\\boldsymbol{u}_j}{\\sum_{i \\in \\mathcal{V}} \\exp(\\boldsymbol{u}_i^\\top \\boldsymbol{v}_c)}\\\\\n",
    "&= \\boldsymbol{u}_o - \\sum_{j \\in \\mathcal{V}} \\left(\\frac{\\text{exp}(\\boldsymbol{u}_j^\\top \\boldsymbol{v}_c)}{ \\sum_{i \\in \\mathcal{V}} \\text{exp}(\\boldsymbol{u}_i^\\top \\boldsymbol{v}_c)}\\right) \\boldsymbol{u}_j\\\\ \n",
    "&= \\boldsymbol{u}_o - \\sum_{j \\in \\mathcal{V}} P(w_j \\mid w_c) \\boldsymbol{u}_j.\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "它的计算需要词典中所有词以$w_c$为中心词的条件概率。有关其他词向量的梯度同理可得。\n",
    "\n",
    "训练结束后，对于词典中的任一索引为$i$的词，我们均得到该词作为中心词和背景词的两组词向量$\\boldsymbol{v}_i$和$\\boldsymbol{u}_i$。在自然语言处理应用中，一般使用跳字模型的中心词向量作为词的表征向量。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 连续词袋模型CBOW\n",
    "\n",
    "连续词袋模型与跳字模型类似。与跳字模型最大的不同在于，连续词袋模型假设基于某中心词在文本序列前后的背景词来生成该中心词。在同样的文本序列“the”“man”“loves”“his”“son”里，以“loves”作为中心词，且背景窗口大小为2时，连续词袋模型关心的是，给定背景词“the”“man”“his”“son”生成中心词“loves”的条件概率（如图10.2所示），也就是\n",
    "\n",
    "$$P(\\textrm{``loves\"}\\mid\\textrm{``the\"},\\textrm{``man\"},\\textrm{``his\"},\\textrm{``son\"}).$$\n",
    "\n",
    "![连续词袋模型关心给定背景词生成中心词的条件概率](./img/cbow.svg)\n",
    "\n",
    "因为连续词袋模型的背景词有多个，我们将这些背景词向量取平均，然后使用和跳字模型一样的方法来计算条件概率。设$\\boldsymbol{v_i}\\in\\mathbb{R}^d$和$\\boldsymbol{u_i}\\in\\mathbb{R}^d$分别表示词典中索引为$i$的词作为背景词和中心词的向量（注意符号的含义与跳字模型中的相反）。设中心词$w_c$在词典中索引为$c$，背景词$w_{o_1}, \\ldots, w_{o_{2m}}$在词典中索引为$o_1, \\ldots, o_{2m}$，那么给定背景词生成中心词的条件概率\n",
    "\n",
    "$$P(w_c \\mid w_{o_1}, \\ldots, w_{o_{2m}}) = \\frac{\\text{exp}\\left(\\frac{1}{2m}\\boldsymbol{u}_c^\\top (\\boldsymbol{v}_{o_1} + \\ldots + \\boldsymbol{v}_{o_{2m}}) \\right)}{ \\sum_{i \\in \\mathcal{V}} \\text{exp}\\left(\\frac{1}{2m}\\boldsymbol{u}_i^\\top (\\boldsymbol{v}_{o_1} + \\ldots + \\boldsymbol{v}_{o_{2m}}) \\right)}.$$\n",
    "\n",
    "为了让符号更加简单，我们记$\\mathcal{W}_o= \\{w_{o_1}, \\ldots, w_{o_{2m}}\\}$，且$\\bar{\\boldsymbol{v}}_o = \\left(\\boldsymbol{v}_{o_1} + \\ldots + \\boldsymbol{v}_{o_{2m}} \\right)/(2m)$，那么上式可以简写成\n",
    "\n",
    "$$P(w_c \\mid \\mathcal{W}_o) = \\frac{\\exp\\left(\\boldsymbol{u}_c^\\top \\bar{\\boldsymbol{v}}_o\\right)}{\\sum_{i \\in \\mathcal{V}} \\exp\\left(\\boldsymbol{u}_i^\\top \\bar{\\boldsymbol{v}}_o\\right)}.$$\n",
    "\n",
    "给定一个长度为$T$的文本序列，设时间步$t$的词为$w^{(t)}$，背景窗口大小为$m$。连续词袋模型的似然函数是由背景词生成任一中心词的概率\n",
    "\n",
    "$$ \\prod_{t=1}^{T}  P(w^{(t)} \\mid  w^{(t-m)}, \\ldots,  w^{(t-1)},  w^{(t+1)}, \\ldots,  w^{(t+m)}).$$\n",
    "\n",
    "##### 训练\n",
    "\n",
    "训练连续词袋模型同训练跳字模型基本一致。连续词袋模型的最大似然估计等价于最小化损失函数\n",
    "\n",
    "$$  -\\sum_{t=1}^T  \\text{log}\\, P(w^{(t)} \\mid  w^{(t-m)}, \\ldots,  w^{(t-1)},  w^{(t+1)}, \\ldots,  w^{(t+m)}).$$\n",
    "\n",
    "注意到\n",
    "\n",
    "$$\\log\\,P(w_c \\mid \\mathcal{W}_o) = \\boldsymbol{u}_c^\\top \\bar{\\boldsymbol{v}}_o - \\log\\,\\left(\\sum_{i \\in \\mathcal{V}} \\exp\\left(\\boldsymbol{u}_i^\\top \\bar{\\boldsymbol{v}}_o\\right)\\right).$$\n",
    "\n",
    "通过微分，我们可以计算出上式中条件概率的对数有关任一背景词向量$\\boldsymbol{v}_{o_i}$（$i = 1, \\ldots, 2m$）的梯度\n",
    "\n",
    "$$\\frac{\\partial \\log\\, P(w_c \\mid \\mathcal{W}_o)}{\\partial \\boldsymbol{v}_{o_i}} = \\frac{1}{2m} \\left(\\boldsymbol{u}_c - \\sum_{j \\in \\mathcal{V}} \\frac{\\exp(\\boldsymbol{u}_j^\\top \\bar{\\boldsymbol{v}}_o)\\boldsymbol{u}_j}{ \\sum_{i \\in \\mathcal{V}} \\text{exp}(\\boldsymbol{u}_i^\\top \\bar{\\boldsymbol{v}}_o)} \\right) = \\frac{1}{2m}\\left(\\boldsymbol{u}_c - \\sum_{j \\in \\mathcal{V}} P(w_j \\mid \\mathcal{W}_o) \\boldsymbol{u}_j \\right).$$\n",
    "\n",
    "有关其他词向量的梯度同理可得。同跳字模型不一样的一点在于，我们一般使用连续词袋模型的背景词向量作为词的表征向量。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2Vec的近似训练\n",
    "\n",
    "由于softmax运算考虑了背景词可能是词典$\\mathcal{V}$中的任一词，以上损失包含了词典大小数目的项的累加。在上一节中我们看到，不论是跳字模型还是连续词袋模型，由于条件概率使用了softmax运算，每一步的梯度计算都包含词典大小数目的项的累加。对于含几十万或上百万词的较大词典，每次的梯度计算开销可能过大。为了降低该计算复杂度，本节将介绍两种近似训练方法，即负采样（negative sampling）或层序softmax（hierarchical softmax）。由于跳字模型和连续词袋模型类似，本节仅以跳字模型为例介绍这两种方法。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "实际上我们的工作重点是怎么构造这两个概率P(w|context(w)) 和 P(context(w)|w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 层次化softmax\n",
    "\n",
    "在说层次化softmax之前，我们先看下霍夫曼编码："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 霍夫曼编码\n",
    "\n",
    "###### 霍夫曼树\n",
    "\n",
    "    带权值的最优二叉树。下面是构造过程：\n",
    "   <img src=\"img/2019-08-20_144147.png\">\n",
    "   \n",
    "叶子节点表示词。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 编码\n",
    "\n",
    "    简单来说，霍夫曼编码就是不等长编码，我们希望频率高的词编码长度小，频率低的词编码长度长。所以基于词频构建霍夫曼树即可。\n",
    "    \n",
    "   <img src=\"img/2019-08-20_144624.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 为什么层次化softmax能简化损失函数的log(p)的计算呢？\n",
    "\n",
    "\n",
    "    树的特点。复杂度O[log(V)], V表示词典大小。\n",
    "    \n",
    "    叶子结点表示词典中的每个词，非叶子结点就不是"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### CBOW\n",
    "<img src=\"img/2019-08-20_172742.png\">\n",
    "\n",
    "<img src=\"img/2019-08-20_172817.png\">\n",
    "\n",
    "##### Skip-Gram\n",
    "\n",
    "<img src=\"img/2019-08-20_172956.png\">\n",
    "\n",
    "\n",
    "上面的for循环是指，霍夫曼路径上的循环。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 为什么层次化softmax能简化训练\n",
    "\n",
    "<img src=\"img/2019-08-21_091943.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 负采样\n",
    "    我们先看看Hierarchical Softmax的的缺点。的确，使用霍夫曼树来代替传统的神经网络，可以提高模型训练的效率。但是如果我们的训练样本里的中心词w是一个很生僻的词，那么就得在霍夫曼树中辛苦的向下走很久了。能不能不用搞这么复杂的一颗霍夫曼树，将模型变的更加简单呢？\n",
    "    \n",
    "    Negative Sampling就是这么一种求解word2vec模型的方法，它摒弃了霍夫曼树，采用了Negative Sampling（负采样）的方法来求解，下面我们就来看看Negative Sampling的求解思路。\n",
    "    \n",
    "    既然名字叫Negative Sampling（负采样），那么肯定使用了采样的方法。采样的方法有很多种，比如之前讲到的大名鼎鼎的MCMC。我们这里的Negative Sampling采样方法并没有MCMC那么复杂。\n",
    "    \n",
    "    比如我们有一个训练样本，中心词是w,它周围上下文共有2c个词，记为context(w)。由于这个中心词w,的确和context(w)相关存在，因此它是一个真实的正例。通过Negative Sampling采样，我们得到neg个和w不同的中心词wi,i=1,2,..neg，这样context(w)和w_i就组成了neg个并不真实存在的负例。利用这一个正例和neg个负例，我们进行二元逻辑回归，得到负采样对应每个词w_i对应的模型参数\\theta_{i}，和每个词的词向量。\n",
    "    \n",
    "    从上面的描述可以看出，Negative Sampling由于没有采用霍夫曼树，每次只是通过采样neg个不同的中心词做负例，就可以训练模型，因此整个过程要比Hierarchical Softmax简单。\n",
    "\n",
    "##### CBOW\n",
    "\n",
    "啥是负样本？给定中心词w的上下文词context(w),那么w是正样本，其他词都是负样本。\n",
    "\n",
    "#### 引入负样本是不是就是引入了逻辑回归思路（二分类），跟上面的层次化softmax中的霍夫曼树（二叉树，也是二分类）类似。\n",
    "\n",
    "<img src=\"img/2019-08-20_182022.png\">\n",
    "<img src=\"img/2019-08-20_182103.png\">\n",
    "<img src=\"img/2019-08-20_182215.png\">\n",
    "\n",
    "伪代码如下：\n",
    "<img src=\"img/2019-08-20_181917.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 源码实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-10-bc77519317f5>, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-10-bc77519317f5>\"\u001b[1;36m, line \u001b[1;32m2\u001b[0m\n\u001b[1;33m    negative = 5; // 5个负样本，标签为0，中心词标签为1\u001b[0m\n\u001b[1;37m                   ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "\n",
    "negative = 5; // 5个负样本，标签为0，中心词标签为1\n",
    "layer1_size = 100; // 词向量维度是100\n",
    "MAX_EXP = 6;  // 做截断处理，[exp(-6), exp(6)]之间\n",
    "window = 5; // 窗口大小\n",
    "// neu1e就是上面的e\n",
    "// syn1neg 是θ参数\n",
    "// neu1是 Xw\n",
    "// syn0是v(u)\n",
    "\n",
    "if (negative > 0) {\n",
    "  for (d = 0; d < negative + 1; d++) {\n",
    "    if (d == 0) {\n",
    "      target = word;\n",
    "      label = 1;\n",
    "    } else {\n",
    "      next_random = next_random * (unsigned long long)25214903917 + 11;\n",
    "      target = table[(next_random >> 16) % table_size];\n",
    "      if (target == 0) {\n",
    "        target = next_random % (vocab_size - 1) + 1;\n",
    "      }\n",
    "      if (target == word) \n",
    "        continue;\n",
    "      label = 0;\n",
    "    }\n",
    "    l2 = target * layer1_size;\n",
    "    f = 0;\n",
    "    for (c = 0; c < layer1_size; c++) {\n",
    "      f += neu1[c] * syn1neg[c + l2];\n",
    "    }\n",
    "    if (f > MAX_EXP) {\n",
    "      g = (label - 1) * alpha;\n",
    "    }else if (f < -MAX_EXP) {\n",
    "      g = (label - 0) * alpha;\n",
    "    }else {\n",
    "      g = (label - expTable[(int)((f + MAX_EXP) * (EXP_TABLE_SIZE / MAX_EXP / 2))]) * alpha;\n",
    "    }\n",
    "    for (c = 0; c < layer1_size; c++) {\n",
    "      neu1e[c] += g * syn1neg[c + l2];\n",
    "    }\n",
    "    for (c = 0; c < layer1_size; c++) {\n",
    "\n",
    "      syn1neg[c + l2] += g * neu1[c];\n",
    "    }\n",
    "  }\n",
    "}\n",
    "// hidden -> in\n",
    "for (a = b; a < window * 2 + 1 - b; a++) {\n",
    "  if (a != window) {\n",
    "    c = sentence_position - window + a;\n",
    "    if (c < 0) \n",
    "      continue;\n",
    "    if (c >= sentence_length) \n",
    "      continue;\n",
    "    last_word = sen[c];\n",
    "    if (last_word == -1) \n",
    "      continue;\n",
    "    for (c = 0; c < layer1_size; c++) {\n",
    "      syn0[c + last_word * layer1_size] += neu1e[c];\n",
    "    }\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Skip-Gram\n",
    "\n",
    "## 下面的u和w应该是写反了，其实是一样的，因为概率是求的联合概率，也就是说u和w成对同时出现\n",
    "<img src=\"img/2019-08-21_095343.png\">\n",
    "<img src=\"img/2019-08-21_095424.png\">\n",
    "<img src=\"img/2019-08-21_095443.png\">\n",
    "<img src=\"img/2019-08-21_095533.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 源码\n",
    "\n",
    "//train skip-gram\n",
    "for (a = b; a < window * 2 + 1 - b; a++) {\n",
    "    if (a != window) {\n",
    "      c = sentence_position - window + a;\n",
    "      if (c < 0) \n",
    "        continue;\n",
    "      if (c >= sentence_length) \n",
    "        continue;\n",
    "      last_word = sen[c];\n",
    "      if (last_word == -1) \n",
    "        continue;\n",
    "      l1 = last_word * layer1_size;\n",
    "      for (c = 0; c < layer1_size; c++) {\n",
    "        neu1e[c] = 0;\n",
    "      }\n",
    "      // NEGATIVE SAMPLING\n",
    "      if (negative > 0) {\n",
    "        for (d = 0; d < negative + 1; d++) {\n",
    "          if (d == 0) {\n",
    "            target = word;\n",
    "            label = 1;\n",
    "          } else {\n",
    "            next_random = next_random * (unsigned long long)25214903917 + 11;\n",
    "            target = table[(next_random >> 16) % table_size];\n",
    "            if (target == 0) \n",
    "                target = next_random % (vocab_size - 1) + 1;\n",
    "            if (target == word) continue;\n",
    "            label = 0;\n",
    "          }\n",
    "          l2 = target * layer1_size;\n",
    "          f = 0;\n",
    "          for (c = 0; c < layer1_size; c++) \n",
    "            f += syn0[c + l1] * syn1neg[c + l2];\n",
    "          if (f > MAX_EXP) \n",
    "            g = (label - 1) * alpha;\n",
    "          else if (f < -MAX_EXP) \n",
    "            g = (label - 0) * alpha;\n",
    "          else \n",
    "            g = (label - expTable[(int)((f + MAX_EXP) * (EXP_TABLE_SIZE / MAX_EXP / 2))]) * alpha;\n",
    "          for (c = 0; c < layer1_size; c++) \n",
    "            neu1e[c] += g * syn1neg[c + l2];\n",
    "          for (c = 0; c < layer1_size; c++) \n",
    "            syn1neg[c + l2] += g * syn0[c + l1];\n",
    "        }\n",
    "      }\n",
    "      // Learn weights input -> hidden\n",
    "      for (c = 0; c < layer1_size; c++) \n",
    "        syn0[c + l1] += neu1e[c];\n",
    "    }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 再谈负采样算法\n",
    "\n",
    "<img src=\"img/2019-08-21_111734.png\">\n",
    "<img src=\"img/2019-08-21_111811.png\">\n",
    "<img src=\"img/2019-08-21_111839.png\">"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
