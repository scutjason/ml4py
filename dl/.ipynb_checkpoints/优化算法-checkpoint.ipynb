{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 动量法\n",
    "\n",
    "#### 为什么要有动量法\n",
    "同一位置上，目标函数在竖直方向（$x_2$轴方向）比在水平方向（$x_1$轴方向）的斜率的绝对值更大。因此，给定学习率，梯度下降迭代自变量时会使自变量在竖直方向比在水平方向移动幅度更大。那么，我们需要一个较小的学习率从而避免自变量在竖直方向上越过目标函数最优解。然而，这会造成自变量在水平方向上朝最优解移动变慢。\n",
    "\n",
    "#### 具体公式\n",
    "动量法的提出是为了解决梯度下降的上述问题。设时间步$t$的自变量为$\\boldsymbol{x}_t$，学习率为$\\eta_t$。\n",
    "在时间步$0$，动量法创建速度变量$\\boldsymbol{v}_0$，并将其元素初始化成0。在时间步$t>0$，动量法对每次迭代的步骤做如下修改：\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\boldsymbol{v}_t &\\leftarrow \\gamma \\boldsymbol{v}_{t-1} + \\eta_t \\boldsymbol{g}_t, \\\\\n",
    "\\boldsymbol{x}_t &\\leftarrow \\boldsymbol{x}_{t-1} - \\boldsymbol{v}_t,\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "其中，动量超参数$\\gamma$满足$0 \\leq \\gamma < 1$。当$\\gamma=0$时，动量法等价于小批量随机梯度下降。\n",
    "\n",
    "在解释动量法的数学原理前，让我们先从实验中观察梯度下降在使用动量法后的迭代轨迹。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def momentum_2d(x1, x2, v1, v2):\n",
    "    v1 = gamma * v1 + eta * 0.2 * x1\n",
    "    v2 = gamma * v2 + eta * 4 * x2\n",
    "    return x1 - v1, x2 - v2, v1, v2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 怎么理解动量法或者说为什么它是有效的\n",
    "\n",
    "##### 指数加权移动平均\n",
    "给定超参数$0 \\leq \\gamma < 1$，当前时间步$t$的变量$y_t$是上一时间步$t-1$的变量$y_{t-1}$和当前时间步另一变量$x_t$的线性组合：\n",
    "\n",
    "$$y_t = \\gamma y_{t-1} + (1-\\gamma) x_t.$$\n",
    "\n",
    "$$y_t \\approx 0.05 \\sum_{i=0}^{19} 0.95^i x_{t-i}.$$\n",
    "\n",
    "在实际中，我们常常将$y_t$看作是对最近$1/(1-\\gamma)$个时间步的$x_t$值的加权平均。例如，当$\\gamma = 0.95$时，$y_t$可以被看作对最近20个时间步的$x_t$值的加权平均；当$\\gamma = 0.9$时，$y_t$可以看作是对最近10个时间步的$x_t$值的加权平均。而且，离当前时间步$t$越近的$x_t$值获得的权重越大（越接近1）。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 由指数加权移动平均理解动量法\n",
    "\n",
    "现在，我们对动量法的速度变量做变形：\n",
    "\n",
    "$$\\boldsymbol{v}_t \\leftarrow \\gamma \\boldsymbol{v}_{t-1} + (1 - \\gamma) \\left(\\frac{\\eta_t}{1 - \\gamma} \\boldsymbol{g}_t\\right). $$\n",
    "\n",
    "由指数加权移动平均的形式可得，速度变量$\\boldsymbol{v}_t$实际上对序列$\\{\\eta_{t-i}\\boldsymbol{g}_{t-i} /(1-\\gamma):i=0,\\ldots,1/(1-\\gamma)-1\\}$做了指数加权移动平均。换句话说，相比于小批量随机梯度下降，动量法在每个时间步的自变量更新量近似于将前者对应的最近$1/(1-\\gamma)$个时间步的更新量做了指数加权移动平均后再除以$1-\\gamma$。所以，在动量法中，自变量在各个方向上的移动幅度不仅取决当前梯度，还取决于过去的各个梯度在各个方向上是否一致。在本节之前示例的优化问题中，所有梯度在水平方向上为正（向右），而在竖直方向上时正（向上）时负（向下）。这样，我们就可以使用较大的学习率，从而使自变量向最优解更快移动。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 从0开始实现动量法\n",
    "\n",
    "def init_momentum_states():\n",
    "    v_w = nd.zeros((features.shape[1], 1))\n",
    "    v_b = nd.zeros(1)\n",
    "    return (v_w, v_b)\n",
    "\n",
    "def sgd_momentum(params, states, hyperparams):\n",
    "    for p, v in zip(params, states):\n",
    "        v[:] = hyperparams['momentum'] * v + hyperparams['lr'] * p.grad\n",
    "        p[:] -= v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### keras实现\n",
    "\n",
    "keras.optimizers.SGD(lr=0.01, momentum=0.0, decay=0.0, nesterov=False)\n",
    "\n",
    "随机梯度下降优化器。\n",
    "\n",
    "包含扩展功能的支持： - 动量（momentum）优化, - 学习率衰减（每次参数更新后） - Nestrov 动量 (NAG) 优化\n",
    "\n",
    "参数\n",
    "\n",
    "    lr: float >= 0. 学习率。\n",
    "    momentum: float >= 0. 参数，用于加速 SGD 在相关方向上前进，并抑制震荡。\n",
    "    decay: float >= 0. 每次参数更新后学习率衰减值。\n",
    "    nesterov: boolean. 是否使用 Nesterov 动量。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
