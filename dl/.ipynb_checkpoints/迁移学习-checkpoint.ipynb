{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "一.背景\n",
    "\n",
    "在之前用深度神经网络进行参数训练时，我们每次的训练都是从头开始的，随机地初始化网络。在网络层数不深的时候，训练时间可以接受，但随着现在网络构越来越复杂，网络层数越来越深，少则几十层，多者上百层甚至上千层，如果任然每次都从头开始训练，时间的开销则会非常大。例如ImageNet网络的训练常常需要几个星期的时间，而对于普通的研究者而言，这是不可接受的。\n",
    "\n",
    "迁移学习的本质是利用将一个问题上训练的所得，用于改进在另一个问题的效果。举例说明，就是把其他人在某个大数据集上训练好的网络模型（参数）运用到到自己的问题上来，这样就可以避免重复的初始化及参数训练。\n",
    "\n",
    "二.相关术语：精调，预训练\n",
    "\n",
    "1.精调(fine-tuning):利用下载好的网络模型，用我们的数据集继续训练，称为精调。\n",
    "\n",
    "    精调的省时性。精调可以使得网络能够从一个良好的初始点开始学习，使得网络的训练时间大大节省，特别是在我们的数据集和网络模型训练数据集相似的情况下。\n",
    "    精调的普适性。深度学习是一个特征不断学习进化的过程，来自于深度神经网络所提取的特征具有共通性，例如卷积神经网络的前几层用于边缘特征提取，中间几层用于部分特征提取，后面几层才是完整的特征，而物体的边缘特征具有共通性。\n",
    "    精调可以降低过拟合。原因是我们下载的网络模型的训练集往往比我们自己的数据集大许多倍，用较少的训练集再去训练，拟合效果更符合期望。\n",
    "\n",
    "精调举例：由于深度神经网络可以从数据中学习较好的特征，所以可以和机器学习相结合，例如用深度神经网络学习到的特征用到线性模型或SVM模型中；另外还可以去掉网络的最后一层，即改变输出，用于其他任务。\n",
    "\n",
    "2.预训练（pre-training）:即使训练的数据很充分，我们还可以先使用其他的数据进行训练，以改善过拟合，同时也会提高性能。这称为预训练。\n",
    "\n",
    "预训练的技巧\n",
    "\n",
    "    如果额外的数据很少，可先固定网络的前几层，以防止过拟合，因为网络的前几层对应的是更底层的特征，更共通的特征，所以可以只训练最后一层或最后几层。\n",
    "    训练时可以使用较低的学习率，因为网络的初始值已经很不错了。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 要理解迁移学习的整个过程就是要搞清楚下面三件事：\n",
    "\n",
    "    迁移学习迁移什么\n",
    "    迁移学习是怎么迁移的\n",
    "    迁移学习什么时候使用\n",
    "\n",
    "#### 迁移什么\n",
    "\n",
    "在预训练模型中存在各种特征数据与权重信息、有些是与分类识别的对象本身关联比较紧密的特征数据与权重信息，有些是一些比较共性的特征数据与信息，是可以被不同的任务或者对象之间共享的，迁移学习就是要迁移那些共性特征数据与信息，从而避免再次学习这些知识，实现快速学习。简单点说迁移学习主要是实现卷积层共性特征迁移，\n",
    "\n",
    "\n",
    "#### 怎么迁移\n",
    "\n",
    "迁移学习早期也被称为感应迁移(inductive transfer)，为了搞清楚，迁移学习到底是怎么迁移的，大神Yoshua Bengio等人尝试定义了一个八层的神经网络，将ImageNet的数据集1000个种类分为A与B两个分类子集，数量均为500，然后继续分别训练生成forzen推断图、然后分别将网络模型A与B的前三层分别copy给没有训练之前网络B，并对B的余下5层随机初始化之后开始训练这两个全新的网络（B3B与A3B），他们想通过这个实验证明、如果B3B与A3B跟之前训练好的网络B有同样的识别准确率就说明自迁移网络B3B与迁移网络A3B的前三层网络特征是共性特征信息，可以用来迁移，如果网络性能下降则说明它们含有目标对象相关的个性特征无法用来迁移。\n",
    "<img src=\"img/34b477c6c2bf4ba1973b61c8224ce1f9.jpeg\">\n",
    "最终的实验结果表明，前面7层都是共性特征，只有网络的最后一层才是任务相关的个性特征数据，无法进行迁移，整个实验结果如下：\n",
    "<img src=\"img/08bd2bad0fb345d69f7e095765228e1e.jpeg\">\n",
    "<img src=\"img/1399b62e5bb94cb4ae5c3c2aec8fe45c.jpeg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "从上面可以看出单纯的迁移学习AnB的方式，随着层数的增加网络性能不断下降，但是通过迁移学习加fine-tuning的方式AnB+对前N层进行重新训练调整优化，迁移学习的效果居然比原来的还要好。充分说明迁移学习+fine-tuning是个训练卷积神经网络的好方法。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 什么时候使用迁移\n",
    "\n",
    "当我们有相似的任务需要完成的时候，我们可以使用预训练的相关模型，在此基础上进行迁移学习即可，"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在实际使用中我们把预训练的网络称为base-network,把要迁移的前n层复制到一个到目标网络（target network）,然后随机初始化目标网络的余下各层、开始训练进行反向传播、反向传播时候有两种方法可以使用：\n",
    "\n",
    "    把前面n层冻结forzen、只对后面的层进行训练，这种方法适合少的样本数据，而且随着层冻结n数值增大、网络性能会下降，这种是单纯的迁移学习。\n",
    "    不冻结前n层、全程参与训练不断调整它们的参数，实现更好的网络性能这种方法称为迁移学习+fine-tuning\n",
    "\n",
    "迁移学习使用\n",
    "\n",
    "在tensorflow中通过tensorflow object detection API框架使用迁移学习是对象检测与识别，"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
