{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attention机制\n",
    "\n",
    "在seq2seq里，解码器在各个时间步依赖相同的固定长度的背景变量C来获取输入序列信息。当编码器为循环神经网络时，背景变量来自它最终时间步的隐藏状态。\n",
    "\n",
    "现在，让我们再次思考那一节提到的翻译例子：输入为英语序列“They”“are”“watching”“.”，输出为法语序列“Ils”“regardent”“.”。不难想到，解码器在生成输出序列中的每一个词时可能只需利用输入序列某一部分的信息。例如，在输出序列的时间步1，解码器可以主要依赖“They”“are”的信息来生成“Ils”，在时间步2则主要使用来自“watching”的编码信息生成“regardent”，最后在时间步3则直接映射句号“.”。这看上去就像是在解码器的每一时间步对输入序列中不同时间步的表征或编码信息分配不同的注意力一样。这也是注意力机制的由来。\n",
    "\n",
    "仍然以循环神经网络为例，注意力机制通过对编码器所有时间步的隐藏状态做加权平均来得到背景变量。解码器在每一时间步调整这些权重，即注意力权重，从而能够在不同时间步分别关注输入序列中的不同部分并编码进相应时间步的背景变量。本节我们将讨论注意力机制是怎么工作的。\n",
    "\n",
    "\n",
    "在seq2seq里我们区分了输入序列或编码器的索引$t$与输出序列或解码器的索引$t'$。该节中，解码器在时间步$t'$的隐藏状态$\\boldsymbol{s}_{t'} = g(\\boldsymbol{y}_{t'-1}, \\boldsymbol{c}, \\boldsymbol{s}_{t'-1})$，其中$\\boldsymbol{y}_{t'-1}$是上一时间步$t'-1$的输出$y_{t'-1}$的表征，且任一时间步$t'$使用相同的背景变量$\\boldsymbol{c}$。但在注意力机制中，解码器的每一时间步将使用可变的背景变量。记$\\boldsymbol{c}_{t'}$是解码器在时间步$t'$的背景变量，那么解码器在该时间步的隐藏状态可以改写为\n",
    "\n",
    "$$\\boldsymbol{s}_{t'} = g(\\boldsymbol{y}_{t'-1}, \\boldsymbol{c}_{t'}, \\boldsymbol{s}_{t'-1}).$$\n",
    "\n",
    "这里的关键是如何计算背景变量$\\boldsymbol{c}_{t'}$和如何利用它来更新隐藏状态$\\boldsymbol{s}_{t'}$。下面将分别描述这两个关键点。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 第一个关键点 计算背景变量"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下图描绘了注意力机制如何为解码器在时间步2计算背景变量。首先，函数a根据解码器在时间步1的隐藏状态和编码器在各个时间步的隐藏状态计算softmax运算的输入。softmax运算输出概率分布并对编码器各个时间步的隐藏状态做加权平均，从而得到背景变量。\n",
    "\n",
    "<img src=\"img/attention.svg\">\n",
    "\n",
    "具体来说，令编码器在时间步$t$的隐藏状态为$\\boldsymbol{h}_t$，且总时间步数为$T$。那么解码器在时间步$t'$的背景变量为所有编码器隐藏状态的加权平均：\n",
    "\n",
    "$$\\boldsymbol{c}_{t'} = \\sum_{t=1}^T \\alpha_{t' t} \\boldsymbol{h}_t,$$\n",
    "\n",
    "其中给定$t'$时，权重$\\alpha_{t' t}$在$t=1,\\ldots,T$的值是一个概率分布。为了得到概率分布，我们可以使用softmax运算:\n",
    "\n",
    "$$\\alpha_{t' t} = \\frac{\\exp(e_{t' t})}{ \\sum_{k=1}^T \\exp(e_{t' k}) },\\quad t=1,\\ldots,T.$$\n",
    "\n",
    "现在，我们需要定义如何计算上式中softmax运算的输入$e_{t' t}$。由于$e_{t' t}$同时取决于解码器的时间步$t'$和编码器的时间步$t$，我们不妨以解码器在时间步$t'-1$的隐藏状态$\\boldsymbol{s}_{t' - 1}$与编码器在时间步$t$的隐藏状态$\\boldsymbol{h}_t$为输入，并通过函数$a$计算$e_{t' t}$：\n",
    "\n",
    "$$e_{t' t} = a(\\boldsymbol{s}_{t' - 1}, \\boldsymbol{h}_t).$$\n",
    "\n",
    "\n",
    "这里函数$a$有多种选择，如果两个输入向量长度相同，一个简单的选择是计算它们的内积$a(\\boldsymbol{s}, \\boldsymbol{h})=\\boldsymbol{s}^\\top \\boldsymbol{h}$。而最早提出注意力机制的论文则将输入连结后通过含单隐藏层的多层感知机变换 [1]：\n",
    "\n",
    "$$a(\\boldsymbol{s}, \\boldsymbol{h}) = \\boldsymbol{v}^\\top \\tanh(\\boldsymbol{W}_s \\boldsymbol{s} + \\boldsymbol{W}_h \\boldsymbol{h}),$$\n",
    "\n",
    "其中$\\boldsymbol{v}$、$\\boldsymbol{W}_s$、$\\boldsymbol{W}_h$都是可以学习的模型参数。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 举例\n",
    "\n",
    "<img src=\"img/11692737-0f87fe8202a4f2d5.jfif\">\n",
    "输入的序列是“我爱中国”，因此，Encoder中的h1、h2、h3、h4就可以分别看做是“我”、“爱”、“中”、“国”所代表的信息。在翻译成英语时，第一个上下文c1应该和“我”这个字最相关，因此对应的 a11就比较大，而相应的 a12 、a13 、 a14 就比较小。c2应该和“爱”最相关，因此对应的 a22 就比较大。最后的c3和h3、h4最相关，因此 a33 、 a34的值就比较大。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 第二个关键点 更新隐藏状态"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "现在我们描述第二个关键点，即更新隐藏状态。以门控循环单元为例，在解码器中我们可以对门控循环单元（GRU）中门控循环单元的设计稍作修改，从而变换上一时间步$t'-1$的输出$\\boldsymbol{y}_{t'-1}$、隐藏状态$\\boldsymbol{s}_{t' - 1}$和当前时间步$t'$的含注意力机制的背景变量$\\boldsymbol{c}_{t'}$ [1]。解码器在时间步$t'$的隐藏状态为\n",
    "\n",
    "$$\\boldsymbol{s}_{t'} = \\boldsymbol{z}_{t'} \\odot \\boldsymbol{s}_{t'-1}  + (1 - \\boldsymbol{z}_{t'}) \\odot \\tilde{\\boldsymbol{s}}_{t'},$$\n",
    "\n",
    "其中的重置门、更新门和候选隐藏状态分别为\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\boldsymbol{r}_{t'} &= \\sigma(\\boldsymbol{W}_{yr} \\boldsymbol{y}_{t'-1} + \\boldsymbol{W}_{sr} \\boldsymbol{s}_{t' - 1} + \\boldsymbol{W}_{cr} \\boldsymbol{c}_{t'} + \\boldsymbol{b}_r),\\\\\n",
    "\\boldsymbol{z}_{t'} &= \\sigma(\\boldsymbol{W}_{yz} \\boldsymbol{y}_{t'-1} + \\boldsymbol{W}_{sz} \\boldsymbol{s}_{t' - 1} + \\boldsymbol{W}_{cz} \\boldsymbol{c}_{t'} + \\boldsymbol{b}_z),\\\\\n",
    "\\tilde{\\boldsymbol{s}}_{t'} &= \\text{tanh}(\\boldsymbol{W}_{ys} \\boldsymbol{y}_{t'-1} + \\boldsymbol{W}_{ss} (\\boldsymbol{s}_{t' - 1} \\odot \\boldsymbol{r}_{t'}) + \\boldsymbol{W}_{cs} \\boldsymbol{c}_{t'} + \\boldsymbol{b}_s),\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "其中含下标的$\\boldsymbol{W}$和$\\boldsymbol{b}$分别为门控循环单元的权重参数和偏差参数。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 发展\n",
    "本质上，注意力机制能够为表征中较有价值的部分分配较多的计算资源。这个有趣的想法自提出后得到了快速发展，特别是启发了依靠注意力机制来编码输入序列并解码出输出序列的变换器（Transformer）模型的设计 [2]。变换器抛弃了卷积神经网络和循环神经网络的架构。它在计算效率上比基于循环神经网络的编码器—解码器模型通常更具明显优势。含注意力机制的变换器的编码结构在后来的BERT预训练模型中得以应用并令后者大放异彩：微调后的模型在多达11项自然语言处理任务中取得了当时最先进的结果 [3]。不久后，同样是基于变换器设计的GPT-2模型于新收集的语料数据集预训练后，在7个未参与训练的语言模型数据集上均取得了当时最先进的结果 [4]。除了自然语言处理领域，注意力机制还被广泛用于图像分类、自动图像描述、唇语解读以及语音识别。\n",
    "\n",
    "#### 小结\n",
    "\n",
    "* 可以在解码器的每个时间步使用不同的背景变量，并对输入序列中不同时间步编码的信息分配不同的注意力。\n",
    "* 广义上，注意力机制的输入包括查询项以及一一对应的键项和值项。\n",
    "* 注意力机制可以采用更为高效的矢量化计算。\n",
    "\n",
    "\n",
    "#### 练习\n",
    "\n",
    "* 基于本节的模型设计，为什么不可以将解码器在不同时间步的隐藏状态$\\boldsymbol{s}_{t' - 1}^\\top \\in \\mathbb{R}^{1 \\times h}, t' \\in 1, \\ldots, T'$连结成查询项矩阵$\\boldsymbol{Q} \\in \\mathbb{R}^{T' \\times h}$，从而同时计算不同时间步的含注意力机制的背景变量$\\boldsymbol{c}_{t'}^\\top, t' \\in 1, \\ldots, T'$？\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 应用领域\n",
    "\n",
    "**** 语音识别 ****\n",
    "   \n",
    "    输入是语音信号序列，输出是文字序列。\n",
    "\n",
    "** 文本摘要 **\n",
    "    \n",
    "    输入是一段文本序列，输出是这段文本序列的摘要序列。通常将文本摘要方法分为两类，extractive 抽取式摘要和 abstractive 生成式摘要。前者是从一篇文档或者多篇文档中通过排序找出最有信息量的句子，组合成摘要；后者类似人类编辑一样，通过理解全文的内容，然后用简练的话将全文概括出来。在应用中，extractive摘要方法更加实用一些，也被广泛使用，但在连贯性、一致性上存在一定的问题，需要进行一些后处理；abstractive 摘要方法可以很好地解决这些问题，但研究起来非常困难。\n",
    "\n",
    "** 对话生成 **\n",
    "    \n",
    "    Seq2Seq 模型提出之后，就有很多的工作将其应用在 Chatbot 任务上，希望可以通过海量的数据来训练模型，做出一个智能体，可以回答任何开放性的问题；而另外一拨人，研究如何将 Seq2Seq 模型配合当前的知识库来做面向具体任务的 Chatbot，在一个非常垂直的领域（比如：购买电影票等）也取得了一定的进展。\n",
    "\n",
    "**诗词生成 ** \n",
    "\n",
    "    让机器为你写诗并不是一个遥远的梦，Seq2Seq 模型一个非常有趣的应用正是诗词生成，即给定诗词的上一句来生成下一句。\n",
    "\n",
    "**生成代码补全 **\n",
    "\n",
    "**预训练 ** \n",
    "    \n",
    "    2015年，Google提出了将Seq2Seq的自动编码器作为LSTM文本分类的一个预训练步骤，从而提高了分类的稳定性。这使得Seq2Seq技术的目的不再局限于得到序列本身，为其应用领域翻开了崭新的一页。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**阅读理解**\n",
    "\n",
    "    将输入的文章和问题分别编码，再对其进行解码得到问题的答案。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
