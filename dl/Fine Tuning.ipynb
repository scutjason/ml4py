{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 迁移学习\n",
    "\n",
    "假设我们想从图像中识别出不同种类的椅子，然后将购买链接推荐给用户。一种可能的方法是先找出100种常见的椅子，为每种椅子拍摄1,000张不同角度的图像，然后在收集到的图像数据集上训练一个分类模型。这个椅子数据集虽然可能比Fashion-MNIST数据集要庞大，但样本数仍然不及ImageNet数据集中样本数的十分之一。这可能会导致适用于ImageNet数据集的复杂模型在这个椅子数据集上过拟合。同时，因为数据量有限，最终训练得到的模型的精度也可能达不到实用的要求。\n",
    "\n",
    "为了应对上述问题，一个显而易见的解决办法是收集更多的数据。然而，收集和标注数据会花费大量的时间和资金。例如，为了收集ImageNet数据集，研究人员花费了数百万美元的研究经费。虽然目前的数据采集成本已降低了不少，但其成本仍然不可忽略。\n",
    "\n",
    "另外一种解决办法是应用迁移学习（transfer learning），将从源数据集学到的知识迁移到目标数据集上。例如，虽然ImageNet数据集的图像大多跟椅子无关，但在该数据集上训练的模型可以抽取较通用的图像特征，从而能够帮助识别边缘、纹理、形状和物体组成等。这些类似的特征对于识别椅子也可能同样有效。\n",
    "\n",
    "#### Fine Tuning步骤\n",
    "\n",
    "\n",
    "    1、在源数据集（如ImageNet数据集）上预训练一个神经网络模型，即源模型。\n",
    "    2、创建一个新的神经网络模型，即目标模型。它复制了源模型上除了输出层外的所有模型设计及其参数。我们假设这些模型参数包含了源数据集上学习到的知识，且这些知识同样适用于目标数据集。我们还假设源模型的输出层跟源数据集的标签紧密相关，因此在目标模型中不予采用。\n",
    "    3、为目标模型添加一个输出大小为目标数据集类别个数的输出层，并随机初始化该层的模型参数。\n",
    "    4、在目标数据集（如椅子数据集）上训练目标模型。我们将从头训练输出层，而其余层的参数都是基于源模型的参数微调得到的。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/finetune.svg\">\n",
    "\n",
    "    图 9.1 微调\n",
    "\n",
    "当目标数据集远小于源数据集时，微调有助于提升模型的泛化能力。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 小结\n",
    "\n",
    "    迁移学习将从源数据集学到的知识迁移到目标数据集上。微调是迁移学习的一种常用技术。\n",
    "    目标模型复制了源模型上除了输出层外的所有模型设计及其参数，并基于目标数据集微调这些参数。而目标模型的输出层需要从头训练。\n",
    "    一般来说，微调参数会使用较小的学习率，而从头训练输出层可以使用较大的学习率。\n",
    "\n",
    "#### 思考\n",
    "\n",
    "    不断增大finetune_net的学习率。精度会有什么变化？\n",
    "    进一步调节对比试验中finetune_net和scratch_net的超参数。它们的精度是不是依然有区别？\n",
    "    将finetune_net.features中的参数固定为源模型的参数而不在训练中迭代，结果会怎样？你可能会用到以下代码。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们新建一个神经网络作为目标模型。它的定义与预训练的源模型一样，但最后的输出个数等于目标数据集的类别数。在下面的代码中，目标模型实例finetune_net的成员变量features中的模型参数被初始化为源模型相应层的模型参数。由于features中的模型参数是在ImageNet数据集上预训练得到的，已经足够好，因此一般只需使用较小的学习率来微调这些参数。而成员变量output中的模型参数采用了随机初始化，一般需要更大的学习率从头训练。假设Trainer实例中的学习率为η，我们设成员变量output中的模型参数在迭代中使用的学习率为10η。\n",
    "\n",
    "我们将Trainer实例中的学习率设得小一点，如0.01，以便微调预训练得到的模型参数。根据前面的设置，我们将以10倍的学习率从头训练目标模型的输出层参数。\n",
    "\n",
    "作为对比，我们定义一个相同的模型，但将它的所有模型参数都初始化为随机值。由于整个模型都需要从头训练，我们可以使用较大的学习率。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
