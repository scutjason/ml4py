{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 背景\n",
    "\n",
    "由前面的RNN的梯度求解公式，我们发现，当时间步数较大或者时间步较小时，循环神经网络的梯度较容易出现衰减或爆炸。虽然裁剪梯度可以应对梯度爆炸，但无法解决梯度衰减的问题。通常由于这个原因，循环神经网络在实际中较难捕捉时间序列中时间步距离较大的依赖关系。\n",
    "\n",
    "一般的RNN网络会存在梯度弥散和梯度爆炸问题，梯度爆炸的问题可以通过使用Gradient Clipping的方法来解决，而梯度弥散的问题则需要重新设计RNN的网络结构。GRU和LSTM就是为了解决梯度弥散的问题而设计的。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 门控循环单元\n",
    "\n",
    "下面将介绍门控循环单元的设计。它引入了重置门（reset gate）和更新门（update gate）的概念，从而修改了循环神经网络中隐藏状态的计算方式。\n",
    "\n",
    "### 重置门和更新门\n",
    "\n",
    "如图6.4所示，门控循环单元中的重置门和更新门的输入均为当前时间步输入$\\boldsymbol{X}_t$与上一时间步隐藏状态$\\boldsymbol{H}_{t-1}$，输出由激活函数为sigmoid函数的全连接层计算得到。\n",
    "\n",
    "\n",
    "![门控循环单元中重置门和更新门的计算](./img/gru_1.svg)\n",
    "\n",
    "\n",
    "具体来说，假设隐藏单元个数为$h$，给定时间步$t$的小批量输入$\\boldsymbol{X}_t \\in \\mathbb{R}^{n \\times d}$（样本数为$n$，输入个数为$d$）和上一时间步隐藏状态$\\boldsymbol{H}_{t-1} \\in \\mathbb{R}^{n \\times h}$。重置门$\\boldsymbol{R}_t \\in \\mathbb{R}^{n \\times h}$和更新门$\\boldsymbol{Z}_t \\in \\mathbb{R}^{n \\times h}$的计算如下：\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\boldsymbol{R}_t = \\sigma(\\boldsymbol{X}_t \\boldsymbol{W}_{xr} + \\boldsymbol{H}_{t-1} \\boldsymbol{W}_{hr} + \\boldsymbol{b}_r),\\\\\n",
    "\\boldsymbol{Z}_t = \\sigma(\\boldsymbol{X}_t \\boldsymbol{W}_{xz} + \\boldsymbol{H}_{t-1} \\boldsymbol{W}_{hz} + \\boldsymbol{b}_z),\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "其中$\\boldsymbol{W}_{xr}, \\boldsymbol{W}_{xz} \\in \\mathbb{R}^{d \\times h}$和$\\boldsymbol{W}_{hr}, \\boldsymbol{W}_{hz} \\in \\mathbb{R}^{h \\times h}$是权重参数，$\\boldsymbol{b}_r, \\boldsymbol{b}_z \\in \\mathbb{R}^{1 \\times h}$是偏差参数。sigmoid函数可以将元素的值变换到0和1之间。因此，重置门$\\boldsymbol{R}_t$和更新门$\\boldsymbol{Z}_t$中每个元素的值域都是$[0, 1]$。\n",
    "\n",
    "### 候选隐藏状态\n",
    "\n",
    "接下来，门控循环单元将计算候选隐藏状态来辅助稍后的隐藏状态计算。如图6.5所示，我们将当前时间步重置门的输出与上一时间步隐藏状态做按元素乘法（符号为$\\odot$）。如果重置门中元素值接近0，那么意味着重置对应隐藏状态元素为0，即丢弃上一时间步的隐藏状态。如果元素值接近1，那么表示保留上一时间步的隐藏状态。然后，将按元素乘法的结果与当前时间步的输入连结，再通过含激活函数tanh的全连接层计算出候选隐藏状态，其所有元素的值域为$[-1, 1]$。\n",
    "\n",
    "![门控循环单元中候选隐藏状态的计算。这里的$\\odot$是按元素乘法](./img/gru_2.svg)\n",
    "\n",
    "具体来说，时间步$t$的候选隐藏状态$\\tilde{\\boldsymbol{H}}_t \\in \\mathbb{R}^{n \\times h}$的计算为\n",
    "\n",
    "$$\\tilde{\\boldsymbol{H}}_t = \\text{tanh}(\\boldsymbol{X}_t \\boldsymbol{W}_{xh} + \\left(\\boldsymbol{R}_t \\odot \\boldsymbol{H}_{t-1}\\right) \\boldsymbol{W}_{hh} + \\boldsymbol{b}_h),$$\n",
    "\n",
    "其中$\\boldsymbol{W}_{xh} \\in \\mathbb{R}^{d \\times h}$和$\\boldsymbol{W}_{hh} \\in \\mathbb{R}^{h \\times h}$是权重参数，$\\boldsymbol{b}_h \\in \\mathbb{R}^{1 \\times h}$是偏差参数。从上面这个公式可以看出，重置门控制了上一时间步的隐藏状态如何流入当前时间步的候选隐藏状态。而上一时间步的隐藏状态可能包含了时间序列截至上一时间步的全部历史信息。因此，重置门可以用来丢弃与预测无关的历史信息。\n",
    "\n",
    "### 隐藏状态\n",
    "\n",
    "最后，时间步$t$的隐藏状态$\\boldsymbol{H}_t \\in \\mathbb{R}^{n \\times h}$的计算使用当前时间步的更新门$\\boldsymbol{Z}_t$来对上一时间步的隐藏状态$\\boldsymbol{H}_{t-1}$和当前时间步的候选隐藏状态$\\tilde{\\boldsymbol{H}}_t$做组合：\n",
    "\n",
    "$$\\boldsymbol{H}_t = \\boldsymbol{Z}_t \\odot \\boldsymbol{H}_{t-1}  + (1 - \\boldsymbol{Z}_t) \\odot \\tilde{\\boldsymbol{H}}_t.$$\n",
    "\n",
    "\n",
    "![门控循环单元中隐藏状态的计算。这里的$\\odot$是按元素乘法](./img/gru_3.svg)\n",
    "\n",
    "\n",
    "值得注意的是，更新门可以控制隐藏状态应该如何被包含当前时间步信息的候选隐藏状态所更新，如图6.6所示。假设更新门在时间步$t'$到$t$（$t' < t$）之间一直近似1。那么，在时间步$t'$到$t$之间的输入信息几乎没有流入时间步$t$的隐藏状态$\\boldsymbol{H}_t$。实际上，这可以看作是较早时刻的隐藏状态$\\boldsymbol{H}_{t'-1}$一直通过时间保存并传递至当前时间步$t$。这个设计可以应对循环神经网络中的梯度衰减问题，并更好地捕捉时间序列中时间步距离较大的依赖关系。\n",
    "\n",
    "我们对门控循环单元的设计稍作总结：\n",
    "\n",
    "* 重置门有助于捕捉时间序列里短期的依赖关系；\n",
    "* 更新门有助于捕捉时间序列里长期的依赖关系。\n",
    "* 门控循环神经网络可以更好地捕捉时间序列中时间步距离较大的依赖关系。\n",
    "* 门控循环单元引入了门的概念，从而修改了循环神经网络中隐藏状态的计算方式。它包括重置门、更新门、候选隐藏状态和隐藏状态。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 更新门\n",
    "\n",
    "更新门帮助模型决定到底要将多少过去的信息传递到未来，或到底前一时间步和当前时间步的信息有多少是需要继续传递的。这一点非常强大，因为模型能决定从过去复制所有的信息以减少梯度消失的风险。我们随后会讨论更新门的使用方法，现在只需要记住 z_t 的计算公式就行。\n",
    "\n",
    "r_t 用于控制前一时刻隐层单元 h_(t-1) 对当前词 x_t 的影响。如果 h_(t-1) 对 x_t 不重要，即从当前词 x_t 开始表述了新的意思，与上文无关。那么开关 r_t 可以打开，使得 h_(t-1) 对 x_t 不产生影响。\n",
    "\n",
    "#### 重置门\n",
    "\n",
    "本质上来说，重置门主要决定了到底有多少过去的信息需要遗忘，我们可以使用以下表达式计算：\n",
    "\n",
    "表达式与更新门的表达式是一样的，只不过线性变换的参数和用处不一样而已。下图展示了该运算过程的表示方法。\n",
    "\n",
    "z_t 用于决定是否忽略当前词 x_t。类似于 LSTM 中的输入门 i_t，z_t 可以判断当前词 x_t 对整体意思的表达是否重要。当 z_t 开关接通下面的支路时，我们将忽略当前词 x_t，同时构成了从 h_(t-1) 到 h_t 的短路连接，这使得梯度得已有效地反向传播。和 LSTM 相同，这种短路机制有效地缓解了梯度消失现象，这个机制于 highway networks 十分相似。\n",
    "\n",
    "\n",
    "#### 当前记忆内容\n",
    "现在我们具体讨论一下这些门控到底如何影响最终的输出。在重置门的使用中，新的记忆内容将使用重置门储存过去相关的信息，它的计算表达式为：\n",
    "<img src=\"img/p1.png\">。\n",
    "输入 x_t 与上一时间步信息 h_(t-1) 先经过一个线性变换，即分别右乘矩阵 W 和 U。\n",
    "\n",
    "    计算重置门 r_t 与 Uh_(t-1) 的 Hadamard 乘积，即 r_t 与 Uh_(t-1) 的对应元素乘积。因为前面计算的重置门是一个由 0 到 1 组成的向量，它会衡量门控开启的大小。例如某个元素对应的门控值为 0，那么它就代表这个元素的信息完全被遗忘掉。该 Hadamard 乘积将确定所要保留与遗忘的以前信息。\n",
    "\n",
    "\n",
    "#### 当前时间步的最终记忆\n",
    "在最后一步，网络需要计算 h_t，该向量将保留当前单元的信息并传递到下一个单元中。在这个过程中，我们需要使用更新门，它决定了当前记忆内容 h'_t 和前一时间步 h_(t-1) 中需要收集的信息是什么。这一过程可以表示为：\n",
    "<img src=\"img/p.png\">\n",
    "\n",
    "    z_t 为更新门的激活结果，它同样以门控的形式控制了信息的流入。z_t 与 h_(t-1) 的 Hadamard 乘积表示前一时间步保留到最终记忆的信息，该信息加上当前记忆保留至最终记忆的信息就等于最终门控循环单元输出的内容。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "现在我们应该比较了解到底 GRU 是如何通过更新门与重置门存储并过滤信息。门控循环单元不会随时间而清除以前的信息，它会保留相关的信息并传递到下一个单元，因此它利用全部信息而避免了梯度消失问题。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GRU与LSTM的区别\n",
    "<img src=\"img/quesbase64155694047372134166.png\">\n",
    "<img src=\"img/2019-08-19_160214.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GRU 有两个有两个门，即一个重置门（reset gate）和一个更新门（update gate）。从直观上来说，重置门决定了如何将新的输入信息与前面的记忆相结合，更新门定义了前面记忆保存到当前时间步的量。如果我们将重置门设置为 1，更新门设置为 0，那么我们将再次获得标准 RNN 模型。使用门控机制学习长期依赖关系的基本思想和 LSTM 一致，但还是有一些关键区别：\n",
    "\n",
    "    GRU 有两个门（重置门与更新门），而 LSTM 有三个门（输入门、遗忘门和输出门）。\n",
    "    GRU 并不会控制并保留内部记忆（c_t），且没有 LSTM 中的输出门。\n",
    "    LSTM 中的输入与遗忘门对应于 GRU 的更新门，重置门直接作用于前面的隐藏状态。\n",
    "    在计算输出时并不应用二阶非线性。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_inputs, num_hiddens, num_outputs = vocab_size, 256, vocab_size\n",
    "\n",
    "def get_params():\n",
    "    def _one(shape):\n",
    "        return np.random.normal(scale=0.01, shape=shape)\n",
    "\n",
    "    def _three():\n",
    "        return (_one((num_inputs, num_hiddens)),\n",
    "                _one((num_hiddens, num_hiddens)),\n",
    "                np.zeros(num_hiddens))\n",
    "\n",
    "    W_xz, W_hz, b_z = _three()  # 更新门参数\n",
    "    W_xr, W_hr, b_r = _three()  # 重置门参数\n",
    "    W_xh, W_hh, b_h = _three()  # 候选隐藏状态参数\n",
    "    # 输出层参数\n",
    "    W_hq = _one((num_hiddens, num_outputs))\n",
    "    b_q = np.zeros(num_outputs, ctx=ctx)\n",
    "    # 附上梯度\n",
    "    params = [W_xz, W_hz, b_z, W_xr, W_hr, b_r, W_xh, W_hh, b_h, W_hq, b_q]\n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def sigmoid(x):\n",
    "    s = 1 / (1 + np.exp(-x))\n",
    "    return s\n",
    "\n",
    "def init_gru_state(batch_size, num_hiddens):\n",
    "    return (np.zeros(shape=(batch_size, num_hiddens)), )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gru(inputs, state, params):\n",
    "    W_xz, W_hz, b_z, W_xr, W_hr, b_r, W_xh, W_hh, b_h, W_hq, b_q = params\n",
    "    H, = state\n",
    "    outputs = []\n",
    "    for X in inputs:\n",
    "        Z = sigmoid(np.dot(X, W_xz) + np.dot(H, W_hz) + b_z)\n",
    "        R = sigmoid(np.dot(X, W_xr) + np.dot(H, W_hr) + b_r)\n",
    "        H_tilda = nd.tanh(np.dot(X, W_xh) + np.dot(R * H, W_hh) + b_h)\n",
    "        H = Z * H + (1 - Z) * H_tilda\n",
    "        Y = np.dot(H, W_hq) + b_q\n",
    "        outputs.append(Y)\n",
    "    return outputs, (H,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# keras.layers.SimpleRNN(units, activation='tanh', use_bias=True, kernel_initializer='glorot_uniform',\n",
    "#                        recurrent_initializer='orthogonal', bias_initializer='zeros', \n",
    "#                        kernel_regularizer=None, recurrent_regularizer=None, bias_regularizer=None, \n",
    "#                        activity_regularizer=None, kernel_constraint=None, recurrent_constraint=None, \n",
    "#                        bias_constraint=None, dropout=0.0, recurrent_dropout=0.0, return_sequences=False, \n",
    "#                        return_state=False, go_backwards=False, stateful=False, unroll=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keras 接口 \n",
    "keras.layers.GRU(units, activation='tanh', recurrent_activation='hard_sigmoid', \n",
    "                 use_bias=True, kernel_initializer='glorot_uniform', \n",
    "                 recurrent_initializer='orthogonal', bias_initializer='zeros', \n",
    "                 kernel_regularizer=None, recurrent_regularizer=None, bias_regularizer=None, \n",
    "                 activity_regularizer=None, kernel_constraint=None, recurrent_constraint=None,\n",
    "                 bias_constraint=None, dropout=0.0, recurrent_dropout=0.0, implementation=1, \n",
    "                 return_sequences=False, return_state=False, go_backwards=False, stateful=False, \n",
    "                 unroll=False, reset_after=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 小结\n",
    "\n",
    "    门控循环神经网络可以更好地捕捉时间序列中时间步距离较大的依赖关系。\n",
    "    门控循环单元引入了门的概念，从而修改了循环神经网络中隐藏状态的计算方式。它包括重置门、更新门、候选隐藏状态和隐藏状态。\n",
    "    重置门有助于捕捉时间序列里短期的依赖关系。\n",
    "    更新门有助于捕捉时间序列里长期的依赖关系。\n",
    "\n",
    "#### 练习\n",
    "\n",
    "    假设时间步t′<t\n",
    "\n",
    "。如果只希望用时间步t′的输入来预测时间步t\n",
    "的输出，每个时间步的重置门和更新门的理想的值是多少？\n",
    "调节超参数，观察并分析对运行时间、困惑度以及创作歌词的结果造成的影响。\n",
    "在相同条件下，比较门控循环单元和不带门控的循环神经网络的运行时间。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
