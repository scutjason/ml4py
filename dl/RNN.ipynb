{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 语言模型\n",
    "\n",
    "其实就是语言的概率，一句话中每个词的概率，一篇文档中每个词的概率等。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RNN\n",
    "\n",
    "#### 不含隐藏状态的神经网络\n",
    "\n",
    "让我们考虑一个含单隐藏层的多层感知机。给定样本数为$n$、输入个数（特征数或特征向量维度）为$d$的小批量数据样本$\\boldsymbol{X} \\in \\mathbb{R}^{n \\times d}$。设隐藏层的激活函数为$\\phi$，那么隐藏层的输出$\\boldsymbol{H} \\in \\mathbb{R}^{n \\times h}$计算为\n",
    "\n",
    "$$\\boldsymbol{H} = \\phi(\\boldsymbol{X} \\boldsymbol{W}_{xh} + \\boldsymbol{b}_h),$$\n",
    "\n",
    "其中隐藏层权重参数$\\boldsymbol{W}_{xh} \\in \\mathbb{R}^{d \\times h}$，隐藏层偏差参数 $\\boldsymbol{b}_h \\in \\mathbb{R}^{1 \\times h}$，$h$为隐藏单元个数。上式相加的两项形状不同，因此将按照广播机制相加。把隐藏变量$\\boldsymbol{H}$作为输出层的输入，且设输出个数为$q$（如分类问题中的类别数），输出层的输出为\n",
    "\n",
    "$$\\boldsymbol{O} = \\boldsymbol{H} \\boldsymbol{W}_{hq} + \\boldsymbol{b}_q,$$\n",
    "\n",
    "其中输出变量$\\boldsymbol{O} \\in \\mathbb{R}^{n \\times q}$, 输出层权重参数$\\boldsymbol{W}_{hq} \\in \\mathbb{R}^{h \\times q}$, 输出层偏差参数$\\boldsymbol{b}_q \\in \\mathbb{R}^{1 \\times q}$。如果是分类问题，我们可以使用$\\text{softmax}(\\boldsymbol{O})$来计算输出类别的概率分布。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 含隐藏状态的循环神经网络\n",
    "\n",
    "现在我们考虑输入数据存在时间相关性的情况。假设$\\boldsymbol{X}_t \\in \\mathbb{R}^{n \\times d}$是序列中时间步$t$的小批量输入，$\\boldsymbol{H}_t  \\in \\mathbb{R}^{n \\times h}$是该时间步的隐藏变量。与多层感知机不同的是，这里我们保存上一时间步的隐藏变量$\\boldsymbol{H}_{t-1}$，并引入一个新的权重参数$\\boldsymbol{W}_{hh} \\in \\mathbb{R}^{h \\times h}$，该参数用来描述在当前时间步如何使用上一时间步的隐藏变量。具体来说，时间步$t$的隐藏变量的计算由当前时间步的输入和上一时间步的隐藏变量共同决定：\n",
    "\n",
    "$$\\boldsymbol{H}_t = \\phi(\\boldsymbol{X}_t \\boldsymbol{W}_{xh} + \\boldsymbol{H}_{t-1} \\boldsymbol{W}_{hh}  + \\boldsymbol{b}_h).$$\n",
    "\n",
    "与多层感知机相比，我们在这里添加了$\\boldsymbol{H}_{t-1} \\boldsymbol{W}_{hh}$一项。由上式中相邻时间步的隐藏变量$\\boldsymbol{H}_t$和$\\boldsymbol{H}_{t-1}$之间的关系可知，这里的隐藏变量能够捕捉截至当前时间步的序列的历史信息，就像是神经网络当前时间步的状态或记忆一样。因此，该隐藏变量也称为隐藏状态。由于隐藏状态在当前时间步的定义使用了上一时间步的隐藏状态，上式的计算是循环的。使用循环计算的网络即循环神经网络（recurrent neural network）。\n",
    "\n",
    "循环神经网络有很多种不同的构造方法。含上式所定义的隐藏状态的循环神经网络是极为常见的一种。若无特别说明，本章中的循环神经网络均基于上式中隐藏状态的循环计算。在时间步$t$，输出层的输出和多层感知机中的计算类似：\n",
    "\n",
    "$$\\boldsymbol{O}_t = \\boldsymbol{H}_t \\boldsymbol{W}_{hq} + \\boldsymbol{b}_q.$$\n",
    "\n",
    "循环神经网络的参数包括隐藏层的权重$\\boldsymbol{W}_{xh} \\in \\mathbb{R}^{d \\times h}$、$\\boldsymbol{W}_{hh} \\in \\mathbb{R}^{h \\times h}$和偏差 $\\boldsymbol{b}_h \\in \\mathbb{R}^{1 \\times h}$，以及输出层的权重$\\boldsymbol{W}_{hq} \\in \\mathbb{R}^{h \\times q}$和偏差$\\boldsymbol{b}_q \\in \\mathbb{R}^{1 \\times q}$。值得一提的是，即便在不同时间步，循环神经网络也始终使用这些模型参数。因此，循环神经网络模型参数的数量不随时间步的增加而增长。\n",
    "\n",
    "下图展示了循环神经网络在3个相邻时间步的计算逻辑。在时间步$t$，隐藏状态的计算可以看成是将输入$\\boldsymbol{X}_t$和前一时间步隐藏状态$\\boldsymbol{H}_{t-1}$连结后输入一个激活函数为$\\phi$的全连接层。该全连接层的输出就是当前时间步的隐藏状态$\\boldsymbol{H}_t$，且模型参数为$\\boldsymbol{W}_{xh}$与$\\boldsymbol{W}_{hh}$的连结，偏差为$\\boldsymbol{b}_h$。当前时间步$t$的隐藏状态$\\boldsymbol{H}_t$将参与下一个时间步$t+1$的隐藏状态$\\boldsymbol{H}_{t+1}$的计算，并输入到当前时间步的全连接输出层。\n",
    "\n",
    "![含隐藏状态的循环神经网络](./img/rnn.svg)\n",
    "\n",
    "实际上，隐藏状态中$\\boldsymbol{X}_t \\boldsymbol{W}_{xh} + \\boldsymbol{H}_{t-1} \\boldsymbol{W}_{hh}$的计算等价于$\\boldsymbol{X}_t$与$\\boldsymbol{H}_{t-1}$连结后的矩阵乘以$\\boldsymbol{W}_{xh}$与$\\boldsymbol{W}_{hh}$连结后的矩阵。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 数据集\n",
    "\n",
    "##### 周杰伦所有专辑的歌词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'想要有直升机\\n想要和你飞到宇宙去\\n想要和你融化在一起\\n融化在宇宙里\\n我每天每天每'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "import zipfile\n",
    "\n",
    "with zipfile.ZipFile('data/jaychou_lyrics.txt.zip') as zin:\n",
    "    with zin.open('jaychou_lyrics.txt') as f:\n",
    "        corpus_chars = f.read().decode('utf-8')\n",
    "corpus_chars[:40]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "63282"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(corpus_chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, '想要有直升机 想要和你飞到宇宙去 想要和你融化在一起 融化在宇宙里 我每天每天每')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 这个数据集有6万多个字符。为了打印方便，我们把换行符替换成空格，然后仅使用前1万个字符来训练模型。\n",
    "\n",
    "corpus_chars = corpus_chars.replace('\\n', ' ').replace('\\r', ' ')\n",
    "corpus_chars = corpus_chars[0:10000]\n",
    "len(corpus_chars),corpus_chars[:40]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1027"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 我们将每个字符映射成一个从0开始的连续整数，又称索引，来方便之后的数据处理。\n",
    "# 为了得到索引，我们将数据集里所有不同字符取出来，然后将其逐一映射到索引来构造词典。\n",
    "# 接着，打印vocab_size，即词典中不同字符的个数，又称词典大小。\n",
    "idx_2_char = list(set(corpus_chars))\n",
    "char_2_idx = dict([(char, i) for i ,char in enumerate(idx_2_char)])\n",
    "vocab_size = len(char_2_idx)\n",
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[949,\n",
       " 130,\n",
       " 63,\n",
       " 277,\n",
       " 327,\n",
       " 479,\n",
       " 10,\n",
       " 949,\n",
       " 130,\n",
       " 745,\n",
       " 768,\n",
       " 981,\n",
       " 647,\n",
       " 93,\n",
       " 576,\n",
       " 577,\n",
       " 10,\n",
       " 949,\n",
       " 130,\n",
       " 745]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 之后，将训练数据集中每个字符转化为索引，并打印前20个字符及其对应的索引。\n",
    "corpus_index = [char_2_idx[char] for char in corpus_chars]\n",
    "corpus_index[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 基于字符级序列的RNN模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 时序数据的采样\n",
    "\n",
    "在训练中我们需要每次随机读取小批量样本和标签。与之前章节的实验数据不同的是，时序数据的一个样本通常包含连续的字符。假设时间步数为5，样本序列为5个字符，即“想”“要”“有”“直”“升”。该样本的标签序列为这些字符分别在训练集中的下一个字符，即“要”“有”“直”“升”“机”。我们有两种方式对时序数据进行采样，分别是随机采样和相邻采样。\n",
    "\n",
    "下面的代码每次从数据里随机采样一个小批量。其中批量大小batch_size指每个小批量的样本数，num_steps为每个样本所包含的时间步数。 在随机采样中，每个样本是原始序列上任意截取的一段序列。相邻的两个随机小批量在原始序列上的位置不一定相毗邻。因此，我们无法用一个小批量最终时间步的隐藏状态来初始化下一个小批量的隐藏状态。在训练模型时，每次随机采样前都需要重新初始化隐藏状态。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "def data_iter_random(corpus_indices, batch_size, num_steps):\n",
    "    # 减1是因为输出的索引是相应输入的索引加1\n",
    "    \n",
    "    # num_steps 就是时间步数，也就是一段文字序列所包含的文字个数\n",
    "    # num_examples 将整个语料切成多少段。\n",
    "    num_examples = (len(corpus_indices) - 1) // num_steps\n",
    "    \n",
    "    # epoch_size 根据用户选择的段数（batch_size），再分成多少份，每份表示一个迭代。\n",
    "    epoch_size = num_examples // batch_size\n",
    "    example_indices = list(range(num_examples))\n",
    "    random.shuffle(example_indices)  # 打乱顺序，保证batch_size之间是随机的。\n",
    "\n",
    "    # 返回从pos开始的长为num_steps的序列\n",
    "    def _data(pos):\n",
    "        return corpus_indices[pos: pos + num_steps]\n",
    "\n",
    "    for i in range(epoch_size):\n",
    "        # 每次读取batch_size个随机样本\n",
    "        i = i * batch_size\n",
    "        batch_indices = example_indices[i: i + batch_size]\n",
    "        X = [_data(j * num_steps) for j in batch_indices]\n",
    "        Y = [_data(j * num_steps + 1) for j in batch_indices]\n",
    "        yield np.array(X), np.array(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 让我们输入一个从0到29的连续整数的人工序列。设批量大小和时间步数分别为2和6。\n",
    "# 打印随机采样每次读取的小批量样本的输入X和标签Y。\n",
    "# 可见，相邻的两个随机小批量在原始序列上的位置不一定相毗邻。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X:  [[12 13 14 15 16 17]\n",
      " [ 6  7  8  9 10 11]] \n",
      "Y: [[13 14 15 16 17 18]\n",
      " [ 7  8  9 10 11 12]] \n",
      "\n",
      "X:  [[18 19 20 21 22 23]\n",
      " [ 0  1  2  3  4  5]] \n",
      "Y: [[19 20 21 22 23 24]\n",
      " [ 1  2  3  4  5  6]] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "my_seq = list(range(30))\n",
    "for X, Y in data_iter_random(my_seq, batch_size=2, num_steps=6):\n",
    "    print('X: ', X, '\\nY:', Y, '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 相邻采样\n",
    "\n",
    "除对原始序列做随机采样之外，我们还可以令相邻的两个随机小批量在原始序列上的位置相毗邻。这时候，我们就可以用一个小批量最终时间步的隐藏状态来初始化下一个小批量的隐藏状态，从而使下一个小批量的输出也取决于当前小批量的输入，并如此循环下去。这对实现循环神经网络造成了两方面影响：一方面， 在训练模型时，我们只需在每一个迭代周期开始时初始化隐藏状态；另一方面，当多个相邻小批量通过传递隐藏状态串联起来时，模型参数的梯度计算将依赖所有串联起来的小批量序列。同一迭代周期中，随着迭代次数的增加，梯度的计算开销会越来越大。 为了使模型参数的梯度计算只依赖一次迭代读取的小批量序列，我们可以在每次读取小批量前将隐藏状态从计算图中分离出来。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def data_iter_consecutive(corpus_indices, batch_size, num_steps):\n",
    "    corpus_indices = np.array(corpus_indices)\n",
    "    data_len = len(corpus_indices)\n",
    "    batch_len = data_len // batch_size\n",
    "    # 将vocab_size平均分成batch_size份\n",
    "    indices = corpus_indices[0: batch_size*batch_len].reshape((\n",
    "        batch_size, batch_len))\n",
    "    epoch_size = (batch_len - 1) // num_steps\n",
    "    for i in range(epoch_size):\n",
    "        i = i * num_steps\n",
    "        # 取所有的batch_size行，列的话，长度就是num_steps\n",
    "        X = indices[:, i: i + num_steps]\n",
    "        Y = indices[:, i + 1: i + num_steps + 1]\n",
    "        yield X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X:  [[ 0  1  2  3  4  5]\n",
      " [15 16 17 18 19 20]] \n",
      "Y: [[ 1  2  3  4  5  6]\n",
      " [16 17 18 19 20 21]] \n",
      "\n",
      "X:  [[ 6  7  8  9 10 11]\n",
      " [21 22 23 24 25 26]] \n",
      "Y: [[ 7  8  9 10 11 12]\n",
      " [22 23 24 25 26 27]] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for X, Y in data_iter_consecutive(my_seq, batch_size=2, num_steps=6):\n",
    "    print('X: ', X, '\\nY:', Y, '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 从零实现RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "import zipfile\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# 导入歌词数据集\n",
    "\n",
    "def load_data_jay_lyrics():\n",
    "    with zipfile.ZipFile('data/jaychou_lyrics.txt.zip') as zin:\n",
    "        with zin.open('jaychou_lyrics.txt') as f:\n",
    "            corpus_chars = f.read().decode('utf-8')\n",
    "        # 去掉\\n 和 \\r\n",
    "        corpus_chars = corpus_chars.replace('\\n', ' ').replace('\\r', ' ')\n",
    "    # 只取前10000个词\n",
    "    corpus_chars = corpus_chars[0:10000]\n",
    "    \n",
    "    # 字典\n",
    "    idx_2_char = list(set(corpus_chars))\n",
    "    char_2_idx = dict([(char, i) for i ,char in enumerate(idx_2_char)])\n",
    "    # 字典大小\n",
    "    vocab_size = len(char_2_idx)\n",
    "    # 字典索引\n",
    "    corpus_index = [char_2_idx[char] for char in corpus_chars]\n",
    "    return corpus_index,char_2_idx,idx_2_char,vocab_size\n",
    "\n",
    "(corpus_indices, char_to_idx, idx_to_char, vocab_size) = load_data_jay_lyrics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 1., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# one-hot向量\n",
    "# vocab_size=100\n",
    "X = np.array([0, 2])\n",
    "\n",
    "def one_hot(X, size):\n",
    "    one_hot = []\n",
    "    try:\n",
    "        for x in X:\n",
    "            vec = np.zeros(size)\n",
    "            vec[x] = 1\n",
    "            one_hot.append(vec)\n",
    "    except:\n",
    "        vec = np.zeros(size)\n",
    "        vec[X] = 1\n",
    "        one_hot.append(vec)\n",
    "    return np.array(one_hot)\n",
    "\n",
    "one_hot(X, vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们每次采样的小批量的形状是(批量大小, 时间步数)。下面的函数将这样的小批量变换成数个可以输入进网络的形状为(批量大小, 词典大小)的矩阵，矩阵个数等于时间步数。也就是说，时间步t的输入为Xt∈Rn×d，其中n为批量大小，d为输入个数，即one-hot向量长度（词典大小）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, (2, 1027))"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def to_onehot(X, size):\n",
    "    return [one_hot(x, size) for x in X.T] # (批量大小 n, 词典大小 d)\n",
    "\n",
    "# 批量 = 2， 时间步数 = 5\n",
    "X = np.arange(10).reshape((2, 5))\n",
    "inputs = to_onehot(X, vocab_size)\n",
    "len(inputs), inputs[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 初始化模型参数\n",
    "\n",
    "隐藏单元个数 num_hiddens是一个超参数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_inputs, num_hiddens, num_outputs = vocab_size, 256, vocab_size\n",
    "\n",
    "def get_params():\n",
    "    def _one(shape):\n",
    "        return np.random.normal(scale=0.01, size=shape)\n",
    "\n",
    "    # 隐藏层参数 \n",
    "    # Ht=ϕ(XtWxh+Ht_1Whh+bh)\n",
    "    W_xh = _one((num_inputs, num_hiddens))\n",
    "    W_hh = _one((num_hiddens, num_hiddens))\n",
    "    b_h = np.zeros(num_hiddens)\n",
    "    \n",
    "    # 输出层参数\n",
    "    # Ot=HtWhq+bq\n",
    "    W_hq = _one((num_hiddens, num_outputs))\n",
    "    b_q = np.zeros(num_outputs)\n",
    "    \n",
    "    params = [W_xh, W_hh, b_h, W_hq, b_q]\n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# get_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 定义模型\n",
    "\n",
    "我们根据循环神经网络的计算表达式实现该模型。首先定义init_rnn_state函数来返回初始化的隐藏状态。它返回由一个形状为(批量大小, 隐藏单元个数)的值为0的npArray组成的元组。使用元组是为了更便于处理隐藏状态含有多个npArray的情况。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 保存隐藏层的t-1时刻状态\n",
    "def init_rnn_state(batch_size, num_hiddens):\n",
    "    return (np.zeros(shape=(batch_size, num_hiddens)), )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# rnn函数定义了在一个时间步里如何计算隐藏状态和输出。这里的激活函数使用了tanh函数。\n",
    "# 当元素在实数域上均匀分布时，tanh函数值的均值为0。\n",
    "\n",
    "def rnn(inputs, state, params):\n",
    "    # inputs和outputs皆为num_steps个形状为(batch_size, vocab_size)的矩阵\n",
    "    W_xh, W_hh, b_h, W_hq, b_q = params\n",
    "    H, = state\n",
    "    outputs = []\n",
    "    for X in inputs:\n",
    "        H = np.tanh(np.dot(X, W_xh) + np.dot(H, W_hh) + b_h)\n",
    "        Y = np.dot(H, W_hq) + b_q\n",
    "        outputs.append(Y)\n",
    "    return outputs, (H,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, (2, 1027), 1, (2, 256))"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 简单的测试来观察输出结果的个数（时间步数），以及第一个时间步的输出层输出的形状和隐藏状态的形状。\n",
    "state = init_rnn_state(X.shape[0], num_hiddens)\n",
    "inputs = to_onehot(X, vocab_size)\n",
    "params = get_params()\n",
    "outputs, state_new = rnn(inputs, state, params)\n",
    "len(outputs), outputs[0].shape, len(state_new),state_new[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 定义预测函数\n",
    "\n",
    "以下函数基于前缀prefix（含有数个字符的字符串）来预测接下来的num_chars个字符。这个函数稍显复杂，其中我们将循环神经单元rnn设置成了函数参数，这样在后面小节介绍其他循环神经网络时能重复使用这个函数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predict_rnn(prefix, num_chars, rnn, params, init_rnn_state,\n",
    "                num_hiddens, vocab_size, idx_to_char, char_to_idx):\n",
    "    state = init_rnn_state(1, num_hiddens)\n",
    "    output = [char_to_idx[prefix[0]]]\n",
    "    for t in range(num_chars + len(prefix) - 1):\n",
    "        # 将上一时间步的输出作为当前时间步的输入\n",
    "        X = to_onehot(np.array([output[-1]]), vocab_size)\n",
    "        # 计算输出和更新隐藏状态\n",
    "        (Y, state) = rnn(X, state, params)\n",
    "        # 下一个时间步的输入是prefix里的字符或者当前的最佳预测字符\n",
    "        if t < len(prefix) - 1:\n",
    "            output.append(char_to_idx[prefix[t + 1]])\n",
    "        else:\n",
    "            output.append(int(np.asscalar(Y[0].argmax(axis=1))))\n",
    "    return ''.join([idx_to_char[i] for i in output])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "(1, 1027)\n",
      "1\n",
      "(1, 1027)\n",
      "1\n",
      "(1, 1027)\n",
      "1\n",
      "(1, 1027)\n",
      "1\n",
      "(1, 1027)\n",
      "1\n",
      "(1, 1027)\n",
      "1\n",
      "(1, 1027)\n",
      "1\n",
      "(1, 1027)\n",
      "1\n",
      "(1, 1027)\n",
      "1\n",
      "(1, 1027)\n",
      "1\n",
      "(1, 1027)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'分开天期相十碎痛入轮银长'"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 测试一下predict_rnn函数。\n",
    "# 我们将根据前缀“分开”创作长度为10个字符（不考虑前缀长度）的一段歌词。\n",
    "#因为模型参数为随机值，所以预测结果也是随机的。\n",
    "predict_rnn('分开', 10, rnn, params, init_rnn_state, num_hiddens, vocab_size, idx_2_char, char_2_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 裁剪梯度\n",
    "\n",
    "循环神经网络中较容易出现梯度衰减或梯度爆炸。为了应对梯度爆炸，我们可以裁剪梯度（clip gradient）。假设我们把所有模型参数梯度的元素拼接成一个向量 g，并设裁剪的阈值是θ。裁剪后的梯度\n",
    "\n",
    "$\\min\\left(\\frac{\\theta}{\\|\\boldsymbol{g}\\|}, 1\\right)\\boldsymbol{g}$\n",
    "\n",
    "的L2范数不超过θ。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def grad_clipping(params, theta):\n",
    "    norm = np.array([0])\n",
    "    for param in params:\n",
    "        norm += (param.grad ** 2).sum()\n",
    "    norm = np.asscalar(norm.sqrt())\n",
    "    if norm > theta:\n",
    "        for param in params:\n",
    "            param.grad[:] *= theta / norm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 困惑度\n",
    "\n",
    "我们通常使用困惑度（perplexity）来评价语言模型的好坏。回忆一下“softmax回归”一节中交叉熵损失函数的定义。困惑度是对交叉熵损失函数做指数运算后得到的值。特别地，\n",
    "\n",
    "    最佳情况下，模型总是把标签类别的概率预测为1，此时困惑度为1；\n",
    "    最坏情况下，模型总是把标签类别的概率预测为0，此时困惑度为正无穷；\n",
    "    基线情况下，模型总是预测所有类别的概率都相同，此时困惑度为类别个数。 （均匀分布）\n",
    "\n",
    "显然，任何一个有效模型的困惑度必须小于类别个数。在本例中，困惑度必须小于词典大小vocab_size。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 定义模型训练函数\n",
    "\n",
    "跟之前章节的模型训练函数相比，这里的模型训练函数有以下几点不同：\n",
    "\n",
    "    使用困惑度评价模型。\n",
    "    在迭代模型参数前裁剪梯度。\n",
    "    对时序数据采用不同采样方法将导致隐藏状态初始化的不同。\n",
    "\n",
    "另外，考虑到后面将介绍的其他循环神经网络，为了更通用，这里的函数实现更长一些。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "import time\n",
    "import keras.losses.categorical_crossentropy\n",
    "\n",
    "def train_and_predict_rnn(rnn, get_params, init_rnn_state, num_hiddens,\n",
    "                          vocab_size, corpus_indices, idx_to_char,\n",
    "                          char_to_idx, is_random_iter, num_epochs, num_steps,\n",
    "                          lr, clipping_theta, batch_size, pred_period,\n",
    "                          pred_len, prefixes):\n",
    "    if is_random_iter:\n",
    "        # 随机采样\n",
    "        data_iter_fn = data_iter_random\n",
    "    else:\n",
    "        # 均匀采样\n",
    "        data_iter_fn = data_iter_consecutive\n",
    "    params = get_params()\n",
    "    loss = categorical_crossentropy()\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        # 如使用相邻采样，在epoch开始时初始化隐藏状态\n",
    "        # 注意，num_epochs是指整个模型训练时的迭代次数，不是时序采样的epoch\n",
    "        if not is_random_iter:  \n",
    "            state = init_rnn_state(batch_size, num_hiddens)\n",
    "        l_sum, n, start = 0.0, 0, time.time()\n",
    "        data_iter = data_iter_fn(corpus_indices, batch_size, num_steps)\n",
    "        for X, Y in data_iter:\n",
    "            # 如使用随机采样，在每个小批量更新前初始化隐藏状态\n",
    "            # 为什么要每次batch_size就要更新呢？ 因为批量之间的序列是不连续的，所以隐藏层的状态用不了\n",
    "            if is_random_iter:  \n",
    "                state = init_rnn_state(batch_size, num_hiddens)\n",
    "            else:  # 否则需要使用detach函数从计算图分离隐藏状态\n",
    "                for s in state:\n",
    "                    s.detach()\n",
    "            with autograd.record():\n",
    "                inputs = to_onehot(X, vocab_size)\n",
    "                # outputs有num_steps个形状为(batch_size, vocab_size)的矩阵\n",
    "                (outputs, state) = rnn(inputs, state, params)\n",
    "                # 拼接之后形状为(num_steps * batch_size, vocab_size)\n",
    "                outputs = nd.concat(*outputs, dim=0)\n",
    "                # Y的形状是(batch_size, num_steps)，转置后再变成长度为\n",
    "                # batch * num_steps 的向量，这样跟输出的行一一对应\n",
    "                y = Y.T.reshape((-1,))\n",
    "                # 使用交叉熵损失计算平均分类误差\n",
    "                l = loss(outputs, y).mean()\n",
    "            l.backward()\n",
    "            grad_clipping(params, clipping_theta)  # 裁剪梯度\n",
    "            d2l.sgd(params, lr, 1)  # 因为误差已经取过均值，梯度不用再做平均\n",
    "            l_sum += l.asscalar() * y.size\n",
    "            n += y.size\n",
    "\n",
    "        if (epoch + 1) % pred_period == 0:\n",
    "            print('epoch %d, perplexity %f, time %.2f sec' % (\n",
    "                epoch + 1, math.exp(l_sum / n), time.time() - start))\n",
    "            for prefix in prefixes:\n",
    "                print(' -', predict_rnn(\n",
    "                    prefix, pred_len, rnn, params, init_rnn_state,\n",
    "                    num_hiddens, vocab_size, idx_to_char, char_to_idx))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RNN的损失函数\n",
    "\n",
    "RNN的损失函数使用交叉熵，RNN输出的激活函数为softmax函数，隐藏层的激活函数为tanh函数。\n",
    "<img src=\"img/2019-08-14_114820.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RNN的梯度  BPTT\n",
    "\n",
    "在RNN网络中，权重矩阵从一个时间步长被传递到下一个时间步长。在反向传播的过程中，计算损失函数在t时刻的梯度需要将从0时刻到t时刻的所有梯度相加，而越早时刻的梯度越可能会出现梯度弥散或者梯度爆炸的现象。BRTT和DNN也有很大的不同点，即这里所有的U,W,V,b,c在序列的各个位置是共享的，反向传播时我们更新的是相同的参数。\n",
    "\n",
    "损失是交叉熵，对于RNN，由于我们在序列的每个位置都有损失函数，因此最终的损失L为：\n",
    "\n",
    "$\\begin{aligned}E_t(y_t, \\hat{y}_t) &= - y_{t} \\log \\hat{y}_{t} \\\\ E(y, \\hat{y}) &=\\sum\\limits_{t} E_t(y_t,\\hat{y}_t) \\\\ & = -\\sum\\limits_{t} y_{t} \\log \\hat{y}_{t} \\end{aligned}$\n",
    "\n",
    "$L = \\sum\\limits_{t=1}^{\\tau}L^{(t)}$\n",
    "\n",
    "\n",
    "其中 yt 是真实值， (^yt) 是预估值，将误差展开可以用图表示为：\n",
    "\n",
    "<img src=\"img/rnn-bptt1-e1461597894645.png\">\n",
    "\n",
    "$x_0,x_1,x_2,x_3,x_4$可以理解为词序列。\n",
    "\n",
    "当然这里还有batch_size的概念，以下我们假设batch_size =1。\n",
    "\n",
    "    W 是 [num_hidden, num_hidden] 隐藏节点的状态转移矩阵（t_0 -> t_1）。\n",
    "    U 是 [input_dim, num_hidden]  输入层到隐藏层矩阵\n",
    "    V 是 [input_dim, num_hidden]  隐藏层到输出层矩阵\n",
    "    \n",
    "参数有：W、U、V、b、c   b是输入->隐藏层，c是隐藏层-输出的偏差\n",
    "\n",
    "先求c和V，因为简单，为啥呢？不涉及到循环。\n",
    "\n",
    "$\\frac{\\partial L}{\\partial c} = \\sum\\limits_{t=1}^{\\tau}\\frac{\\partial L^{(t)}}{\\partial c}  = \\sum\\limits_{t=1}^{\\tau}\\hat{y}^{(t)} - y^{(t)}$\n",
    "\n",
    "$\\frac{\\partial L}{\\partial V} =\\sum\\limits_{t=1}^{\\tau}\\frac{\\partial L^{(t)}}{\\partial V}  = \\sum\\limits_{t=1}^{\\tau}(\\hat{y}^{(t)} - y^{(t)}) (h^{(t)})^T$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$h^{(t)} = \\sigma(z^{(t)}) = \\sigma(Ux^{(t)} + Wh^{(t-1)} +b )$\n",
    "\n",
    "其中σ为RNN的激活函数，一般为tanh, b为线性关系的偏倚。\n",
    "\n",
    "序列索引号t时模型的输出o(t)的表达式比较简单：\n",
    "$o^{(t)} = Vh^{(t)} +c$\n",
    "\n",
    "在最终在序列索引号t时我们的预测输出为:\n",
    "$\\hat{y}^{(t)} = \\sigma(o^{(t)})$\n",
    "\n",
    "通常由于RNN是识别类的分类模型，所以上面这个激活函数一般是softmax。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "但是W,U,b的梯度计算就比较的复杂了。从RNN的模型可以看出，在反向传播时，在在某一序列位置t的梯度损失由当前位置的输出对应的梯度损失和序列索引位置t+1时的梯度损失两部分共同决定。对于W在某一序列位置t的梯度损失需要反向传播一步步的计算。我们定义序列索引t位置的隐藏状态的梯度为"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/2019-08-14_182857.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### numpy实现RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import itertools\n",
    "import operator\n",
    "import numpy as np\n",
    "import nltk\n",
    "import sys\n",
    "from datetime import datetime\n",
    "from utils import *\n",
    "from collections import Counter\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "# %matplotlib inline\n",
    "\n",
    "# Download NLTK model data (you need to do this once)\n",
    "# nltk.downxload(\"book\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 准备数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading CSV file...\n",
      "Parsed 15001 sentences.\n",
      "Found 117581 unique words tokens.\n",
      "index_to_word top 10 ['the', 'to', 'a', 'and', 'I', 'of', 'is', 'you', 'in', 'that']\n",
      "Using vocabulary size 8000.\n",
      "The least frequent word in our vocabulary is 'drastically' and appeared 11 times.\n",
      "\n",
      "Example sentence: 'SENTENCE_START body SENTENCE_END'\n",
      "\n",
      "Example sentence after Pre-processing: '['SENTENCE_START', 'body', 'SENTENCE_END']'\n"
     ]
    }
   ],
   "source": [
    "vocabulary_size = 8000\n",
    "unknown_token = \"UNKNOWN_TOKEN\"\n",
    "sentence_start_token = \"SENTENCE_START\"\n",
    "sentence_end_token = \"SENTENCE_END\"\n",
    "\n",
    "# Read the data and append SENTENCE_START and SENTENCE_END tokens\n",
    "print (\"Reading CSV file...\")\n",
    "with open('data/reddit-comments-2015-08.csv', 'r', encoding=\"utf-8\") as f:\n",
    "    reader = csv.reader(f, skipinitialspace=True)\n",
    "#     reader.next()\n",
    "    # Split full comments into sentences\n",
    "    sentences=[ x[0].strip() for x in reader]\n",
    "    # Append SENTENCE_START and SENTENCE_END\n",
    "    sentences = [\"%s %s %s\" % (sentence_start_token, x, sentence_end_token) for x in sentences]\n",
    "print(\"Parsed %d sentences.\" % (len(sentences)))\n",
    "    \n",
    "# Tokenize the sentences into words\n",
    "tokenized_sentences = [sent.split() for sent in sentences]\n",
    "\n",
    "# Count the word frequencies\n",
    "word_freq =  Counter()\n",
    "for s in sentences:\n",
    "    words = s.split()\n",
    "    if len(words) == 0:continue\n",
    "    for word in words :\n",
    "        word_freq[word] += 1\n",
    "print(\"Found %d unique words tokens.\" % len(word_freq.items()))\n",
    "\n",
    "# Get the most common words and build index_to_word and word_to_index vectors\n",
    "vocab = word_freq.most_common(vocabulary_size-1)\n",
    "index_to_word = [x[0] for x in vocab]\n",
    "index_to_word.append(unknown_token)\n",
    "word_to_index = dict([(w,i) for i,w in enumerate(index_to_word)])\n",
    "print( \"index_to_word top 10\", index_to_word[0:10])\n",
    "# print( \"迈向:\", word_to_index[\"迈向\"])\n",
    "print( \"Using vocabulary size %d.\" % vocabulary_size)\n",
    "print( \"The least frequent word in our vocabulary is '%s' and appeared %d times.\" % (vocab[-1][0], vocab[-1][1]))\n",
    "\n",
    "# Replace all words not in our vocabulary with the unknown token\n",
    "for i, sent in enumerate(tokenized_sentences):\n",
    "    tokenized_sentences[i] = [w if w in word_to_index else unknown_token for w in sent]\n",
    "\n",
    "print( \"\\nExample sentence: '%s'\" % sentences[0])\n",
    "print( \"\\nExample sentence after Pre-processing: '%s'\" % tokenized_sentences[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create the training data\n",
    "X_train = np.asarray([[word_to_index[w] for w in sent[:-1]] for sent in tokenized_sentences])\n",
    "y_train = np.asarray([[word_to_index[w] for w in sent[1:]] for sent in tokenized_sentences])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x:\n",
      "SENTENCE_START I think UNKNOWN_TOKEN is one of the most interesting early sound UNKNOWN_TOKEN UNKNOWN_TOKEN UNKNOWN_TOKEN really made incredible use of sound, especially for a director who had UNKNOWN_TOKEN done silent films at the time. Much of the film is completely silent which adds to the UNKNOWN_TOKEN for me and the anxiety that the film UNKNOWN_TOKEN There's never really a wasted sound UNKNOWN_TOKEN every line of dialogue is extremely important. Even Peter UNKNOWN_TOKEN UNKNOWN_TOKEN is UNKNOWN_TOKEN to the UNKNOWN_TOKEN\n",
      "[10, 4, 56, 7999, 6, 53, 5, 0, 107, 653, 567, 597, 7999, 7999, 7999, 65, 166, 3871, 117, 5, 6048, 457, 13, 2, 5719, 71, 67, 7999, 291, 5417, 4329, 31, 0, 287, 4167, 5, 0, 1692, 6, 347, 5417, 89, 1596, 1, 0, 7999, 13, 59, 3, 0, 4015, 9, 0, 1692, 7999, 454, 130, 65, 2, 4168, 597, 7999, 153, 509, 5, 7334, 6, 780, 2932, 426, 7850, 7999, 7999, 6, 7999, 1, 0, 7999]\n",
      "\n",
      "y:\n",
      "I think UNKNOWN_TOKEN is one of the most interesting early sound UNKNOWN_TOKEN UNKNOWN_TOKEN UNKNOWN_TOKEN really made incredible use of sound, especially for a director who had UNKNOWN_TOKEN done silent films at the time. Much of the film is completely silent which adds to the UNKNOWN_TOKEN for me and the anxiety that the film UNKNOWN_TOKEN There's never really a wasted sound UNKNOWN_TOKEN every line of dialogue is extremely important. Even Peter UNKNOWN_TOKEN UNKNOWN_TOKEN is UNKNOWN_TOKEN to the UNKNOWN_TOKEN SENTENCE_END\n",
      "[4, 56, 7999, 6, 53, 5, 0, 107, 653, 567, 597, 7999, 7999, 7999, 65, 166, 3871, 117, 5, 6048, 457, 13, 2, 5719, 71, 67, 7999, 291, 5417, 4329, 31, 0, 287, 4167, 5, 0, 1692, 6, 347, 5417, 89, 1596, 1, 0, 7999, 13, 59, 3, 0, 4015, 9, 0, 1692, 7999, 454, 130, 65, 2, 4168, 597, 7999, 153, 509, 5, 7334, 6, 780, 2932, 426, 7850, 7999, 7999, 6, 7999, 1, 0, 7999, 11]\n"
     ]
    }
   ],
   "source": [
    "# Print an training data example\n",
    "x_example, y_example = X_train[17], y_train[17]\n",
    "print( \"x:\\n%s\\n%s\" % (\" \".join([index_to_word[x] for x in x_example]), x_example))\n",
    "print( \"\\ny:\\n%s\\n%s\" % (\" \".join([index_to_word[x] for x in y_example]), y_example))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 创建RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    return np.exp(x)/np.sum(np.exp(x),axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class RNNNumpy:\n",
    "    \n",
    "    def __init__(self, word_dim, hidden_dim=100, bptt_truncate=4):\n",
    "        # Assign instance variables\n",
    "        self.word_dim = word_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.bptt_truncate = bptt_truncate\n",
    "        # Randomly initialize the network parameters\n",
    "        self.U = np.random.uniform(-np.sqrt(1./word_dim), np.sqrt(1./word_dim), (hidden_dim, word_dim))\n",
    "        self.V = np.random.uniform(-np.sqrt(1./hidden_dim), np.sqrt(1./hidden_dim), (word_dim, hidden_dim))\n",
    "        self.W = np.random.uniform(-np.sqrt(1./hidden_dim), np.sqrt(1./hidden_dim), (hidden_dim, hidden_dim))\n",
    "\n",
    "    def forward_propagation(self, x):\n",
    "        # The total number of time steps\n",
    "        T = len(x)\n",
    "        # During forward propagation we save all hidden states in s because need them later.\n",
    "        # We add one additional element for the initial hidden, which we set to 0\n",
    "        s = np.zeros((T + 1, self.hidden_dim))\n",
    "        s[-1] = np.zeros(self.hidden_dim)\n",
    "        # The outputs at each time step. Again, we save them for later.\n",
    "        o = np.zeros((T, self.word_dim))\n",
    "        # For each time step...\n",
    "        for t in np.arange(T):\n",
    "            # Note that we are indxing U by x[t]. This is the same as multiplying U with a one-hot vector.\n",
    "            s[t] = np.tanh(self.U[:,x[t]] + self.W.dot(s[t-1]))\n",
    "            o[t] = softmax(self.V.dot(s[t]))\n",
    "        return [o, s]\n",
    "\n",
    "# RNNNumpy.forward_propagation = forward_propagation\n",
    "\n",
    "    def predict(self, x):\n",
    "        # Perform forward propagation and return index of the highest score\n",
    "        o, s = self.forward_propagation(x)\n",
    "        return np.argmax(o, axis=1)\n",
    "\n",
    "    # RNNNumpy.predict = predict\n",
    "    def calculate_total_loss(self, x, y):\n",
    "        L = 0\n",
    "        # For each sentence...\n",
    "        for i in np.arange(len(y)):\n",
    "            o, s = self.forward_propagation(x[i])\n",
    "            # We only care about our prediction of the \"correct\" words   \n",
    "\n",
    "            # yn 是 one-hot向量\n",
    "            # 所以L 只需要去对应1位置上的概率即可。\n",
    "            correct_word_predictions = o[np.arange(len(y[i])), y[i]]\n",
    "            # Add to the loss based on how off we were\n",
    "            L += -1 * np.sum(np.log(correct_word_predictions))\n",
    "        return L\n",
    "\n",
    "    def calculate_loss(self, x, y):\n",
    "        # Divide the total loss by the number of training examples\n",
    "        N = np.sum((len(y_i) for y_i in y))\n",
    "        return self.calculate_total_loss(x,y)/N\n",
    "\n",
    "    def bptt(self, x, y):\n",
    "        T = len(y)\n",
    "        # Perform forward propagation\n",
    "        o, s = self.forward_propagation(x)\n",
    "        # We accumulate the gradients in these variables\n",
    "        dLdU = np.zeros(self.U.shape)\n",
    "        dLdV = np.zeros(self.V.shape)\n",
    "        dLdW = np.zeros(self.W.shape)\n",
    "        delta_o = o\n",
    "        # 一次性全部取出所有y对应的y_hat，然后-1\n",
    "        delta_o[np.arange(len(y)), y] -= 1.\n",
    "        # For each output backwards...\n",
    "        for t in np.arange(T)[::-1]:\n",
    "            dLdV += np.outer(delta_o[t], s[t].T)\n",
    "            # Initial delta calculation\n",
    "            delta_t = self.V.T.dot(delta_o[t]) * (1 - (s[t] ** 2))\n",
    "            # Backpropagation through time (for at most self.bptt_truncate steps)\n",
    "            for bptt_step in np.arange(max(0, t-self.bptt_truncate), t+1)[::-1]:\n",
    "                # print \"Backpropagation step t=%d bptt step=%d \" % (t, bptt_step)\n",
    "                dLdW += np.outer(delta_t, s[bptt_step-1])              \n",
    "                dLdU[:,x[bptt_step]] += delta_t\n",
    "                # Update delta for next step\n",
    "                delta_t = self.W.T.dot(delta_t) * (1 - s[bptt_step-1] ** 2)\n",
    "        return [dLdU, dLdV, dLdW]\n",
    "    \n",
    "    # 防止梯度弥散\n",
    "    def gradient_check(self, x, y, h=0.001, error_threshold=0.01):\n",
    "        # Calculate the gradients using backpropagation. We want to checker if these are correct.\n",
    "        bptt_gradients = self.bptt(x, y)\n",
    "        # List of all parameters we want to check.\n",
    "        model_parameters = ['U', 'V', 'W']\n",
    "        # Gradient check for each parameter\n",
    "        for pidx, pname in enumerate(model_parameters):\n",
    "            # Get the actual parameter value from the mode, e.g. model.W\n",
    "            parameter = operator.attrgetter(pname)(self)\n",
    "            print \"Performing gradient check for parameter %s with size %d.\" % (pname, np.prod(parameter.shape))\n",
    "            # Iterate over each element of the parameter matrix, e.g. (0,0), (0,1), ...\n",
    "            it = np.nditer(parameter, flags=['multi_index'], op_flags=['readwrite'])\n",
    "            while not it.finished:\n",
    "                ix = it.multi_index\n",
    "                # Save the original value so we can reset it later\n",
    "                original_value = parameter[ix]\n",
    "                # Estimate the gradient using (f(x+h) - f(x-h))/(2*h)\n",
    "                parameter[ix] = original_value + h\n",
    "                gradplus = model.calculate_total_loss([x],[y])\n",
    "                parameter[ix] = original_value - h\n",
    "                gradminus = model.calculate_total_loss([x],[y])\n",
    "                estimated_gradient = (gradplus - gradminus)/(2*h)\n",
    "                # Reset parameter to original value\n",
    "                parameter[ix] = original_value\n",
    "                # The gradient for this parameter calculated using backpropagation\n",
    "                backprop_gradient = bptt_gradients[pidx][ix]\n",
    "                # calculate The relative error: (|x - y|/(|x| + |y|))\n",
    "                relative_error = np.abs(backprop_gradient - estimated_gradient)/(np.abs(backprop_gradient) + np.abs(estimated_gradient))\n",
    "                # If the error is to large fail the gradient check\n",
    "                if relative_error > error_threshold:\n",
    "                    print \"Gradient Check ERROR: parameter=%s ix=%s\" % (pname, ix)\n",
    "                    print \"+h Loss: %f\" % gradplus\n",
    "                    print \"-h Loss: %f\" % gradminus\n",
    "                    print \"Estimated_gradient: %f\" % estimated_gradient\n",
    "                    print \"Backpropagation gradient: %f\" % backprop_gradient\n",
    "                    print \"Relative Error: %f\" % relative_error\n",
    "                    return \n",
    "                it.iternext()\n",
    "            print \"Gradient check for parameter %s passed.\" % (pname)\n",
    "\n",
    "    # Performs one step of SGD.\n",
    "    def numpy_sdg_step(self, x, y, learning_rate):\n",
    "        # Calculate the gradients\n",
    "        dLdU, dLdV, dLdW = self.bptt(x, y)\n",
    "        # Change parameters according to gradients and learning rate\n",
    "        self.U -= learning_rate * dLdU\n",
    "        self.V -= learning_rate * dLdV\n",
    "        self.W -= learning_rate * dLdW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "下面我们将讨论一些关于一维数组的乘法的问题\n",
      "*: [ 2  6 12]\n",
      "np.dot(): 20\n",
      "np.multiply(): [ 2  6 12]\n",
      "np.outer(): [[ 2  3  4]\n",
      " [ 4  6  8]\n",
      " [ 6  9 12]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    " \n",
    "print(\"下面我们将讨论一些关于一维数组的乘法的问题\")\n",
    "A=np.array([1,2,3])\n",
    "B=np.array([2,3,4])\n",
    "c=[1,2,3]\n",
    "print(\"*:\",A*B)#对数组执行的是对应位置元素相乘\n",
    "print(\"np.dot():\",np.dot(A,B))#当dot遇到佚为1，执行按位乘并相加\n",
    "print(\"np.multiply():\",np.multiply(A,B))#对数组执行的是对应位置的元素相乘\n",
    "print(\"np.outer():\",np.outer(A,B))#A的一个元素和B的元素相乘的到结果的一行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "43\n",
      "(43, 8000)\n",
      "[[0.00012515 0.00012471 0.00012528 ... 0.00012576 0.00012413 0.0001239 ]\n",
      " [0.00012522 0.0001245  0.00012627 ... 0.00012484 0.00012424 0.00012467]\n",
      " [0.0001247  0.00012517 0.00012424 ... 0.00012478 0.00012466 0.00012511]\n",
      " ...\n",
      " [0.00012421 0.00012493 0.00012506 ... 0.00012499 0.00012491 0.00012363]\n",
      " [0.00012615 0.00012495 0.00012454 ... 0.00012503 0.00012509 0.00012388]\n",
      " [0.00012466 0.000125   0.00012489 ... 0.00012541 0.00012528 0.00012475]]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(10)\n",
    "model = RNNNumpy(vocabulary_size)\n",
    "o, s = model.forward_propagation(X_train[10])\n",
    "print(len(X_train[10]))\n",
    "print( o.shape)\n",
    "print( o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(43,)\n"
     ]
    }
   ],
   "source": [
    "predictions = model.predict(X_train[10])\n",
    "print( predictions.shape)\n",
    "# for pred in predictions:\n",
    "#     print(index_to_word[pred])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected Loss for random predictions: 8.987197\n",
      "Actual loss: 8.987130\n"
     ]
    }
   ],
   "source": [
    "# Limit to 1000 examples to save time\n",
    "# 对于整个语料集而言，假设词典有C个词, 因此每个词的概率都是 1/C\n",
    "# 所以损失函数 L = -1/N * N * log(1/C) = log(C):\n",
    "print( \"Expected Loss for random predictions: %f\" % np.log(vocabulary_size))\n",
    "print( \"Actual loss: %f\" % model.calculate_loss(X_train[:1000], y_train[:1000]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# To avoid performing millions of expensive calculations we use a smaller vocabulary size for checking.\n",
    "grad_check_vocab_size = 100\n",
    "np.random.seed(10)\n",
    "model = RNNNumpy(grad_check_vocab_size, 10, bptt_truncate=1000)\n",
    "model.gradient_check([0,1,2,3], [1,2,3,4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Outer SGD Loop\n",
    "# - model: The RNN model instance\n",
    "# - X_train: The training data set\n",
    "# - y_train: The training data labels\n",
    "# - learning_rate: Initial learning rate for SGD\n",
    "# - nepoch: Number of times to iterate through the complete dataset\n",
    "# - evaluate_loss_after: Evaluate the loss after this many epochs\n",
    "def train_with_sgd(model, X_train, y_train, learning_rate=0.005, nepoch=100, evaluate_loss_after=5):\n",
    "    # We keep track of the losses so we can plot them later\n",
    "    losses = []\n",
    "    num_examples_seen = 0\n",
    "    for epoch in range(nepoch):\n",
    "        # Optionally evaluate the loss\n",
    "        if (epoch % evaluate_loss_after == 0):\n",
    "            loss = model.calculate_loss(X_train, y_train)\n",
    "            losses.append((num_examples_seen, loss))\n",
    "            time = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "            print \"%s: Loss after num_examples_seen=%d epoch=%d: %f\" % (time, num_examples_seen, epoch, loss)\n",
    "            # Adjust the learning rate if loss increases\n",
    "            if (len(losses) > 1 and losses[-1][1] > losses[-2][1]):\n",
    "                learning_rate = learning_rate * 0.5  \n",
    "                print \"Setting learning rate to %f\" % learning_rate\n",
    "            sys.stdout.flush()\n",
    "        # For each training example...\n",
    "        for i in range(len(y_train)):\n",
    "            # One SGD step\n",
    "            model.sgd_step(X_train[i], y_train[i], learning_rate)\n",
    "            num_examples_seen += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 完成，开始训练:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(10)\n",
    "model = RNNNumpy(vocabulary_size)\n",
    "%timeit model.sgd_step(X_train[10], y_train[10], 0.005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(10)\n",
    "# Train on a small subset of the data to see what happens\n",
    "model = RNNNumpy(vocabulary_size)\n",
    "losses = train_with_sgd(model, X_train[:100], y_train[:100], nepoch=10, evaluate_loss_after=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 生成文本\n",
    "\n",
    "生成过程其实就是模型的应用过程，只需要反复执行预测函数即可："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-220-58dbd58ed662>, line 22)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-220-58dbd58ed662>\"\u001b[1;36m, line \u001b[1;32m22\u001b[0m\n\u001b[1;33m    while len(sent) &lt; senten_min_length:\u001b[0m\n\u001b[1;37m                       ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "def generate_sentence(model):\n",
    "    # We start the sentence with the start token\n",
    "    new_sentence = [word_to_index[sentence_start_token]]\n",
    "    # Repeat until we get an end token\n",
    "    while not new_sentence[-1] == word_to_index[sentence_end_token]:\n",
    "        next_word_probs = model.forward_propagation(new_sentence)\n",
    "        sampled_word = word_to_index[unknown_token]\n",
    "        # We don't want to sample unknown words\n",
    "        while sampled_word == word_to_index[unknown_token]:\n",
    "            samples = np.random.multinomial(1, next_word_probs[-1])\n",
    "            sampled_word = np.argmax(samples)\n",
    "        new_sentence.append(sampled_word)\n",
    "    sentence_str = [index_to_word[x] for x in new_sentence[1:-1]]\n",
    "    return sentence_str\n",
    " \n",
    "num_sentences = 10\n",
    "senten_min_length = 7\n",
    " \n",
    "for i in range(num_sentences):\n",
    "    sent = []\n",
    "    # We want long sentences, not sentences with one or two words\n",
    "    while len(sent) &lt; senten_min_length:\n",
    "        sent = generate_sentence(model)\n",
    "    print \" \".join(sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### keras 实现RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.layers import Dense, Activation\n",
    "from keras.layers.recurrent import SimpleRNN\n",
    "from keras.models import Sequential\n",
    "\n",
    "import numpy as np\n",
    "import math\n",
    "import random\n",
    "import zipfile\n",
    "\n",
    "\n",
    "# 导入歌词数据集\n",
    "def load_data_jay_lyrics():\n",
    "    with zipfile.ZipFile('data/jaychou_lyrics.txt.zip') as zin:\n",
    "        with zin.open('jaychou_lyrics.txt') as f:\n",
    "            corpus_chars = f.read().decode('utf-8')\n",
    "        # 去掉\\n 和 \\r\n",
    "        corpus_chars = corpus_chars.replace('\\n', ' ').replace('\\r', ' ')\n",
    "    # 只取前10000个词\n",
    "    corpus_chars = corpus_chars[0:10000]\n",
    "    \n",
    "    # 字典\n",
    "    idx_2_char = list(set(corpus_chars))\n",
    "    char_2_idx = dict([(char, i) for i ,char in enumerate(idx_2_char)])\n",
    "    # 字典大小\n",
    "    vocab_size = len(char_2_idx)\n",
    "    # 字典索引\n",
    "    corpus_index = [char_2_idx[char] for char in corpus_chars]\n",
    "    return corpus_index,char_2_idx,idx_2_char,vocab_size\n",
    "\n",
    "(corpus_indices, char_to_idx, idx_to_char, vocab_size) = load_data_jay_lyrics()\n",
    "\n",
    "# 定义模型\n",
    "# 一个含单隐藏层、隐藏单元个数为256的循环神经网络层rnn_layer，并对权重做初始化。\n",
    "num_hiddens = 256\n",
    "num_epochs, batch_size, lr, clipping_theta = 250, 32, 1e2, 1e-2\n",
    "num_steps = 35\n",
    "\n",
    "model = Sequential()\n",
    "# model.add(SimpleRNN(32, return_sequences=True))\n",
    "model.add(SimpleRNN(num_hiddens, return_sequences=False,\n",
    "                    input_shape=(num_steps, vocab_size), unroll=True))\n",
    "model.add(Dense(vocab_size))\n",
    "model.add(Activation(\"softmax\"))\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=\"rmsprop\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 35.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import time\n",
    "\n",
    "# 训练和预测\n",
    "NUM_EPOCHS_PER_ITERATION=1\n",
    "pred_period=50\n",
    "prefixes = ['分开', '不分开']\n",
    "is_random_iter = False\n",
    "num_chars = 50\n",
    "if is_random_iter:\n",
    "    data_iter_fn = data_iter_random\n",
    "else:\n",
    "    data_iter_fn = data_iter_consecutive\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    l_sum, n, start = 0.0, 0, time.time()\n",
    "    data_iter = data_iter_fn(corpus_indices, batch_size, num_steps)\n",
    "    for X, Y in data_iter:\n",
    "        X = to_onehot(X, vocab_size)\n",
    "        Y = to_onehot(Y, vocab_size)\n",
    "        model.fit(X, Y, batch_size=batch_size, epochs=NUM_EPOCHS_PER_ITERATION)\n",
    "    if (epoch + 1) % pred_period == 0:\n",
    "#         print('epoch %d, perplexity %f, time %.2f sec' % (\n",
    "#             epoch + 1, math.exp(l_sum / n), time.time() - start))\n",
    "        for prefix in prefixes:\n",
    "            output = [char_to_idx[prefix[0]]]\n",
    "            for t in range(num_chars + len(prefix) - 1):\n",
    "                X = to_onehot(np.array([output[-1]]), vocab_size)\n",
    "                pred = model.predict(X, verbose=0)[0]\n",
    "                if t < len(prefix) - 1:\n",
    "                    output.append(char_to_idx[prefix[t + 1]])\n",
    "                else:\n",
    "                    output.append(np.argmax(pred))\n",
    "\n",
    "            print(' -', ''.join([idx_to_char[i] for i in output]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 先看其他博客实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2791, 59, 161803)"
      ]
     },
     "execution_count": 378,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.layers import Dense, Activation\n",
    "from keras.layers.recurrent import SimpleRNN, LSTM, GRU\n",
    "from keras.models import Sequential\n",
    "\n",
    "\n",
    "# Read lines from an example source file.\n",
    "with open(\"data/alice_in_wonderland.txt\", 'r', encoding='utf-8') as _in:\n",
    "    lines = []\n",
    "    for line in _in:\n",
    "        line = line.strip().lower()\n",
    "        if len(line) == 0:\n",
    "            continue\n",
    "        lines.append(line)\n",
    "text = \" \".join(lines)\n",
    "chars = set([c for c in text])\n",
    "nb_chars = len(chars)  # vocab_size\n",
    "len(lines),nb_chars,len(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create a character index and reverse mapping to go between a numerical\n",
    "# ID and a specific character. The numerical ID will correspond to a column\n",
    "# number when using a one-hot encoded representation of character inputs.\n",
    "char2index = {c: i for i, c in enumerate(chars)}\n",
    "index2char = {i: c for i, c in enumerate(chars)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['project gu',\n",
       "  'roject gut',\n",
       "  'oject gute',\n",
       "  'ject guten',\n",
       "  'ect gutenb',\n",
       "  'ct gutenbe',\n",
       "  't gutenber',\n",
       "  ' gutenberg',\n",
       "  'gutenberg’',\n",
       "  'utenberg’s'],\n",
       " ['t', 'e', 'n', 'b', 'e', 'r', 'g', '’', 's', ' '],\n",
       " 161793)"
      ]
     },
     "execution_count": 340,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# For convenience, choose a fixed sequence length of 10 characters.\n",
    "SEQLEN, STEP = 10, 1  # num_steps\n",
    "input_chars, label_chars = [], []\n",
    "\n",
    "# Convert the data into a series of different SEQLEN-length subsequences.\n",
    "for i in range(0, len(text) - SEQLEN, STEP):\n",
    "    input_chars.append(text[i: i + SEQLEN])\n",
    "    label_chars.append(text[i + SEQLEN])\n",
    "input_chars[:10],label_chars[:10],len(input_chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((161793, 10, 59), (161793, 59))"
      ]
     },
     "execution_count": 344,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compute the one-hot encoding of the input sequences X and the next\n",
    "# character (the label) y\n",
    "X = np.zeros((len(input_chars), SEQLEN, nb_chars), dtype=np.bool)\n",
    "y = np.zeros((len(input_chars), nb_chars), dtype=np.bool)\n",
    "for i, input_char in enumerate(input_chars):\n",
    "    for j, ch in enumerate(input_char):\n",
    "        X[i, j, char2index[ch]] = 1\n",
    "    y[i, char2index[label_chars[i]]] = 1\n",
    "X.shape,y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Set up a bunch of metaparameters for the network and training regime.\n",
    "BATCH_SIZE, HIDDEN_SIZE = 128, 128\n",
    "NUM_ITERATIONS = 25\n",
    "NUM_EPOCHS_PER_ITERATION = 1\n",
    "NUM_PREDS_PER_EPOCH = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create a super simple recurrent neural network. There is one recurrent\n",
    "# layer that produces an embedding of size HIDDEN_SIZE from the one-hot\n",
    "# encoded input layer. This is followed by a Dense fully-connected layer\n",
    "# across the set of possible next characters, which is converted to a\n",
    "# probability score via a standard softmax activation with a multi-class\n",
    "# cross-entropy loss function linking the prediction to the one-hot\n",
    "# encoding character label.\n",
    "model = Sequential()\n",
    "model.add(\n",
    "#     GRU(  # Note you can vary this with LSTM or SimpleRNN to try alternatives.\n",
    "#         HIDDEN_SIZE,\n",
    "#         return_sequences=False,\n",
    "#         input_shape=(SEQLEN, nb_chars),\n",
    "#         unroll=True\n",
    "#     )\n",
    "    SimpleRNN(HIDDEN_SIZE, \n",
    "              return_sequences=False,\n",
    "              input_shape=(SEQLEN, nb_chars),\n",
    "              unroll=True)\n",
    ")\n",
    "model.add(Dense(nb_chars))\n",
    "model.add(Activation(\"softmax\"))\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=\"rmsprop\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "Iteration #: 0\n",
      "Epoch 1/1\n",
      "161793/161793 [==============================] - 11s 68us/step - loss: 1.7508\n",
      "Generating from seed: k half the\n",
      " round the caterus the was so done the rabbit the mare the rabbit the mare the rabbit the mare the r\n",
      "\n",
      "Wall time: 11.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Execute a series of training and demonstration iterations.\n",
    "for iteration in range(NUM_ITERATIONS):\n",
    "\n",
    "    # For each iteration, run the model fitting procedure for a number of epochs.\n",
    "    print(\"=\" * 50)\n",
    "    print(\"Iteration #: %d\" % (iteration))\n",
    "    model.fit(X, y, batch_size=BATCH_SIZE, epochs=NUM_EPOCHS_PER_ITERATION)\n",
    "\n",
    "    # Select a random example input sequence.\n",
    "    test_idx = np.random.randint(len(input_chars))\n",
    "    test_chars = input_chars[test_idx]\n",
    "\n",
    "    # For a number of prediction steps using the current version of the trained\n",
    "    # model, construct a one-hot encoding of the test input and append a prediction.\n",
    "    print(\"Generating from seed: %s\" % (test_chars))\n",
    "#     print(test_chars, end=\"\")\n",
    "    for i in range(NUM_PREDS_PER_EPOCH):\n",
    "\n",
    "        # Here is the one-hot encoding.\n",
    "        X_test = np.zeros((1, SEQLEN, nb_chars))\n",
    "        for j, ch in enumerate(test_chars):\n",
    "            X_test[0, j, char2index[ch]] = 1\n",
    "\n",
    "        # Make a prediction with the current model.\n",
    "        pred = model.predict(X_test, verbose=0)[0]\n",
    "        y_pred = index2char[np.argmax(pred)]\n",
    "\n",
    "        # Print the prediction appended to the test example.\n",
    "        print(y_pred, end=\"\")\n",
    "\n",
    "        # Increment the test example to contain the prediction as if it\n",
    "        # were the correct next letter.\n",
    "        test_chars = test_chars[1:] + y_pred\n",
    "    break\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    The sky wa -> s\n",
    "    he sky was ->  \n",
    "    e sky was  -> f\n",
    "    sky was f -> a\n",
    "    sky was fa -> l\n",
    "\n",
    "上面的算法是，输入样本是 SEQLEN个字符序列，而标签label是SEQLEN长的字符的下一个字符。\n",
    "\n",
    "这跟动手学深度学习的例子不同，上面的例子样本是每一个字符，而标签是该字符的下一个字符。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 依葫芦画瓢，改一下"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2583, 63282)"
      ]
     },
     "execution_count": 380,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.layers import Dense, Activation\n",
    "from keras.layers.recurrent import SimpleRNN, LSTM, GRU\n",
    "from keras.models import Sequential\n",
    "import random\n",
    "import zipfile\n",
    "\n",
    "with zipfile.ZipFile('data/jaychou_lyrics.txt.zip') as zin:\n",
    "    with zin.open('jaychou_lyrics.txt') as f:\n",
    "        text = f.read().decode('utf-8')\n",
    "\n",
    "chars = list(set(text))\n",
    "nb_chars = len(chars)\n",
    "nb_chars,len(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "char2index = {c: i for i, c in enumerate(chars)}\n",
    "index2char = {i: c for i, c in enumerate(chars)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['想要有直升机\\n想要和',\n",
       "  '你飞到宇宙去\\n想要和',\n",
       "  '你融化在一起\\n融化在',\n",
       "  '宇宙里\\n我每天每天每',\n",
       "  '天在想想想想著你\\n这',\n",
       "  '样的甜蜜\\n让我开始乡',\n",
       "  '相信命运\\n感谢地心引',\n",
       "  '力\\n让我碰到你\\n漂亮',\n",
       "  '的让我面红的可爱女人',\n",
       "  '\\n温柔的让我心疼的可'],\n",
       " ['要有直升机\\n想要和你',\n",
       "  '飞到宇宙去\\n想要和你',\n",
       "  '融化在一起\\n融化在宇',\n",
       "  '宙里\\n我每天每天每天',\n",
       "  '在想想想想著你\\n这样',\n",
       "  '的甜蜜\\n让我开始乡相',\n",
       "  '信命运\\n感谢地心引力',\n",
       "  '\\n让我碰到你\\n漂亮的',\n",
       "  '让我面红的可爱女人\\n',\n",
       "  '温柔的让我心疼的可爱'],\n",
       " 6329)"
      ]
     },
     "execution_count": 391,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SEQLEN, STEP = 10, 10\n",
    "# 令 STEP = SEQLEN， 保证两个input之间不重叠\n",
    "\n",
    "input_chars, label_chars = [], []\n",
    "\n",
    "for i in range(0, len(text) - 1, STEP):\n",
    "    input_chars.append(text[i: i + SEQLEN])\n",
    "    label_chars.append(text[i+1: i+1+ SEQLEN])\n",
    "input_chars[:10],label_chars[:10],len(input_chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((6329, 10, 2583), (6329, 10, 2583))"
      ]
     },
     "execution_count": 392,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = np.zeros((len(input_chars), SEQLEN, nb_chars), dtype=np.bool)\n",
    "y = np.zeros((len(input_chars), SEQLEN, nb_chars), dtype=np.bool)\n",
    "for i, input_char in enumerate(input_chars):\n",
    "    for j, ch in enumerate(input_char):\n",
    "        X[i, j, char2index[ch]] = 1\n",
    "for i, label_char in enumerate(label_chars):\n",
    "    for j, ch in enumerate(label_char):\n",
    "        y[i, j, char2index[ch]] = 1\n",
    "X.shape,y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE, HIDDEN_SIZE = 32, 256\n",
    "NUM_ITERATIONS = 25\n",
    "NUM_EPOCHS_PER_ITERATION = 1\n",
    "NUM_PREDS_PER_EPOCH = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(\n",
    "#     GRU(  # Note you can vary this with LSTM or SimpleRNN to try alternatives.\n",
    "#         HIDDEN_SIZE,\n",
    "#         return_sequences=False,\n",
    "#         input_shape=(SEQLEN, nb_chars),\n",
    "#         unroll=True\n",
    "#     )\n",
    "    SimpleRNN(HIDDEN_SIZE, \n",
    "              # 堆叠多个RNN层获得更好的结果,,相当于多个隐藏层若，为True则返回整个序列，否则仅返回输出序列的最后一个输出.\n",
    "              # True：  输入是[samples, time_steps, input_dim], 输出是[samples, time_steps, output_dim]\n",
    "              # False： 输入是[samples, time_steps, input_dim], 输出是[samples, output_dim]  \n",
    "              #                         没有time_steps了，也就是说一段时序样本，只对应一个output，见上面的例子\n",
    "              return_sequences=True, \n",
    "              input_shape=(SEQLEN, nb_chars), # input_shape=(序列的个数,序列的维度),\n",
    "              unroll=True)\n",
    ")\n",
    "model.add(Dense(nb_chars))\n",
    "model.add(Activation(\"softmax\"))\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=\"rmsprop\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "Iteration #: 0\n",
      "Epoch 1/1\n",
      "6329/6329 [==============================] - 9s 1ms/step - loss: 3.8906\n",
      "Generating from seed: 一起 努力 的感觉\n",
      "\n",
      "我要的 \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "==================================================\n",
      "Iteration #: 1\n",
      "Epoch 1/1\n",
      "6329/6329 [==============================] - 9s 1ms/step - loss: 3.6849\n",
      "Generating from seed: 疤\n",
      "血染盔甲我挥泪杀\n",
      "\n",
      "一是的\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "==================================================\n",
      "Iteration #: 2\n",
      "Epoch 1/1\n",
      "6329/6329 [==============================] - 9s 1ms/step - loss: 3.4916\n",
      "Generating from seed: 蛮好看\n",
      "黑板是吸收知\n",
      "道的气不                                              ==================================================\n",
      "Iteration #: 3\n",
      "Epoch 1/1\n",
      "6329/6329 [==============================] - 9s 1ms/step - loss: 3.3104\n",
      "Generating from seed: 留我孤单　\n",
      "在湖面成\n",
      "了  再\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "==================================================\n",
      "Iteration #: 4\n",
      "Epoch 1/1\n",
      "6329/6329 [==============================] - 9s 1ms/step - loss: 3.1396\n",
      "Generating from seed: 慢慢来 这首歌我自己\n",
      "再以\n",
      "当\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "==================================================\n",
      "Iteration #: 5\n",
      "Epoch 1/1\n",
      "6329/6329 [==============================] - 9s 1ms/step - loss: 2.9832\n",
      "Generating from seed: 脸上\n",
      "麦田已倒向战车\n",
      "经着不\n",
      "一                                             ==================================================\n",
      "Iteration #: 6\n",
      "Epoch 1/1\n",
      "2208/6329 [=========>....................] - ETA: 5s - loss: 2.8069"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[0;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1039\u001b[1;33m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[0;32m   1040\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[1;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[0;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    198\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 199\u001b[1;33m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    201\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mo\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2713\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2714\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2715\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2716\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2717\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2674\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2675\u001b[1;33m             \u001b[0mfetched\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2676\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2677\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1449\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_created_with_new_api\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1450\u001b[0m           return tf_session.TF_SessionRunCallable(\n\u001b[1;32m-> 1451\u001b[1;33m               self._session._session, self._handle, args, status, None)\n\u001b[0m\u001b[0;32m   1452\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1453\u001b[0m           return tf_session.TF_DeprecatedSessionRunCallable(\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Execute a series of training and demonstration iterations.\n",
    "for iteration in range(NUM_ITERATIONS):\n",
    "\n",
    "    # For each iteration, run the model fitting procedure for a number of epochs.\n",
    "    print(\"=\" * 50)\n",
    "    print(\"Iteration #: %d\" % (iteration))\n",
    "    model.fit(X, y, batch_size=BATCH_SIZE, epochs=NUM_EPOCHS_PER_ITERATION)\n",
    "\n",
    "    # Select a random example input sequence.\n",
    "#     test_chars = \"分开\"\n",
    "    test_idx = np.random.randint(len(input_chars))\n",
    "    test_chars = input_chars[test_idx]\n",
    "    \n",
    "    # For a number of prediction steps using the current version of the trained\n",
    "    # model, construct a one-hot encoding of the test input and append a prediction.\n",
    "    print(\"Generating from seed: %s\" % (test_chars))\n",
    "    for i in range(NUM_PREDS_PER_EPOCH):\n",
    "\n",
    "        # Here is the one-hot encoding.\n",
    "        X_test = np.zeros((1, SEQLEN, nb_chars))\n",
    "        for j, ch in enumerate(test_chars):\n",
    "            X_test[0, j, char2index[ch]] = 1\n",
    "\n",
    "        # Make a prediction with the current model.\n",
    "        pred = model.predict(X_test, verbose=0)[0]\n",
    "#         print(pred.shape)\n",
    "        y_pred = index2char[np.argmax(pred[-1,:])]\n",
    "        \n",
    "        # Print the prediction appended to the test example.\n",
    "        print(y_pred, end=\"\")\n",
    "\n",
    "        # Increment the test example to contain the prediction as if it\n",
    "        # were the correct next letter.\n",
    "        test_chars = test_chars[i+1:] + y_pred\n",
    "#     print(test_chars)\n",
    "#     output = [char2index[test_chars[0]]]\n",
    "#     for i in range(NUM_PREDS_PER_EPOCH + len(test_chars) - 1):\n",
    "#         # Here is the one-hot encoding.\n",
    "#         X_test = np.zeros((1, 1, nb_chars))\n",
    "#         ch = output[-1]\n",
    "#         X_test[0, 0, ch] = 1\n",
    "\n",
    "#         # Make a prediction with the current model.\n",
    "#         pred = model.predict(X_test, verbose=0)[0]\n",
    "#         y_pred = index2char[np.argmax(pred)]\n",
    "#         if i < len(test_chars)-1:\n",
    "#             output.append(char2index[test_chars[t + 1]])\n",
    "#         else:\n",
    "#             output.append(np.argmax(pred))\n",
    "#     print(' -'.join([index2char[i] for i in output]))\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 从0实现，网上版本"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 411,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vocaburary_size = 8000\n",
    "unknown_token = 'UNKNOWN_TOKEN'\n",
    "sentence_start_token = \"SENTENCE_START\"\n",
    "sentence_end_token = \"SENTENCE_END\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import itertools\n",
    "import nltk\n",
    "import numpy as np\n",
    "import operator\n",
    "import datetime\n",
    "import sys\n",
    "\n",
    "nltk.download()\n",
    "print(\"download ok.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "https://github.com/satojkovic/SimpleRNN/blob/master/rnn_tutorial_own.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 关于RNN的训练参数的保存"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.14.4. 训练深度学习模型\n",
    "\n",
    "在训练深度学习模型时，正向传播和反向传播之间相互依赖。下面我们仍然以本节中的样例模型分别阐述它们之间的依赖关系。\n",
    "\n",
    "一方面，正向传播的计算可能依赖于模型参数的当前值，而这些模型参数是在反向传播的梯度计算后通过优化算法迭代的。例如，计算正则化项s=(λ/2)(∥W(1)∥2F+∥W(2)∥2F)\n",
    "依赖模型参数W(1)和W(2)\n",
    "\n",
    "的当前值，而这些当前值是优化算法最近一次根据反向传播算出梯度后迭代得到的。\n",
    "\n",
    "另一方面，反向传播的梯度计算可能依赖于各变量的当前值，而这些变量的当前值是通过正向传播计算得到的。举例来说，参数梯度∂J/∂W(2)=(∂J/∂o)h⊤+λW(2)\n",
    "的计算需要依赖隐藏层变量的当前值h\n",
    "\n",
    "。这个当前值是通过从输入层到输出层的正向传播计算并存储得到的。\n",
    "\n",
    "因此，在模型参数初始化完成后，我们交替地进行正向传播和反向传播，并根据反向传播计算的梯度迭代模型参数。既然我们在反向传播中使用了正向传播中计算得到的中间变量来避免重复计算，那么这个复用也导致正向传播结束后不能立即释放中间变量内存。这也是训练要比预测占用更多内存的一个重要原因。另外需要指出的是，这些中间变量的个数大体上与网络层数线性相关，每个变量的大小跟批量大小和输入个数也是线性相关的，它们是导致较深的神经网络使用较大批量训练时更容易超内存的主要原因。\n",
    "\n",
    "3.14.5. 小结\n",
    "\n",
    "    正向传播沿着从输入层到输出层的顺序，依次计算并存储神经网络的中间变量。\n",
    "    反向传播沿着从输出层到输入层的顺序，依次计算并存储神经网络中间变量和参数的梯度。\n",
    "    在训练深度学习模型时，正向传播和反向传播相互依赖。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
