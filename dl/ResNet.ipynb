{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 残差网络（ResNet）\n",
    "\n",
    "让我们先思考一个问题：对神经网络模型添加新的层，充分训练后的模型是否只可能更有效地降低训练误差？\n",
    "\n",
    "理论上，原模型解的空间只是新模型解的空间的子空间。也就是说，如果我们能将新添加的层训练成恒等映射f(x)=x\n",
    "，新模型和原模型将同样有效。由于新模型可能得出更优的解来拟合训练数据集，因此添加层似乎更容易降低训练误差。然而在实践中，添加过多的层后训练误差往往不降反升。即使利用批量归一化带来的数值稳定性使训练深层模型更加容易，该问题仍然存在。针对这一问题，何恺明等人提出了残差网络（ResNet） [1]。它在2015年的ImageNet图像识别挑战赛夺魁，并深刻影响了后来的深度神经网络的设计。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 残差块\n",
    "\n",
    "让我们聚焦于神经网络局部。如图5.9所示，设输入为x\n",
    "。假设我们希望学出的理想映射为f(x)，从而作为图5.9上方激活函数的输入。左图虚线框中的部分需要直接拟合出该映射f(x)，而右图虚线框中的部分则需要拟合出有关恒等映射的残差映射f(x)−x。残差映射在实际中往往更容易优化。以本节开头提到的恒等映射作为我们希望学出的理想映射f(x)。我们只需将图5.9中右图虚线框内上方的加权运算（如仿射）的权重和偏差参数学成0，那么f(x)即为恒等映射。实际中，当理想映射f(x)极接近于恒等映射时，残差映射也易于捕捉恒等映射的细微波动。图5.9右图也是ResNet的基础块，即残差块（residual block）。在残差块中，输入可通过跨层的数据线路更快地向前传播。\n",
    "<img src= \"img/residual-block.svg\">\n",
    "\n",
    "ResNet沿用了VGG全3×3卷积层的设计。残差块里首先有2个有相同输出通道数的3×3卷积层。每个卷积层后接一个批量归一化层和ReLU激活函数。然后我们将输入跳过这两个卷积运算后直接加在最后的ReLU激活函数前。这样的设计要求两个卷积层的输出与输入形状一样，从而可以相加。如果想改变通道数，就需要引入一个额外的1×1卷积层来将输入变换成需要的形状后再做相加运算。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ResNet模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ResNet的前两层跟之前介绍的GoogLeNet中的一样：在输出通道数为64、步幅为2的7×7卷积层后接步幅为2的3×3的最大池化层。不同之处在于ResNet每个卷积层后增加的批量归一化层。\n",
    "\n",
    "GoogLeNet在后面接了4个由Inception块组成的模块。ResNet则使用4个由残差块组成的模块，每个模块使用若干个同样输出通道数的残差块。第一个模块的通道数同输入通道数一致。由于之前已经使用了步幅为2的最大池化层，所以无须减小高和宽。之后的每个模块在第一个残差块里将上一个模块的通道数翻倍，并将高和宽减半。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import keras\n",
    "from keras.optimizers import SGD\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, GlobalAveragePooling2D,Activation, Dense\n",
    "from keras.layers import Conv2D, MaxPool2D, BatchNormalization\n",
    "\n",
    "\n",
    "# 残差块\n",
    "def residual(input, num_channels, kernel_size=3, stride=1, padding=\"same\", use_1x1conv=False):\n",
    "    x = input \n",
    "    \n",
    "    y = Conv2D(filters=num_channels, kernel_size=(kernel_size, kernel_size), \n",
    "               strides=(stride,stride), padding = padding)(input)\n",
    "    y = BatchNormalization(axis=3)(y)\n",
    "    y = Activation('relu')(y)\n",
    "    \n",
    "    # 第二个3x3卷积\n",
    "    Conv2D(filters=num_channels, kernel_size=(kernel_size, kernel_size), \n",
    "               strides=(stride,stride), padding = padding)(y)\n",
    "    y = BatchNormalization(axis=3)(y)\n",
    "    \n",
    "    # 如果使用了 1 x 1的卷积层，那么就要让input卷积一下，改变input的通道数\n",
    "    if use_1x1conv:\n",
    "        x = Conv2D(filters=num_channels, kernel_size=(1, 1), \n",
    "               strides=(stride,stride), padding = padding)(x)\n",
    "        \n",
    "       # 相加\n",
    "    z = keras.layers.add([x, y])\n",
    "    \n",
    "    return Activation('relu')(z)\n",
    "\n",
    "# ResNet模型\n",
    "# ResNet的前两层跟之前介绍的GoogLeNet中的一样：在输出通道数为64、步幅为2的7×7卷积层后接步幅为2的3×3的最大池化层。\n",
    "# 不同之处在于ResNet每个卷积层后增加的批量归一化层。\n",
    "\n",
    "\n",
    "\n",
    "# 输入\n",
    "input_img = Input(shape=(224, 224, 1))\n",
    "# x = residual(input_img, 3)\n",
    "# 通道数增加一倍，但是高宽减半  (6, 3, 3)\n",
    "# x = residual(input_img, num_channels=6, use_1x1conv=True, stride=2)\n",
    "\n",
    "x = Conv2D(filters=64, kernel_size=(7, 7), \n",
    "               strides=(2,2), padding = \"same\")(input_img)\n",
    "x = BatchNormalization(axis=3)(x)\n",
    "x = Activation('relu')(x)\n",
    "x = MaxPool2D(pool_size=(3,3), strides=(2,2), padding='same')(x)\n",
    "\n",
    "# GoogLeNet在后面接了4个由Inception块组成的模块。\n",
    "# ResNet则使用4个由残差块组成的模块，每个模块使用若干个同样输出通道数的残差块。\n",
    "# 第一个模块的通道数同输入通道数一致。由于之前已经使用了步幅为2的最大池化层，所以无须减小高和宽。\n",
    "# 之后的每个模块在第一个残差块里将上一个模块的通道数翻倍，并将高和宽减半。\n",
    "\n",
    "# 下面我们来实现这个模块。注意，这里对第一个模块做了特别处理。\n",
    "def resnet_block(x, num_channels, num_residuals, first_block=False):\n",
    "    for i in range(num_residuals):\n",
    "        if i == 0 and not first_block:\n",
    "            x = residual(input=x, num_channels=num_channels, use_1x1conv=True, stride=2)\n",
    "        else:\n",
    "            x = residual(input=x, num_channels=num_channels)\n",
    "    return x\n",
    "\n",
    "# 接着我们为ResNet加入所有残差块。这里每个模块使用两个残差块。\n",
    "x = resnet_block(x, 64, 2, first_block=True)\n",
    "x = resnet_block(x, 128, 2)\n",
    "x = resnet_block(x, 256, 2)\n",
    "x = resnet_block(x, 512, 2)\n",
    "\n",
    "# 最后，与GoogLeNet一样，加入全局平均池化层后接上全连接层输出。\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "\n",
    "# 最后全连接成 class 类别 10\n",
    "x = Dense(10, activation='softmax')(x)\n",
    "\n",
    "\n",
    "model = Model(inputs=input_img, outputs=x)\n",
    "\n",
    "\n",
    "\n",
    "# conv5 output shape:      (1, 64, 112, 112)\n",
    "# batchnorm4 output shape:         (1, 64, 112, 112)\n",
    "# relu0 output shape:      (1, 64, 112, 112)\n",
    "# pool0 output shape:      (1, 64, 56, 56)\n",
    "# sequential1 output shape:        (1, 64, 56, 56)\n",
    "# sequential2 output shape:        (1, 128, 28, 28)\n",
    "# sequential3 output shape:        (1, 256, 14, 14)\n",
    "# sequential4 output shape:        (1, 512, 7, 7)\n",
    "# pool1 output shape:      (1, 512, 1, 1)\n",
    "# dense0 output shape:     (1, 10)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_39 (InputLayer)           (None, 224, 224, 1)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_67 (Conv2D)              (None, 112, 112, 64) 3200        input_39[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_49 (BatchNo (None, 112, 112, 64) 256         conv2d_67[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_45 (Activation)      (None, 112, 112, 64) 0           batch_normalization_49[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_7 (MaxPooling2D)  (None, 56, 56, 64)   0           activation_45[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_68 (Conv2D)              (None, 56, 56, 64)   36928       max_pooling2d_7[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_50 (BatchNo (None, 56, 56, 64)   256         conv2d_68[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_46 (Activation)      (None, 56, 56, 64)   0           batch_normalization_50[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_51 (BatchNo (None, 56, 56, 64)   256         activation_46[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_18 (Add)                    (None, 56, 56, 64)   0           max_pooling2d_7[0][0]            \n",
      "                                                                 batch_normalization_51[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_47 (Activation)      (None, 56, 56, 64)   0           add_18[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_70 (Conv2D)              (None, 56, 56, 64)   36928       activation_47[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_52 (BatchNo (None, 56, 56, 64)   256         conv2d_70[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_48 (Activation)      (None, 56, 56, 64)   0           batch_normalization_52[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_53 (BatchNo (None, 56, 56, 64)   256         activation_48[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_19 (Add)                    (None, 56, 56, 64)   0           activation_47[0][0]              \n",
      "                                                                 batch_normalization_53[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_49 (Activation)      (None, 56, 56, 64)   0           add_19[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_72 (Conv2D)              (None, 28, 28, 128)  73856       activation_49[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_54 (BatchNo (None, 28, 28, 128)  512         conv2d_72[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_50 (Activation)      (None, 28, 28, 128)  0           batch_normalization_54[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_74 (Conv2D)              (None, 28, 28, 128)  8320        activation_49[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_55 (BatchNo (None, 28, 28, 128)  512         activation_50[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_20 (Add)                    (None, 28, 28, 128)  0           conv2d_74[0][0]                  \n",
      "                                                                 batch_normalization_55[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_51 (Activation)      (None, 28, 28, 128)  0           add_20[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_75 (Conv2D)              (None, 28, 28, 128)  147584      activation_51[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_56 (BatchNo (None, 28, 28, 128)  512         conv2d_75[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_52 (Activation)      (None, 28, 28, 128)  0           batch_normalization_56[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_57 (BatchNo (None, 28, 28, 128)  512         activation_52[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_21 (Add)                    (None, 28, 28, 128)  0           activation_51[0][0]              \n",
      "                                                                 batch_normalization_57[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_53 (Activation)      (None, 28, 28, 128)  0           add_21[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_77 (Conv2D)              (None, 14, 14, 256)  295168      activation_53[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_58 (BatchNo (None, 14, 14, 256)  1024        conv2d_77[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_54 (Activation)      (None, 14, 14, 256)  0           batch_normalization_58[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_79 (Conv2D)              (None, 14, 14, 256)  33024       activation_53[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_59 (BatchNo (None, 14, 14, 256)  1024        activation_54[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_22 (Add)                    (None, 14, 14, 256)  0           conv2d_79[0][0]                  \n",
      "                                                                 batch_normalization_59[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_55 (Activation)      (None, 14, 14, 256)  0           add_22[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_80 (Conv2D)              (None, 14, 14, 256)  590080      activation_55[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_60 (BatchNo (None, 14, 14, 256)  1024        conv2d_80[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_56 (Activation)      (None, 14, 14, 256)  0           batch_normalization_60[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_61 (BatchNo (None, 14, 14, 256)  1024        activation_56[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_23 (Add)                    (None, 14, 14, 256)  0           activation_55[0][0]              \n",
      "                                                                 batch_normalization_61[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_57 (Activation)      (None, 14, 14, 256)  0           add_23[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_82 (Conv2D)              (None, 7, 7, 512)    1180160     activation_57[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_62 (BatchNo (None, 7, 7, 512)    2048        conv2d_82[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_58 (Activation)      (None, 7, 7, 512)    0           batch_normalization_62[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_84 (Conv2D)              (None, 7, 7, 512)    131584      activation_57[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_63 (BatchNo (None, 7, 7, 512)    2048        activation_58[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_24 (Add)                    (None, 7, 7, 512)    0           conv2d_84[0][0]                  \n",
      "                                                                 batch_normalization_63[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_59 (Activation)      (None, 7, 7, 512)    0           add_24[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_85 (Conv2D)              (None, 7, 7, 512)    2359808     activation_59[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_64 (BatchNo (None, 7, 7, 512)    2048        conv2d_85[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_60 (Activation)      (None, 7, 7, 512)    0           batch_normalization_64[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_65 (BatchNo (None, 7, 7, 512)    2048        activation_60[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_25 (Add)                    (None, 7, 7, 512)    0           activation_59[0][0]              \n",
      "                                                                 batch_normalization_65[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_61 (Activation)      (None, 7, 7, 512)    0           add_25[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling2d_2 (Glo (None, 512)          0           activation_61[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 10)           5130        global_average_pooling2d_2[0][0] \n",
      "==================================================================================================\n",
      "Total params: 4,917,386\n",
      "Trainable params: 4,909,578\n",
      "Non-trainable params: 7,808\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 小结\n",
    "\n",
    "    残差块通过跨层的数据通道从而能够训练出有效的深度神经网络。\n",
    "    ResNet深刻影响了后来的深度神经网络的设计。\n",
    "\n",
    "#### 思考\n",
    "\n",
    "    参考ResNet论文的表1来实现不同版本的ResNet [1]。\n",
    "    对于比较深的网络， ResNet论文中介绍了一个“瓶颈”架构来降低模型复杂度。尝试实现它 [1]。\n",
    "    在ResNet的后续版本里，作者将残差块里的“卷积、批量归一化和激活”结构改成了“批量归一化、激活和卷积”，实现这个改进（[2]，图1）。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
