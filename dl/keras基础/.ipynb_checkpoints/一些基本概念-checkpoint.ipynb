{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 张量"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "规模最小的张量是0阶张量，即标量，也就是一个数。\n",
    "\n",
    "    比如 1\n",
    "\n",
    "当我们把一些数有序的排列起来，就形成了1阶张量，也就是一个向量。 \n",
    "\n",
    "    比如 [1,2]\n",
    "\n",
    "如果我们继续把一组向量有序的排列起来，就形成了2阶张量，也就是一个矩阵。\n",
    "        \n",
    "    比如 [[1,2],[3,4]]\n",
    "\n",
    "把矩阵摞起来，就是3阶张量，我们可以称为一个立方体，具有3个颜色通道的彩色图片就是一个这样的立方体。 \n",
    "\n",
    "    比如：[[[1,2],[3,4]], [[5,6],[7,8]]]\n",
    "\n",
    "把立方体摞起来，好吧这次我们真的没有给它起别名了，就叫4阶张量了，不要去试图想像4阶张量是什么样子，它就是个数学上的概念。\n",
    "\n",
    "张量的阶数有时候也称为维度，或者轴，轴这个词翻译自英文axis。譬如一个矩阵[[1,2],[3,4]]，是一个2阶张量，有两个维度或轴，沿着第0个轴（为了与python的计数方式一致，本文档维度和轴从0算起）你看到的是[1,2]，[3,4]两个向量，沿着第1个轴你看到的是[1,3]，[2,4]两个向量。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### data_format\n",
    "\n",
    "在TensorFlow中，100张RGB三通道的16×32（高为16宽为32）彩色图的表达形式是（100,16,32,3）。把通道3放最后面。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### batch\n",
    "\n",
    "这个概念与Keras无关，老实讲不应该出现在这里的，但是因为它频繁出现，而且不了解这个技术的话看函数说明会很头痛，这里还是简单说一下。\n",
    "\n",
    "深度学习的优化算法，说白了就是梯度下降。每次的参数更新有两种方式。\n",
    "\n",
    "第一种，遍历全部数据集算一次损失函数，然后算函数对各个参数的梯度，更新梯度。这种方法每更新一次参数都要把数据集里的所有样本都看一遍，计算量开销大，计算速度慢，不支持在线学习，这称为Batch gradient descent，批梯度下降。\n",
    "\n",
    "另一种，每看一个数据就算一下损失函数，然后求梯度更新参数，这个称为随机梯度下降，stochastic gradient descent。这个方法速度比较快，但是收敛性能不太好，可能在最优点附近晃来晃去，hit不到最优点。两次参数的更新也有可能互相抵消掉，造成目标函数震荡的比较剧烈。\n",
    "\n",
    "为了克服两种方法的缺点，现在一般采用的是一种折中手段，mini-batch gradient decent，小批的梯度下降，这种方法把数据分为若干个批，按批来更新参数，这样，一个批中的一组数据共同决定了本次梯度的方向，下降起来就不容易跑偏，减少了随机性。另一方面因为批的样本数与整个数据集相比小了很多，计算量也不是很大。\n",
    "\n",
    "基本上现在的梯度下降都是基于mini-batch的，所以Keras的模块中经常会出现batch_size，就是指这个。\n",
    "\n",
    "顺便说一句，Keras中用的优化器SGD是stochastic gradient descent的缩写，但不代表是一个样本就更新一回，还是基于mini-batch的。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### epochs\n",
    "\n",
    "简单说，epochs指的就是训练过程中数据将被“看”多少次。在整个数据集上的一轮迭代。因此一个epoch的含义是模型完整的看了一遍数据集。 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 保存和读取模型\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "\n",
    "model.save('my_model.h5')  # creates a HDF5 file 'my_model.h5'\n",
    "del model  # deletes the existing model\n",
    "\n",
    "# returns a compiled model\n",
    "# identical to the previous one\n",
    "model = load_model('my_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 如果你只是希望保存模型的结构，而不包含其权重或配置信息，可以使用：\n",
    "# save as JSON\n",
    "json_string = model.to_json()\n",
    "\n",
    "# save as YAML\n",
    "yaml_string = model.to_yaml()\n",
    "\n",
    "\n",
    "# 读取模型\n",
    "# model reconstruction from JSON:\n",
    "from keras.models import model_from_json\n",
    "model = model_from_json(json_string)\n",
    "\n",
    "# model reconstruction from YAML\n",
    "model = model_from_yaml(yaml_string)\n",
    "\n",
    "\n",
    "# 保持和读取权重\n",
    "\n",
    "model.save_weights('my_model_weights.h5')\n",
    "model.load_weights('my_model_weights.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
