{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 目标检测\n",
    "\n",
    "在图像分类任务里，我们假设图像里只有一个主体目标，并关注如何识别该目标的类别。然而，很多时候图像里有多个我们感兴趣的目标，我们不仅想知道它们的类别，还想得到它们在图像中的具体位置。在计算机视觉里，我们将这类任务称为目标检测（object detection）或物体检测。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 边界框\n",
    "\n",
    "在目标检测里，我们通常使用边界框（bounding box）来描述目标位置。边界框是一个矩形框，可以由矩形左上角的x\n",
    "和y轴坐标与右下角的x和y轴坐标确定。我们根据上面的图的坐标信息来定义图中狗和猫的边界框。图中的坐标原点在图像的左上角，原点往右和往下分别为x轴和y轴的正方向。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们可以在图中将边界框画出来，以检查其是否准确。画之前，我们定义一个辅助函数bbox_to_rect。它将边界框表示成matplotlib的边界框格式。\n",
    "\n",
    "我们将边界框加载在图像上，可以看到目标的主要轮廓基本在框内。\n",
    "\n",
    "在目标检测里不仅需要找出图像里面所有感兴趣的目标，而且要知道它们的位置。位置一般由矩形边界框来表示。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 锚框\n",
    "它以每个像素为中心生成多个大小和宽高比（aspect ratio）不同的边界框。这些边界框被称为锚框（anchor box）\n",
    "\n",
    "假设输入图像高为 h，宽为 w。我们分别以图像的每个像素为中心生成不同形状的锚框。设大小为 s∈ (0,1] 且宽高比为 r > 0，锚框的宽和高分别为 $ws\\sqrt{r}$ 和 $hs/\\sqrt{r}$。当中心位置给定时，已知宽和高的锚框是确定的。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下面我们分别设定好一组大小s1,s2,...sn和一组宽高比 r1,...,rm。如果以每个像素为中心时使用所有的大小与宽高比的组合，输入图像将一共得到w * h * n * m个锚框。虽然这些锚框可能覆盖了所有的真实边界框，但计算复杂度容易过高。因此，我们通常只对包含s1或r1的大小与宽高比的组合感兴趣，即\n",
    "\n",
    "    (a1,r1),(s1,r2),...,(s1,rm),(s2,r1),(s3,r1)(s4,r1),...,(sn,r1)\n",
    "也就是说，以相同像素为中心的锚框数量为m + n -1。对于整个输入图像，我们将一共生成wh(m + n -1)个锚框。\n",
    "\n",
    "\n",
    "#### 交并比\n",
    "\n",
    "我们刚刚提到某个锚框较好地覆盖了图像中的狗。如果该目标的真实边界框已知，这里的“较好”该如何量化呢？一种直观的方法是衡量锚框和真实边界框之间的相似度。我们知道，Jaccard系数（Jaccard index）可以衡量两个集合的相似度。给定集合$\\mathcal{A}$和$\\mathcal{B}$，它们的Jaccard系数即二者交集大小除以二者并集大小：\n",
    "\n",
    "$$J(\\mathcal{A},\\mathcal{B}) = \\frac{\\left|\\mathcal{A} \\cap \\mathcal{B}\\right|}{\\left| \\mathcal{A} \\cup \\mathcal{B}\\right|}.$$\n",
    "\n",
    "\n",
    "实际上，我们可以把边界框内的像素区域看成是像素的集合。如此一来，我们可以用两个边界框的像素集合的Jaccard系数衡量这两个边界框的相似度。当衡量两个边界框的相似度时，我们通常将Jaccard系数称为交并比（intersection over union，IoU），即两个边界框相交面积与相并面积之比，如图9.2所示。交并比的取值范围在0和1之间：0表示两个边界框无重合像素，1表示两个边界框相等。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 标注训练集的锚框\n",
    "在训练集中，我们将每个锚框视为一个训练样本。为了训练目标检测模型，我们需要为每个锚框标注两类标签：一是锚框所含目标的类别，简称类别；二是真实边界框相对锚框的偏移量，简称偏移量（offset）。在目标检测时，我们首先生成多个锚框，然后为每个锚框预测类别以及偏移量，接着根据预测的偏移量调整锚框位置从而得到预测边界框，最后筛选需要输出的预测边界框。\n",
    "\n",
    "我们知道，在目标检测的训练集中，每个图像已标注了真实边界框的位置以及所含目标的类别。在生成锚框之后，我们主要依据与锚框相似的真实边界框的位置和类别信息为锚框标注。那么，该如何为锚框分配与其相似的真实边界框呢？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. 什么是非极大值抑制\n",
    "非极大值抑制，简称为NMS算法，英文为Non-Maximum Suppression。其思想是搜素局部最大值，抑制极大值。NMS算法在不同应用中的具体实现不太一样，但思想是一样的。非极大值抑制，在计算机视觉任务中得到了广泛的应用，例如边缘检测、人脸检测、目标检测（DPM，YOLO，SSD，Faster R-CNN）等。\n",
    "\n",
    "#### 2. 为什么要用非极大值抑制\n",
    "\n",
    "以目标检测为例：目标检测的过程中在同一目标的位置上会产生大量的候选框，这些候选框相互之间可能会有重叠，此时我们需要利用非极大值抑制找到最佳的目标边界框，消除冗余的边界框。Demo如下图：\n",
    "<img src=\"img/3232548-b3b5aa8390bb212f.png\">\n",
    "    \n",
    "    左图是人脸检测的候选框结果，每个边界框有一个置信度得分(confidence score)，如果不使用非极大值抑制，就会有多个候选框出现。右图是使用非极大值抑制之后的结果，符合我们人脸检测的预期结果。\n",
    "    \n",
    "    \n",
    " #### 3. 如何使用非极大值抑制\n",
    "前提：目标边界框列表及其对应的置信度得分列表，设定阈值，阈值用来删除重叠较大的边界框。\n",
    "IoU：intersection-over-union，即两个边界框的交集部分除以它们的并集。\n",
    "非极大值抑制的流程如下：\n",
    "\n",
    "根据置信度得分进行排序\n",
    "选择置信度最高的比边界框添加到最终输出列表中，将其从边界框列表中删除\n",
    "计算所有边界框的面积\n",
    "计算置信度最高的边界框与其它候选框的IoU。\n",
    "删除IoU大于阈值的边界框\n",
    "重复上述过程，直至边界框列表为空。\n",
    "\n",
    "\n",
    "我们来描述一下非极大值抑制的工作原理。\n",
    "\n",
    "对于一个预测边界框B，模型会计算各个类别的预测概率。设其中最大的预测概率为p，该概率所对应的类别即B的预测类别。我们也将p称为预测边界框B的置信度。\n",
    "\n",
    "在同一图像上，我们将预测类别非背景的预测边界框按置信度从高到低排序，得到列表L。从L中选取置信度最高的预测边界框B1作为基准，将所有与B1的交并比大于某阈值的非基准预测边界框从L中移除。这里的阈值是预先设定的超参数。此时，L保留了置信度最高的预测边界框并移除了与其相似的其他预测边界框。 接下来，从L中选取置信度第二高的预测边界框B2作为基准，将所有与B2的交并比大于某阈值的非基准预测边界框从L中移除。重复这一过程，直到L中所有的预测边界框都曾作为基准。此时L中任意一对预测边界框的交并比都小于阈值。最终，输出列表L中的所有预测边界框。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 小结\n",
    "\n",
    "    以每个像素为中心，生成多个大小和宽高比不同的锚框。\n",
    "    交并比是两个边界框相交面积与相并面积之比。\n",
    "    在训练集中，为每个锚框标注两类标签：一是锚框所含目标的类别；二是真实边界框相对锚框的偏移量。\n",
    "    预测时，可以使用非极大值抑制来移除相似的预测边界框，从而令结果简洁。\n",
    "\n",
    "####  思考\n",
    "\n",
    "    改变MultiBoxPrior函数中sizes和ratios的取值，观察生成的锚框的变化。\n",
    "    构造交并比为0.5的两个边界框，观察它们的重合度。\n",
    "    按本节定义的为锚框标注偏移量的方法（常数采用默认值），验证偏移量labels[0]的输出结果。\n",
    "    修改“标注训练集的锚框”与“输出预测边界框”两小节中的变量anchors，结果有什么变化？\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 多尺度目标检测\n",
    "\n",
    "考虑到如果以每个像素点为中心，锚框数量会巨大无比，所以为了减小计算量，那么我们想办法减少锚框个数。\n",
    "\n",
    "一种简单的方法是在输入图像中均匀采样一小部分像素，并以采样的像素为中心生成锚框。此外，在不同尺度下，我们可以生成不同数量和不同大小的锚框。值得注意的是，较小目标比较大目标在图像上出现位置的可能性更多。举个简单的例子：形状为1×1、1×2和2×2的目标在形状为2×2的图像上可能出现的位置分别有4、2和1种。因此，当使用较小锚框来检测较小目标时，我们可以采样较多的区域；而当使用较大锚框来检测较大目标时，我们可以采样较少的区域。\n",
    "\n",
    "通过定义特征图的形状来确定任一图像上均匀采样的锚框中心。\n",
    "\n",
    "具体来说，当特征图的宽和高分别设为fmap_w和fmap_h时，该函数将在任一图像上均匀采样fmap_h行fmap_w列个像素，并分别以它们为中心生成大小为s（假设列表s长度为1）的不同宽高比（ratios）的锚框。\n",
    "<img src=\"img/chapter_computer-vision_multiscale-object-detection_5_0.svg\">\n",
    "\n",
    "我们将特征图的高和宽分别减半，并用更大的锚框检测更大的目标。当锚框大小设0.4时，有些锚框的区域有重合。\n",
    "<img src=\"img/chapter_computer-vision_multiscale-object-detection_7_0.svg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "尺度大小就是特征图的大小。\n",
    "\n",
    "感受野可以理解为锚框有关。\n",
    "\n",
    "小结\n",
    "\n",
    "    可以在多个尺度下生成不同数量和不同大小的锚框，从而在多个尺度下检测不同大小的目标。\n",
    "    特征图的形状能确定任一图像上均匀采样的锚框中心。\n",
    "    用输入图像在某个感受野区域内的信息来预测输入图像上与该区域相近的锚框的类别和偏移量。\n",
    "\n",
    "思考\n",
    "\n",
    "    给定一张输入图像，设特征图变量的形状为1×ci×h×w，其中ci、h和w分别为特征图的个数、高和宽。你能想到哪些将该变量变换为锚框的类别和偏移量的方法？输出的形状分别是什么？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SSD单发多框检测\n",
    "\n",
    "一种简单实用的目标检测模型。\n",
    "\n",
    "#### 模型\n",
    "\n",
    "图9.4描述了单发多框检测模型的设计。它主要由一个基础网络块和若干个多尺度特征块串联而成。其中基础网络块用来从原始图像中抽取特征，因此一般会选择常用的深度卷积神经网络。单发多框检测论文中选用了在分类层之前截断的VGG [1]，现在也常用ResNet替代。我们可以设计基础网络，使它输出的高和宽较大。这样一来，基于该特征图生成的锚框数量较多，可以用来检测尺寸较小的目标。接下来的每个多尺度特征块将上一层提供的特征图的高和宽缩小（如减半），并使特征图中每个单元在输入图像上的感受野变得更广阔。如此一来，图9.4中越靠近顶部的多尺度特征块输出的特征图越小，故而基于特征图生成的锚框也越少，加之特征图中每个单元感受野越大，因此更适合检测尺寸较大的目标。由于单发多框检测基于基础网络块和各个多尺度特征块生成不同数量和不同大小的锚框，并通过预测锚框的类别和偏移量（即预测边界框）检测不同大小的目标，因此单发多框检测是一个多尺度的目标检测模型。\n",
    "\n",
    "<img src=\"img/ssd.svg\">\n",
    "    \n",
    "    单发多框检测模型主要由一个基础网络块和若干多尺度特征块串联而成"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "目前，基于深度学习算法的一系列目标检测算法大致可以分为两大流派：\n",
    "1.两步走（two-stage）算法：先产生候选区域然后再进行CNN分类(RCNN系列)，\n",
    "2.一步走（one-stage）算法：直接对输入图像应用算法并输出类别和相应的定位(YOLO系列)\n",
    "\n",
    "SSD也是属于one-stage。SD则是直接使用神经网络（VGG）提取到的多层feature map 来解决不同大小物体的检测。算法中与引入了一个新的概念：default box，其实和Faster R-CNN中的anchors的概念很类似。\n",
    "\n",
    "#### 类别预测层\n",
    "\n",
    "\n",
    "\n",
    "#### 边界框预测层\n",
    "\n",
    "\n",
    "\n",
    "#### 连结多尺度的预测\n",
    "\n",
    "\n",
    "#### 高和宽减半块\n",
    "\n",
    "\n",
    "#### 基础网络块\n",
    "\n",
    "\n",
    "#### 完整的模型\n",
    "\n",
    "单发多框检测模型一共包含5个模块，每个模块输出的特征图既用来生成锚框，又用来预测这些锚框的类别和偏移量。第一模块为基础网络块，第二模块至第四模块为高和宽减半块，第五模块使用全局最大池化层将高和宽降到1。因此第二模块至第五模块均为图9.4中的多尺度特征块。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fast-RCNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
