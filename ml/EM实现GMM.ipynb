{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EM原理\n",
    "\n",
    "### 1. 前言\n",
    "\n",
    "概率模型有时既含有观测变量(observable variable)，又含有隐变量或潜在变量（latent variable），如果仅有观测变量，那么给定数据就能用极大似然估计或贝叶斯估计来估计model参数；但是当模型含有隐变量时，需要一种含有隐变量的概率模型参数估计的极大似然方法估计——EM算法\n",
    "\n",
    "#### 2. EM算法原理\n",
    "\n",
    "EM算法称为期望极大值算法（expectation maximizition algorithm，EM），是一种启发式的迭代算法。\n",
    "\n",
    "EM算法的思路是使用启发式的迭代方法，既然我们无法直接求出模型分布参数，那么我们可以先猜想隐含数据（EM算法的E步），接着基于观察数据和猜测的隐含数据一起来极大化对数似然，求解我们的模型参数（EM算法的M步)。\n",
    "\n",
    "可以通过K-Means算法来简单理解EM算法的过程。\n",
    "\n",
    "#### E步：\n",
    "\n",
    "在初始化K个中心点后，我们对所有的样本归到K个类别。\n",
    "\n",
    "#### M步：\n",
    "\n",
    "在所有的样本归类后，重新求K个类别的中心点，相当于更新了均值。\n",
    "\n",
    "#### 3. EM算法公式\n",
    "\n",
    "对于m个样本观察数据$x=(x^{(1)},x^{(2)},...x^{(m)})$中，找出样本的模型参数θ，极大化模型分布的对数似然函数如下，假设数据中有隐含变量$x=(x^{(1)},x^{(2)},...x^{(m)})$。\n",
    "$$L(\\theta) = \\sum\\limits_{i=1}^m logP(x^{(i)}|\\theta)$$\n",
    "加入隐含变量公式变为如下，注意到下式中$Q_i(z(i))$是一个分布，因此$\\sum Q_i(z(i))logP(x(i),z(i)|θ)$可以理解为$logP(x(i),z(i)|θ)$基于条件概率分布$Q_i(z(i))$的期望。\n",
    "$$Q_i(z^{(i)}) = P( z^{(i)}|x^{(i)},\\theta)$$\n",
    "$$L(\\theta) = \\sum\\limits_{i=1}^m log\\sum\\limits_{z^{(i)}}Q_i(z^{(i)})P(x^{(i)},z^{(i)}|\\theta)\\;\\;\\;s.t.\\sum\\limits_{z}Q_i(z^{(i)}) =1\\;\\;\\;\\;\\;(1)$$\n",
    "\n",
    "    注意：上面L似然函数的来历，请见另一篇EM算法原理。其实他是ELBO。\n",
    "\n",
    "根据Jensen不等式,(1)式变为(2)\n",
    "$$E [f \\left ( g(X) \\right ) ] \\ge f \\left (E[g(X)] \\right )$$\n",
    "$$L(\\theta) = \\sum\\limits_{i=1}^m log\\sum\\limits_{z^{(i)}}Q_i(z^{(i)})P(x^{(i)},z^{(i)}|\\theta)\\ge\\sum\\limits_{i=1}^m \\sum\\limits_{z^{(i)}}Q_i(z^{(i)})logP(x^{(i)},z^{(i)}|\\theta)\\;\\;\\;s.t.\\sum\\limits_{z}Q_i(z^{(i)}) =1\\;\\;\\;\\;\\;(2)$$\n",
    "\n",
    "#### 4. EM算法流程\n",
    "\n",
    "输入：观察数据$x=(x^{(1)},x^{(2)},...x^{(m)})$，联合分布$p(x,z|θ)$, 条件分布$p(z|x,θ)$, EM算法退出的阈值γ。\n",
    "\n",
    "1、随机初始化模型参数θ的初值$θ^0$。\n",
    "\n",
    "2、E步：计算联合分布的条件概率期望。\n",
    "$$Q_i(z^{(i)}) = P( z^{(i)}|x^{(i)},\\theta^{j})$$\n",
    "$$L(\\theta, \\theta^{j}) = \\sum\\limits_{i=1}^m\\sum\\limits_{z^{(i)}}Q_i(z^{(i)})log{P(x^{(i)},z^{(i)}|\\theta)}$$\n",
    "3、M步：极大化$L(θ,θ^j)$,得到$θ^{j+1}$:\n",
    "$$\\theta^{j+1} = arg \\max \\limits_{\\theta}L(\\theta, \\theta^{j})$$\n",
    "4、重复2，3两步，直到极大似然估计$L(θ,θ^j)$的变化小于γ\n",
    "\n",
    "#### 5. 总结\n",
    "\n",
    "如果我们从算法思想的角度来思考EM算法，我们可以发现我们的算法里已知的是观察数据，未知的是隐含数据和模型参数，在E步，我们所做的事情是固定模型参数的值，优化隐含数据的分布，而在M步，我们所做的事情是固定隐含数据分布，优化模型参数的值。\n",
    "\n",
    "本节介绍的EM算法是通用的EM算法框架，其实EM算法有很多实现方式，其中比较流行的一种实现方式是高斯混合模型（Gaussian Mixed Model）。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EM + GMM\n",
    "#### 1.GMM介绍\n",
    "\n",
    "高斯混合模型（Gaussian Mixed Model）指的是多个高斯分布函数的线性组合，理论上GMM可以拟合出任意类型的分布，通常用于解决同一集合下的数据包含多个不同的分布的情况。\n",
    "<img src=\"图片/GMM1.png\">\n",
    "\n",
    "2. GMM原理解析\n",
    "\n",
    "根据我们之前EM算法-原理详解，我们已经学习了EM算法的一般形式：\n",
    "$$Q_i(z^{(i)}) = P( z^{(i)}|x^{(i)},\\theta^{j})\\;\\;\\;\\;(1)$$\n",
    "$$\\sum\\limits_{z}Q_i(z^{(i)}) =1$$\n",
    "$$L(\\theta, \\theta^{j}) = \\sum\\limits_{i=1}^m\\sum\\limits_{z^{(i)}}Q_i(z^{(i)})log{P(x^{(i)},z^{(i)}|\\theta)}$$\n",
    "现在我们用高斯分布来一步一步的完成EM算法。\n",
    "\n",
    "设有随机变量X，则混合高斯模型可以用下式表示：\n",
    "$$p(\\boldsymbol{x}|\\boldsymbol{\\pi},\\boldsymbol{\\mu},\\boldsymbol{\\Sigma})=\\sum_{k=1}^K\\pi_k\\mathcal{N}(\\boldsymbol{x}|\\boldsymbol{\\mu}_k,\\boldsymbol{\\Sigma}_k)$$\n",
    "$$\\sum_{k=1}^K\\pi_k=1$$\n",
    "$$0<\\pi_k<1$$\n",
    "其中$\\mathcal{N}(\\boldsymbol{x}|\\boldsymbol{\\mu}_k, \\boldsymbol{\\Sigma}_k)$称为混合模型中的第$k$个分量（component）。可以看到$π_k$相当于每个分量$\\mathcal{N}(\\boldsymbol{x}|\\boldsymbol{\\mu}_k, \\boldsymbol{\\Sigma}_k)$的权重"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"图片/2019-07-31_173257.png\">\n",
    "<img src=\"图片/2019-07-31_173341.png\">\n",
    "<img src=\"图片/2019-07-31_173405.png\">\n",
    "#### 4. GMM算法流程\n",
    "<img src=\"图片/2019-07-31_173443.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 代码实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy.stats import multivariate_normal\n",
    "\n",
    "def log_weight_prob(x, alpha, mu, cov):\n",
    "    N = x.shape[0]\n",
    "    return np.mat(np.log(alpha) + log_prob(x, mu, cov)).reshape([N, 1])\n",
    "\n",
    "def log_prob(x, mu, cov):\n",
    "    norm = multivariate_normal(mean=mu, cov=cov)\n",
    "    return norm.logpdf(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy.special import logsumexp\n",
    "\n",
    "\n",
    "class GMM(object):\n",
    "    def __init__(self, k, tol = 1e-3, reg_covar = 1e-7):\n",
    "        self.K = k\n",
    "        self.tol = tol\n",
    "        self.reg_covar=reg_covar\n",
    "        self.times = 100\n",
    "        self.loglike = 0\n",
    "\n",
    "\n",
    "    def fit(self, trainMat):\n",
    "        self.X = trainMat\n",
    "        self.N, self.D = trainMat.shape\n",
    "        self.GMM_EM()\n",
    "\n",
    "    # gmm入口\n",
    "    def GMM_EM(self):\n",
    "        self.scale_data()\n",
    "        self.init_params()\n",
    "        for i in range(self.times):\n",
    "            log_prob_norm, self.gamma = self.e_step(self.X)\n",
    "            self.mu, self.cov, self.alpha = self.m_step()\n",
    "            newloglike = self.loglikelihood(log_prob_norm)\n",
    "            # print(newloglike)\n",
    "            if abs(newloglike - self.loglike) < self.tol:\n",
    "                break\n",
    "            self.loglike = newloglike\n",
    "\n",
    "\n",
    "    #预测类别\n",
    "    def predict(self, testMat):\n",
    "        log_prob_norm, gamma = self.e_step(testMat)\n",
    "        category = gamma.argmax(axis=1).flatten().tolist()[0]\n",
    "        return np.array(category)\n",
    "\n",
    "\n",
    "    #e步，估计gamma\n",
    "    def e_step(self, data):\n",
    "        gamma_log_prob = np.mat(np.zeros((self.N, self.K)))\n",
    "\n",
    "        for k in range(self.K):\n",
    "            gamma_log_prob[:, k] = log_weight_prob(data, self.alpha[k], self.mu[k], self.cov[k])\n",
    "\n",
    "        log_prob_norm = logsumexp(gamma_log_prob, axis=1)\n",
    "        log_gamma = gamma_log_prob - log_prob_norm[:, np.newaxis]\n",
    "        return log_prob_norm, np.exp(log_gamma)\n",
    "\n",
    "\n",
    "    #m步，最大化loglikelihood\n",
    "    def m_step(self):\n",
    "        newmu = np.zeros([self.K, self.D])\n",
    "        newcov = []\n",
    "        newalpha = np.zeros(self.K)\n",
    "        for k in range(self.K):\n",
    "            Nk = np.sum(self.gamma[:, k])\n",
    "            newmu[k, :] = np.dot(self.gamma[:, k].T, self.X) / Nk\n",
    "            cov_k = self.compute_cov(k, Nk)\n",
    "            newcov.append(cov_k)\n",
    "            newalpha[k] = Nk / self.N\n",
    "\n",
    "        newcov = np.array(newcov)\n",
    "        return newmu, newcov, newalpha\n",
    "\n",
    "\n",
    "    #计算cov，防止非正定矩阵reg_covar\n",
    "    def compute_cov(self, k, Nk):\n",
    "        diff = np.mat(self.X - self.mu[k])\n",
    "        cov = np.array(diff.T * np.multiply(diff, self.gamma[:,k]) / Nk)\n",
    "        cov.flat[::self.D + 1] += self.reg_covar\n",
    "        return cov\n",
    "\n",
    "\n",
    "    #数据预处理\n",
    "    def scale_data(self):\n",
    "        for d in range(self.D):\n",
    "            max_ = self.X[:, d].max()\n",
    "            min_ = self.X[:, d].min()\n",
    "            self.X[:, d] = (self.X[:, d] - min_) / (max_ - min_)\n",
    "        self.xj_mean = np.mean(self.X, axis=0)\n",
    "        self.xj_s = np.sqrt(np.var(self.X, axis=0))\n",
    "\n",
    "\n",
    "    #初始化参数\n",
    "    def init_params(self):\n",
    "        self.mu = np.random.rand(self.K, self.D)\n",
    "        self.cov = np.array([np.eye(self.D)] * self.K) * 0.1\n",
    "        self.alpha = np.array([1.0 / self.K] * self.K)\n",
    "\n",
    "\n",
    "    #log近似算法，可以防止underflow，overflow\n",
    "    def loglikelihood(self, log_prob_norm):\n",
    "        return np.sum(log_prob_norm)\n",
    "\n",
    "\n",
    "    # def loglikelihood(self):\n",
    "    #     P = np.zeros([self.N, self.K])\n",
    "    #     for k in range(self.K):\n",
    "    #         P[:,k] = prob(self.X, self.mu[k], self.cov[k])\n",
    "    #\n",
    "    #     return np.sum(np.log(P.dot(self.alpha)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "my gmm k = 3, alpha = [0.55623, 0.25711, 0.18667], maxloglike = 248.4644821168552\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'BayesianGaussianMixture' object has no attribute 'fit_predict'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m<timed exec>\u001b[0m in \u001b[0;36mcheckResult\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'BayesianGaussianMixture' object has no attribute 'fit_predict'"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from sklearn import mixture\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def checkResult():\n",
    "    X = np.loadtxt(\"./data/amix1-est.dat\")\n",
    "    searchK = 4\n",
    "    epoch = 5\n",
    "    maxLogLikelihood = 0\n",
    "    maxResult = None\n",
    "    maxK = 0\n",
    "    alpha = None\n",
    "    for i in range(2, searchK):\n",
    "        k = i\n",
    "        for j in range(epoch):\n",
    "            model1 = GMM(k)\n",
    "            model1.fit(X)\n",
    "            if model1.loglike > maxLogLikelihood:\n",
    "                maxLogLikelihood = model1.loglike\n",
    "                maxResult = model1.predict(X)\n",
    "                maxK = k\n",
    "                alpha = model1.alpha\n",
    "\n",
    "    alpha, maxResult = changeLabel(alpha, maxResult)\n",
    "    print(\"my gmm k = %s, alpha = %s, maxloglike = %s\"%(maxK,[round(a, 5) for a in alpha],maxLogLikelihood))\n",
    "\n",
    "    model2 = mixture.BayesianGaussianMixture(n_components=maxK,covariance_type='full')\n",
    "    result2 = model2.fit_predict(X)\n",
    "    alpha2, result2 = changeLabel(model2.weights_.tolist(), result2)\n",
    "\n",
    "    result = np.sum(maxResult==result2)\n",
    "    percent = np.mean(maxResult==result2)\n",
    "    print(\"sklearn gmm k = %s, alpha = %s, maxloglike = %s\"%(maxK,[round(a, 5) for a in alpha2],model2.lower_bound_))\n",
    "\n",
    "    print(\"succ = %s/%s\"%(result, len(result2)))\n",
    "    print(\"succ = %s\"%(percent))\n",
    "\n",
    "    print(maxResult[:100])\n",
    "    print(result2[:100])\n",
    "\n",
    "\n",
    "def changeLabel(alpha, predict):\n",
    "    alphaSorted = sorted(alpha, reverse=True)\n",
    "    labelOld = []\n",
    "    for i in predict:\n",
    "        if i not in labelOld:\n",
    "            labelOld.append(i)\n",
    "        if len(labelOld) == len(alpha):\n",
    "            break\n",
    "    labelNew = sorted(labelOld)\n",
    "    for i, old in enumerate(labelOld):\n",
    "        predict[predict == old] = labelNew[i] + 100\n",
    "    return alphaSorted, predict - 100\n",
    "\n",
    "checkResult()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
