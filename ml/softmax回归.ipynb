{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 指数分布族"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./图片/指数分布族.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<img src=\"图片/1621753507.jpg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "k个类别，m个样本。\n",
    "\n",
    "怎么求这个L(θ)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<img src= \"图片/1453236547.jpg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "l(x,y^) 是损失函数，y^表示预测概率。y=j表示真值"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### softmax回归推导\n",
    "\n",
    "当我们对分类的Loss进行改进的时候，我们要通过梯度下降，每次优化一个step大小的梯度，这个时候我们就要求Loss对每个权重矩阵的偏导，然后应用链式法则。那么这个过程的第一步，就是对softmax求导传回去，不用着急，我后面会举例子非常详细的说明。在这个过程中，你会发现用了softmax函数之后，梯度求导过程非常非常方便！\n",
    "\n",
    "<img src=\"图片/softmax1.jpg\">\n",
    "\n",
    "我们能得到下面公式：\n",
    "    \n",
    "    z4 = w41*o1+w42*o2+w43*o3\n",
    "    z5 = w51*o1+w52*o2+w53*o3\n",
    "    z6 = w61*o1+w62*o2+w63*o3\n",
    "    \n",
    "z4,z5,z6分别代表结点4,5,6的输出，01,02,03代表是结点1,2,3往后传的输入.那么我们可以经过softmax函数得到\n",
    "\n",
    "<img src=\"图片/softmax2.png\">\n",
    "<img src=\"图片/softmax3.png\">\n",
    "<img src=\"图片/softmax4.png\">\n",
    "<img src=\"图片/softmax5.png\">\n",
    "<img src=\"图片/softmax6.jpg\">\n",
    "<img src=\"图片/softmax7.png\">\n",
    "<img src=\"图片/softmax8.jpg\">\n",
    "<img src=\"图片/softmax9.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 说明\n",
    "\n",
    "1、为什么要分两种情况？\n",
    "    \n",
    "    很明显：每一个参数，以$w_41$为例，只有对节点4求偏导时，才会有$w_41$，而对节点5、6求偏导根本与$w_41$无关。\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### softmax回归与交叉熵\n",
    "\n",
    "<img src=\"图片/交叉熵1.png\">\n",
    "<img src=\"图片/交叉熵2.png\">\n",
    "<img src=\"图片/交叉熵3.png\">\n",
    "<img src=\"图片/交叉熵4.png\">\n",
    "<img src=\"图片/交叉熵5.png\">\n",
    "<img src=\"图片/交叉熵6.png\">\n",
    "<img src=\"图片/交叉熵7.png\">\n",
    "<img src=\"图片/交叉熵8.png\">\n",
    "<img src=\"图片/交叉熵9.png\">\n",
    "<img src=\"图片/交叉熵10.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 实际上softmax回归是逻辑回归推广到多分类\n",
    "# 从逻辑回归我们知道，LR其实是用回归解决分类问题，里面的关键是有个线性回归到概率的一个映射函数。\n",
    "# 1/ (1 + e^θ*Xb),  其中 y_ =θ^T*Xb 对吧，这个y_是我们线性回归求出的预测值。\n",
    "\n",
    "\n",
    "# 怎么用逻辑回归推广到多分类呢?\n",
    "# 首先我们的线性回归的部分，也就是预测值y_这一块是不变的，对吧， y_ = θ^T*Xb \n",
    "# 对于我们的样本数据（x1,y1）, (x1,y2), ...，（xn,yn）  \n",
    "# y 有k个分类，总共有n个样本。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
