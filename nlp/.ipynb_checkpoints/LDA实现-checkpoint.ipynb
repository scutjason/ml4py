{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gibbs 采样原版"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.io as sio\n",
    "import sys\n",
    "import logging\n",
    "import time\n",
    "\n",
    "rng = np.random.RandomState(seed=666)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 计算对数似然\n",
    "def loglikelihood(doc_topic_matrix, topic_word_matrix, doc_word_matrix, alpha, beta):\n",
    "    word_list, doc_list = doc_word_matrix.nonzero()\n",
    "    theta = np.asarray(doc_topic_matrix, dtype=np.float)\n",
    "    phi = np.asarray(topic_word_matrix.T, dtype=np.float)\n",
    "    theta = theta + alpha\n",
    "    theta_norm = np.sum(theta, axis=1)[:, np.newaxis]\n",
    "    theta = theta / theta_norm\n",
    "    phi = phi + beta\n",
    "    phi_norm = np.sum(phi, axis=1)[:, np.newaxis]\n",
    "    phi = phi / phi_norm\n",
    "\n",
    "    ll = []\n",
    "    for word_indx, doc_indx in zip(word_list, doc_list):\n",
    "        product = np.dot(theta[doc_indx, :], phi[:, word_indx])\n",
    "        ll.append(product)\n",
    "\n",
    "    logll = np.sum(np.log(ll))\n",
    "\n",
    "    return logll, theta, phi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def baseline_lda(doc_word_matrix, K, iters, alpha, beta):\n",
    "    num_voc, num_doc = doc_word_matrix.shape\n",
    "    doc_topic_matrix = np.zeros([num_doc, K], dtype=np.int)\n",
    "    topic_word_matrix = np.zeros([num_voc, K], dtype=np.int)\n",
    "    topic_sum = np.zeros(K)\n",
    "\n",
    "    word_list, doc_list = doc_word_matrix.nonzero()\n",
    "    doc_word_assigned_topic = np.zeros(doc_word_matrix.shape, dtype=np.int)\n",
    "    for word_indx, doc_indx in zip(word_list, doc_list):\n",
    "        assigned_topic = rng.randint(K)\n",
    "        doc_word_assigned_topic[word_indx, doc_indx] = assigned_topic\n",
    "        doc_topic_matrix[doc_indx, assigned_topic] += 1\n",
    "        topic_word_matrix[word_indx, assigned_topic] += 1\n",
    "        topic_sum[assigned_topic] += 1\n",
    "\n",
    "    st = time.time()\n",
    "    logger.info('start training with baseline_lda k=%d max_iter=%d' % (K, iters))\n",
    "\n",
    "    for i in range(iters):\n",
    "        for word_indx, doc_indx in zip(word_list, doc_list):\n",
    "            assigned_topic = doc_word_assigned_topic[word_indx, doc_indx]\n",
    "\n",
    "            doc_topic_matrix[doc_indx, assigned_topic] -= 1\n",
    "            topic_word_matrix[word_indx, assigned_topic] -= 1\n",
    "            topic_sum[assigned_topic] -= 1\n",
    "\n",
    "            conditional_probability = (topic_word_matrix[word_indx] + beta) / (topic_sum + num_voc * beta) * (doc_topic_matrix[doc_indx] + alpha)\n",
    "            conditional_probability = conditional_probability / sum(conditional_probability)\n",
    "            new_assigned_topic = int(rng.choice(K, 1, p=conditional_probability))\n",
    "\n",
    "            doc_word_assigned_topic[word_indx, doc_indx] = new_assigned_topic\n",
    "            doc_topic_matrix[doc_indx, new_assigned_topic] += 1\n",
    "            topic_word_matrix[word_indx, new_assigned_topic] += 1\n",
    "            topic_sum[new_assigned_topic] += 1\n",
    "\n",
    "        iter_time = time.time() - st\n",
    "        st = time.time()\n",
    "\n",
    "        ll, theta, phi = loglikelihood(doc_topic_matrix, topic_word_matrix, doc_word_matrix, alpha, beta)\n",
    "        ll_time = time.time() - st\n",
    "        st = time.time()\n",
    "        logger.info('iter %d sampling_time %f loglikelihood_time %f ll %f' % (i, iter_time, ll_time, ll))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### spare lda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sparse_lda(doc_word_matrix, K, iters, alpha, beta):\n",
    "    num_voc, num_doc = doc_word_matrix.shape\n",
    "    doc_topic_matrix = np.zeros((num_doc, K), dtype=np.int)\n",
    "    topic_word_matrix = np.zeros((num_voc, K), dtype=np.int)\n",
    "    topic_sum = np.zeros(K)\n",
    "    Vbeta = num_voc * beta\n",
    "\n",
    "    # random word topic assignment initialization\n",
    "    word_list, doc_list = doc_word_matrix.nonzero()\n",
    "    doc_word_assigned_topic = np.zeros(doc_word_matrix.shape, dtype=np.int)\n",
    "    for word_indx, doc_indx in zip(word_list, doc_list):\n",
    "        assigned_topic = rng.randint(K)\n",
    "        doc_word_assigned_topic[word_indx, doc_indx] = assigned_topic\n",
    "        doc_topic_matrix[doc_indx, assigned_topic] += 1\n",
    "        topic_word_matrix[word_indx, assigned_topic] += 1\n",
    "        topic_sum[assigned_topic] += 1\n",
    "\n",
    "\n",
    "    # cache variable computed\n",
    "    ssum = alpha * beta * np.sum(1/(topic_sum + Vbeta))\n",
    "    q1 = alpha  / (topic_sum + Vbeta)\n",
    "\n",
    "    st = time.time()\n",
    "    logger.info('start training with sparse_lda k=%d max_iter=%d' % (K, iters))\n",
    "\n",
    "    for i in range(iters):\n",
    "        for doc_indx in xrange(num_doc):\n",
    "            temp = doc_topic_matrix[doc_indx] / (topic_sum + Vbeta)\n",
    "            q1 += temp\n",
    "            rsum = beta * temp.sum()\n",
    "\n",
    "            current_doc = doc_word_matrix.getcol(doc_indx)\n",
    "            has_word_indices = current_doc.nonzero()[0]\n",
    "\n",
    "            for word_indx in iter(has_word_indices):\n",
    "                assigned_topic = doc_word_assigned_topic[word_indx, doc_indx]\n",
    "                # remove chosen word-topic pair\n",
    "                doc_topic_matrix[doc_indx, assigned_topic] -= 1\n",
    "                topic_word_matrix[word_indx, assigned_topic] -= 1\n",
    "                topic_sum[assigned_topic] -= 1\n",
    "\n",
    "                # update the bucket sums\n",
    "                denominator = topic_sum[assigned_topic] + Vbeta\n",
    "                nt_d = doc_topic_matrix[doc_indx, assigned_topic]\n",
    "                ssum = ssum + alpha * beta * (1 / denominator - 1 / (denominator - 1))\n",
    "                rsum = rsum - (nt_d + 1) * beta / (denominator + 1) + (nt_d * beta) / denominator\n",
    "                q1[assigned_topic] = (alpha + nt_d) / denominator\n",
    "                p = topic_word_matrix[word_indx] * q1\n",
    "                qsum = p.sum()\n",
    "\n",
    "                total_sum = ssum + rsum + qsum\n",
    "                U = rng.rand() * total_sum\n",
    "                tmp = U\n",
    "\n",
    "                if U < ssum:\n",
    "                    for t in range(K):\n",
    "                        U -= 1 / (topic_sum[t] + Vbeta)\n",
    "                        if U <= 0:\n",
    "                            new_assigned_topic = t\n",
    "                            break\n",
    "\n",
    "                elif U < (ssum + rsum):\n",
    "                    U -= ssum\n",
    "                    U /= beta\n",
    "                    current_doc_topic = doc_topic_matrix[doc_indx]\n",
    "                    for topic_indx in range(K):\n",
    "                        U -= current_doc_topic[topic_indx] / (topic_sum[topic_indx] + Vbeta)\n",
    "                        if U <= 0:\n",
    "                            new_assigned_topic = topic_indx\n",
    "                            break\n",
    "\n",
    "                else:\n",
    "                    U -= (ssum + rsum)\n",
    "                    for topic_indx in range(K):\n",
    "                        U -= p[topic_indx]\n",
    "                        if U <= 0:\n",
    "                            new_assigned_topic = topic_indx\n",
    "                            break\n",
    "\n",
    "                nt_d = doc_topic_matrix[doc_indx, new_assigned_topic]\n",
    "                ssum = ssum + alpha * beta * (1 / (denominator + 1) - 1 / denominator)\n",
    "                rsum = rsum - nt_d * beta / denominator + (nt_d + 1) * beta / (denominator + 1)\n",
    "                q1[new_assigned_topic] = (alpha + nt_d + 1) / (denominator + 1)\n",
    "\n",
    "                doc_word_assigned_topic[word_indx, doc_indx] = new_assigned_topic\n",
    "                doc_topic_matrix[doc_indx, new_assigned_topic] += 1\n",
    "                topic_word_matrix[word_indx, new_assigned_topic] += 1\n",
    "                topic_sum[new_assigned_topic] += 1\n",
    "\n",
    "            q1 -= doc_topic_matrix[doc_indx] / (topic_sum + Vbeta)\n",
    "\n",
    "\n",
    "        iter_time = time.time() - st\n",
    "        st = time.time()\n",
    "\n",
    "        ll, theta, phi = loglikelihood(doc_topic_matrix, topic_word_matrix, doc_word_matrix, alpha, beta)\n",
    "        ll_time = time.time() - st\n",
    "        st = time.time()\n",
    "        logger.info('iter %d sampling_time %f loglikelihood_time %f ll %f' % (i, iter_time, ll_time, ll))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### spark lda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
