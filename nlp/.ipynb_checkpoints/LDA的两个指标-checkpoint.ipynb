{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### gensium"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1、困惑度 perp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这个问题在《LDA漫游指南》一书中做了很好的解答，详见第4章第4.2节\n",
    "\n",
    "topic number K：\n",
    "\n",
    "许多读者问，如何设置主题个数，其实现在没有特别好的办（HDP等较为复杂的模型可以自动确定这个参数，但是模型复杂，计算复杂），\n",
    "\n",
    "$\\color{red}{目前只有交叉验证（cross validation），通过设置不同的K值训练后验证比较求得最佳值}$\n",
    "\n",
    "我的建议是一开始不要设置太大而逐步增大实验，\n",
    "Blei在论文《Latent Dirichlet Allocation》提出过一个方法，采用设置不同的topic数量，画出topic_number-perplexity曲线；\n",
    "\n",
    "Thomas L. Grifﬁths等人在《Finding scientific topics》也提出过一个验证方法，画出topic_number-logP(w|T)曲线，\n",
    "\n",
    "然后找到曲线中的纵轴最高点便是topic数量的最佳值。有兴趣的读者可以去读读这两篇论文原文的相应部分。\n",
    "\n",
    "这个参数同时也跟文章数量有关，可以通过一个思想实验来验证：\n",
    "\n",
    "设想两个极端情况：如果仅有一篇文章做训练，则设置几百个topic不合适，\n",
    "\n",
    "如果将好几亿篇文章拿来做topic model，则仅仅设置很少topic也是不合适的。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "（1）Perplexity是什么？\n",
    "\n",
    "通常用于评价聚类算法好坏的方法有两种，其一是使用带分类标签的测试数据集，然后使用一些算法，比如Normalized Mutual Information,Variation of Information distance,来判断聚类结果与真实结果的差距，其二是使用无分类标签的测试数据集，用训练出来的模型来跑测试数据集，然后计算在测试数据集上，所有token似然值几何平均数的倒数，也就是perplexity指标，这个指标可以直观理解为用于生成测试数据集的词表大小的期望值，而这个词表中所有词汇符合平均分布[1]。其公式如下："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src= \"img/困惑度.jpg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 我们来看一篇英文文档怎么说perplexity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/困惑度1.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "大概意思是说，困惑度是用于评测语言模型中，聚类算法的一个通用方法，它用于评测一个unseen的测试集，基于训练集训练出来的模型结果M。实际上就是求在测试集W上的一个似然函数。，P(W|M)中，W可以是一篇文档、或者每一个词。对于LDA模型而言，通常W是一个word\n",
    "<img src=\"img/困惑度2.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "复杂度越高表示训练出来的模型参数（主要是P(z|d) 和 p(w|d)），对于测试集而言，会有表达出错会更多些。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-4.863109226028125\n",
      "0.03436040315132479\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import LdaModel\n",
    "from gensim.corpora import Dictionary\n",
    "import numpy as np\n",
    "\n",
    "def perplexity(ldamodel, testset, dictionary, size_dictionary, num_topics):\n",
    "    \"\"\"calculate the perplexity of a lda-model\"\"\"\n",
    "    # dictionary : {7822:'deferment', 1841:'circuitry',19202:'fabianism'...]\n",
    "    # print('the info of this ldamodel: ')\n",
    "    # print('num of testset: %s; size_dictionary: %s; num of topics: %s' % (len(testset), size_dictionary, num_topics))\n",
    "    prob_doc_sum = 0.0\n",
    "    topic_word_list = []\n",
    "    for topic_id in range(num_topics):\n",
    "        topic_word = ldamodel.show_topic(topic_id, size_dictionary)\n",
    "        dic = {}\n",
    "        for word, probability in topic_word:\n",
    "            dic[word] = probability\n",
    "        topic_word_list.append(dic)\n",
    "    doc_topics_ist = []\n",
    "    for doc in testset:\n",
    "        doc_topics_ist.append(ldamodel.get_document_topics(doc, minimum_probability=0))\n",
    "    testset_word_num = 0\n",
    "    for i in range(len(testset)):\n",
    "        prob_doc = 0.0  # the probablity of the doc\n",
    "        doc = testset[i]\n",
    "        doc_word_num = 0  # the num of words in the doc\n",
    "        for word_id, num in doc:\n",
    "            prob_word = 0.0  # the probablity of the word\n",
    "            doc_word_num += num\n",
    "            word = dictionary[word_id]\n",
    "            for topic_id in range(num_topics):\n",
    "                # cal p(w) : p(w) = sum_z{(p(z)*p(w|z))}\n",
    "                try:\n",
    "                    prob_topic = doc_topics_ist[i][topic_id][1]  # p(z_i) = p(z = i|d)\n",
    "                except Exception:\n",
    "                    prob_topic = 0.00001\n",
    "                # print(\"prob_topic\", prob_topic)\n",
    "                prob_topic_word = topic_word_list[topic_id][word]\n",
    "                prob_word += prob_topic*prob_topic_word\n",
    "            prob_doc += np.log(prob_word) # p(d) = sum(log(p(w)))\n",
    "        prob_doc_sum += prob_doc\n",
    "        testset_word_num += doc_word_num\n",
    "    prep = np.exp2(-prob_doc_sum/testset_word_num) # perplexity = exp(-sum(p(d)/sum(Nd))\n",
    "    # print(\"the perplexity of this ldamodel is : %s\" % prep)\n",
    "    return prep\n",
    "\n",
    "docs = [['human', 'interface', 'computer'],\n",
    "         ['survey', 'user', 'computer', 'system', 'response', 'time'],\n",
    "         ['eps', 'user', 'interface', 'system'],\n",
    "         ['system', 'human', 'system', 'eps'],\n",
    "         ['user', 'response', 'time'],\n",
    "         ['trees'],\n",
    "         ['graph', 'trees'],\n",
    "         ['graph', 'minors', 'trees'],\n",
    "         ['graph', 'minors', 'survey']]\n",
    "dct = Dictionary(docs)\n",
    "corpus = [dct.doc2bow(_) for _ in docs]\n",
    "c_train, c_test = corpus[:7], corpus[7:]\n",
    "\n",
    "ldamodel = LdaModel(corpus=c_train, num_topics=2, id2word=dct)\n",
    "log_perp =ldamodel.log_perplexity(c_test)\n",
    "print(log_perp)\n",
    "print(np.exp2(log_perp))\n",
    "# print(\"my perp: \", perplexity(ldamodel, c_test, dct, 9, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2、主题一致性"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-14.689200643963385\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "-7.105015580153772"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "\n",
    "texts = [['human', 'interface', 'computer'],\n",
    "         ['survey', 'user', 'computer', 'system', 'response', 'time'],\n",
    "         ['eps', 'user', 'interface', 'system'],\n",
    "         ['system', 'human', 'system', 'eps'],\n",
    "         ['user', 'response', 'time'],\n",
    "         ['trees'],\n",
    "         ['graph', 'trees'],\n",
    "         ['graph', 'minors', 'trees'],\n",
    "         ['graph', 'minors', 'survey']]\n",
    "\n",
    "dictionary = Dictionary(texts)\n",
    "corpus = [dictionary.doc2bow(text) for text in texts]\n",
    "\n",
    "# 1.1、训练模型\n",
    "goodLdaModel = LdaModel(corpus=corpus, id2word=dictionary, iterations=100, num_topics=2)\n",
    "# 1.2、通过预训练好的模型来求coh\n",
    "goodcm = CoherenceModel(model=goodLdaModel, corpus=corpus, dictionary=dictionary, coherence='u_mass')\n",
    "print(goodcm.get_coherence())\n",
    "\n",
    "# 2.1 不提供模型，直接提供topics\n",
    "topics = [['human', 'computer', 'system', 'interface'],\n",
    "          ['graph', 'minors', 'trees', 'eps']]\n",
    "\n",
    "# note that a dictionary has to be provided.\n",
    "cm = CoherenceModel(topics=topics, corpus=corpus, dictionary=dictionary, coherence='u_mass')\n",
    "cm.get_coherence()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 论文 Exploring the Space of Topic Coherence Measures\n",
    "\n",
    "根据论文描述主题一致性框架，分为以下4个维度：\n",
    "\n",
    "    词集切割 Segmentation of word subsets：将词集分成多个子词集？？\n",
    "    概率估计 Probability Estimation：子词集的概率。\n",
    "    确认度量  Confirmation Measure：根据某些预定义的标准（比如％一致性）确定质量并指定一些数字以符合条件。 例如，根据XXX标准，75％的产品质量良好。\n",
    "    聚合 Aggregation：将所有质量数字组合在一起，并为整体质量得出一个数字。其实就是合并上面的每个子词集的得分。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 一些基础概念\n",
    "\n",
    "##### PMI\n",
    "PMI指标来衡量两个事物之间的相关性（比如两个词）。其原理很简单。根据$PMI(w_i, w_j) = log\\frac{p(w_i, w_j) + ε} { p (w_i) * p(w_j) }$ ε 取＜1时效果较好，主要是为了得到有效的PMI值，因为二者不相关时，$p(w_i w_j) = 0$ ,log(0) 是负无穷大, 即两个单词共现的概率除以两个单词的频率乘积, 二者相关性越大，则就相比于越大。为什么加log，主要来自于信息论的理论，可以简单理解为，当对取log之后就将一个概率转换为了信息量（要再乘以-1将其变为正数），以2为底时可以简单理解为用多少个bits可以表示这个变量。\n",
    "\n",
    "当对取log之后就将一个概率转换为了信息量（要再乘以-1将其变为正数），以2为底时可以简单理解为用多少个bits可以表示这个变量\n",
    "\n",
    "##### UCI 一致性\n",
    "<img src=\"img/2019-08-01_175215.png\">  也叫逐点求PMI法\n",
    "    \n",
    "    比如：我们有3个语料\n",
    "    “ the game is a team sport ”,\n",
    "    “ the game is played with a ball ”, \n",
    "    “ the game demands great physical efforts ”.\n",
    "    \n",
    "     假设某个topic下的topn 词为 { game, sport, ball, team },\n",
    "     那么我们的C_uci指标计算方式为：\n",
    "<img src=\"img/2019-08-01_175557.png\">\n",
    "\n",
    "##### UMASS 一致性\n",
    "UMASS 只与改词前面的词相比，是一种非对称性的衡量方法。\n",
    "<img src=\"img/2019-08-01_175753.png\">\n",
    "\n",
    "    同样：用C_umass方法：\n",
    "<img src=\"img/2019-08-01_175938.png\">\n",
    "\n",
    "##### 除了PMI，其他相关性方法\n",
    "比如将每个词变成向量w_v，用余弦距离算相关性。 gensim里的'c_v'参数就是用余弦距离计算相关度。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 单词一致性\n",
    "\n",
    "具体步骤是：S -> M -> P -> ∑\n",
    "    \n",
    "<img src=\"img/2019-08-01_181314.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### S 步骤\n",
    "首先，将单词集t分割成一组单词子集S。\n",
    "\n",
    "       有3种S子集对： \n",
    "       \n",
    "       1对1\n",
    "       1对它前面的子集\n",
    "       1对它后面的子集\n",
    "<img src=\"img/2019-08-02_092331.png\">\n",
    "\n",
    "        另外一种S子集对的划分方式如下：\n",
    "<img src=\"img/2019-08-02_092154.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### P阶段\n",
    "\n",
    "用于求这个S子集对的概率的方法，大概有这么几种：\n",
    "\n",
    "\n",
    "$P_{bd}$：\n",
    "    \n",
    "    这个方法是直接用单个词出现的文档数  / 文档总数。不考虑单词在文档中出现的次数和词序\n",
    "    如果是算W1 和 W2这两个词的联合概率，那也是一样，两个词同时出现的文档数 / 文档总数\n",
    "    \n",
    "$P_{bs}$：\n",
    "    \n",
    "    同理，不过这里的bs表示单词出现的句子数。\n",
    "    \n",
    "$P_{sw}$：\n",
    "    \n",
    "    也差不多，只不过这里用了一个滑动窗口和判断W1和W1的共现次数。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### M阶段\n",
    "用上面描述的具体的衡量方法来判断 S（W1， W * ) ， W1是单词，W * 可以是单词也可以是子词集，通过W1 和 W * 共现的频率来判断 W * 到底跟W1 联系有多紧密。\n",
    "\n",
    "这里有两种结算概率的方法\n",
    "###### 直接法\n",
    "    具体的各种计算概率方法：\n",
    "<img src=\"img/2019-08-02_094105.png\">\n",
    "\n",
    "md, mr and ml are called difference-, ratio- and likelihood-measure。\n",
    "\n",
    "There, log-likelihood (mll) and log-ratio measure (mlr) are also defined the last is the PMI, the central element of the UCI coherence.\n",
    "\n",
    "Normalized log-ratio measure (mnlr) is the NPMI. The log-conditional-probability measure (mlc) is equivalent to the calculation used by UMass coherence\n",
    "\n",
    "The last two confirmation measures are the Jaccard and log-Jaccard measures.\n",
    "\n",
    "A small constant $ε$ is added to prevent logarithm of zero. Following [18], we set it to a small value ($ε = 10^-12$)\n",
    "\n",
    "###### 间接法\n",
    "间接法，其实就是看衡量两个词的相似性，一般通过cos、jacard距离等。 用距离去衡量相似性更有优势，怎么说？比如z和x很相似，当时他们同时出现的概率确很低，这样他们之间的联合概率就很低，这并不是正确的结果。但是呢，x和z确与W词集中的很多词都公共有关。\n",
    "\n",
    "比如说汽车品牌b1和b2，他们应该是语义相似的，但是呢？确很少共同出现，然后他们又分别于道路、速度等这些词强相关，这样通过直接法求得的概率结果必然不准确。\n",
    "\n",
    "        有下面几种相似性衡量方法\n",
    "<img src=\"img/2019-08-02_095555.png\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ∑\n",
    "\n",
    "对每一个单独的词的M相加在求平均，一般是 算数平均  arithmetic_mean。\n",
    "\n",
    "\n",
    "### 4个阶段汇总就是\n",
    "<img src=\"img/2019-08-02_102502.png\">\n",
    "<img src=\"img/2019-08-02_102549.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 问题\n",
    "1、perplexity和coherence到底哪个靠谱？？\n",
    "    \n",
    "    Model perplexity and topic coherence provide a convenient measure to judge how good a given topic model is. In my experience, topic coherence score, in particular, has been more helpful. \n",
    "    业界普遍认为coherence效果可能更好"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 彻底搞透LDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### gensim版"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from gensim.corpora import Dictionary\n",
    "from gensim.models.ldamulticore import LdaMulticore\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 1、加载停用词\n",
    "\n",
    "stopwords = open(\"stop_words.txt\", \"r\", encoding=\"gbk\").read().split(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1339, ['俺', '如', '呜', '啊', '把', '会', '焉', '地', '因', '冲'])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(stopwords),stopwords[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 1、文本预处理 tokenize\n",
    "\n",
    "def tokenize(text):\n",
    "    text = text.lower()\n",
    "    words = re.sub(\"[A-Za-z0-9.?/。,，;；、：:\"\"‘’“”'!！$%()》《()（）—_√□【】+<>-]\\n\", \" \", text).split()\n",
    "    words = [w for w in words if w not in stopwords and len(w) > 1]\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "get documents done.\n",
      "Wall time: 2min 29s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# 3、获取语料 \n",
    "\n",
    "import pymongo\n",
    "# mongodb\n",
    "host='dds-bp130b89ecd3ee24-pub.mongodb.rds.aliyuncs.com'\n",
    "port=3717\n",
    "db='Fin-Tech'\n",
    "user='xuyong'\n",
    "passwd='FinTech_xy'\n",
    "collect='doc_info_seg_text'\n",
    "\n",
    "documents=[]\n",
    "documents_name=[]\n",
    "\n",
    "# 2010-2017，共8年的财报\n",
    "client = pymongo.MongoClient(host, port)\n",
    "db = client[db]\n",
    "db.authenticate(user, passwd, mechanism='SCRAM-SHA-1')\n",
    "col = db[collect]\n",
    "for w in col.find({\n",
    "    'report_type': 1, \n",
    "    'quart_v': {'$in': [4]},\n",
    "#     'security_code': {'$in': code},\n",
    "    'report_year': {'$gte': 2016, '$lte': 2016}},\n",
    "    {'_id': 0, 'seg_text_clean': 1, 'file_name': 1}):\n",
    "        documents.append(w['seg_text_clean'])\n",
    "        documents_name.append(w['file_name'])\n",
    "print('get documents done.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3106, 70843, 21182, 12895)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(documents),len(documents[0]),len(documents[0].split(\" \")),len(tokenize(documents[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenlize done.\n",
      "Wall time: 20min 17s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# 4、分词、去停用词\n",
    "processed_docs = [tokenize(doc) for doc in documents]\n",
    "print('tokenlize done.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 41.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import copy\n",
    "# 5、生成词表\n",
    "word_count_dict = Dictionary(processed_docs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "180378"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(word_count_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "风险\n",
      "风险属性\n",
      "风险意识\n",
      "风险控制\n",
      "风险敞口\n",
      "风险管理\n",
      "风险评估\n",
      "风险因素\n",
      "风险防范\n",
      "风险点\n",
      "风险投资\n",
      "风险监控\n",
      "风险教育\n",
      "风险价值\n",
      "风险偏好\n",
      "风险准备金\n",
      "风险处置\n",
      "风险容忍度\n",
      "风险承受能力\n",
      "风险控制机制\n",
      "风险提示\n",
      "风险收益\n",
      "风险收益分析\n",
      "风险暴露\n",
      "风险暴露程度\n",
      "风险监管\n",
      "风险管理制度\n",
      "风险管理委员会\n",
      "风险管理部\n",
      "风险类别\n",
      "风险警示\n",
      "风险加权资产\n",
      "风险可控\n",
      "风险承担\n",
      "风险指标\n",
      "风险状况\n",
      "风险程度\n",
      "风险经营\n",
      "风险调整\n",
      "风险资产\n",
      "风险较高\n",
      "风险防控\n",
      "风险预警\n",
      "风险导向原则\n",
      "风险领域\n",
      "风险较大\n",
      "风险隐患\n",
      "风险报酬\n",
      "风险套利\n",
      "风险管理系统\n",
      "风险转移\n",
      "风险抵押金\n",
      "风险识别\n",
      "风险理念\n",
      "风险计量\n",
      "风险评级\n",
      "风险量化\n",
      "风险防范体系\n",
      "风险披露\n",
      "风险等级\n",
      "风险加大\n",
      "风险控制委员会\n",
      "风险性\n",
      "风险提示函\n",
      "风险联动\n",
      "风险问责\n",
      "风险问题\n",
      "风险管控体系\n",
      "风险评价\n",
      "风险清单\n",
      "风险保障\n",
      "风险规避\n",
      "风险资本\n",
      "风险最小\n",
      "风险头寸\n",
      "风险辨识\n",
      "风险分析\n",
      "风险巨大\n",
      "风险提示公告\n",
      "风险系数\n",
      "风险分担\n",
      "风险内控\n",
      "风险极低\n",
      "风险容限\n",
      "风险成本\n",
      "风险分散\n",
      "风险金\n",
      "风险警示板\n",
      "风险实质\n",
      "风险值\n",
      "风险参数\n",
      "风险控制系统\n",
      "风险业务管理\n",
      "风险事故\n",
      "风险调整资本回报率\n",
      "风险经理\n",
      "风险溢价\n",
      "风险放大\n",
      "风险自负\n",
      "风险最大\n",
      "风险投资基金\n",
      "风险咨询\n",
      "风险调整回报率\n",
      "风险要素\n",
      "风险投资公司\n",
      "风险信息化\n",
      "风险厌恶\n",
      "Wall time: 83 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "cnt = 0\n",
    "risk = []\n",
    "for key in word_count_dict.token2id.keys():\n",
    "    if key.find(\"风险\") is 0:\n",
    "        print(key)\n",
    "        risk.append(key)\n",
    "#     cnt += 1\n",
    "#     if cnt % 1000 ==0:\n",
    "#         print(cnt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "107"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(risk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 566 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# 5.1 过滤一些高频和低频次\n",
    "# no_below : int, optional\n",
    "#             Keep tokens which are contained in at least `no_below` documents.  至少要有no_below篇doc中，出现这个token\n",
    "# 一般选\n",
    "word_count_dict.filter_extremes(no_below=2, no_above=0.9, keep_n=200000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "90174"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(word_count_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 46.9 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "risk_2 = []\n",
    "for key in word_count_dict.token2id.keys():\n",
    "    if key.find(\"风险\") is 0:\n",
    "        risk_2.append(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'风险',\n",
       " '风险业务管理',\n",
       " '风险信息化',\n",
       " '风险厌恶',\n",
       " '风险参数',\n",
       " '风险咨询',\n",
       " '风险容限',\n",
       " '风险放大',\n",
       " '风险自负',\n",
       " '风险要素',\n",
       " '风险警示板',\n",
       " '风险评估',\n",
       " '风险调整回报率'}"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(risk) - set(risk_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
