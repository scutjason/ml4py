{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "狄利克雷超参在多项式分布中通常存在平滑效应。\n",
    "\n",
    "    通过降低α和β的值，减少LDA中的平滑效应，这将带来更加决定性的主题关联，Θ和Φ将变得更加稀疏。\n",
    "    Φ的稀疏性由β控制，意味着模型更喜欢给每个主题分配少一些的词项，这将影响模型的主题数。\n",
    "    Θ的稀疏由α控制，意味着模型更喜欢用少一些主题描述文档。\n",
    "    α的估计是文档在它们的（隐性）语义方面不同程度的指标，β的估计表明普遍共现词组的大小。\n",
    "    事实上，学习狄利克雷参数的最好方法将是使用（collapsed）Gibbs采样器已经可用的信息，如：主题相关的计数统计信息而不是积分掉的多项式参数Θ和Φ。\n",
    "    \n",
    "    \n",
    "    增大beta会减小单个文档中主题的数量，减少beta能够产生更多的主题[11]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LDA 原始论文"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/lda1.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/lda2.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LDA应用：\n",
    "\n",
    "1、文本分类\n",
    "\n",
    "2、协同过滤--推荐系统"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/lda4.png\">\n",
    "    \n",
    "    lda的各个参数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/lda6.png\">\n",
    "\n",
    "        β超参的影响\n",
    "        \n",
    "<img src=\"img/lda7.png\">    \n",
    "    \n",
    "        α超参影响"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 参数解释\n",
    "\n",
    "Dirichlet hyperparameters generally have a smoothing effect on multinomial parameters. \n",
    "\n",
    "Reducing this smoothing effect in LDA by lowering the values of α and β will result in more decisive topic associations, thus Θ and Φ will become sparser. \n",
    "减小α和β的值，那么Θ和Φ 矩阵会变得稀疏。\n",
    "\n",
    "\n",
    "Sparsity of Φ, controlled by β, means that the model prefers to assign few terms to each topic, which again may influence the number of topics that the model assumes to be inherent in the data. \n",
    "Φ月稀疏意味着，每个topic下的独有的word会减少，其实是好事。 \n",
    "\n",
    "\n",
    "This is related to how “similar” words need to be (that is, how often they need to co-occur across different contexts26) to find themselves assigned to the same topic. \n",
    "\n",
    "That is, for sparse topics, the model will fit better to the data if K is set higher because the model is reluctant to assign several topics to a given term. \n",
    "越稀疏的矩阵，那我们最好就是选更大的主题。\n",
    "\n",
    "\n",
    "This is one reason why in models that learn K, such as non-parametric Bayesian approaches [TJB+06], K strongly depends on the hyperparameters. \n",
    "\n",
    "Sparsity of Θ, controlled by α, means that the model prefers to characterise documents by few topics.\n",
    "\n",
    "As the relationship between hyperparameters, topic number and model behaviour is a mutual one, it can be used for synthesis of models with specific properties, as well as for analysis of features inherent in the data.\n",
    "\n",
    "Heuristically, good model quality (see next section for analysis methods) has been reported for α = 50/K and β = 0.01.\n",
    "\n",
    "On the other hand, learning α and β from the data can be used to increase model quality (w.r.t. to the objective of the estimation method), given the number of topics K. \n",
    "\n",
    "Further, hyperparameter estimates may reveal specific properties of the data set modelled.\n",
    "\n",
    "The estimate for α is an indicator of how different documents are in terms of their(latent) semantics, and the estimate for β suggests how large the groups of commonlyco-occurring words are. However, the interpretation of estimated hyperparameters is not always simple, and the influence of specific constellations of document content has not yet been thoroughly investigated. \n",
    "\n",
    "In the following, we consider estimation of α, which is analogous to that of β."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### α的影响\n",
    "\n",
    "<img src=\"img/lda8.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
