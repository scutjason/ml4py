{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "狄利克雷超参在多项式分布中通常存在平滑效应。\n",
    "\n",
    "    通过降低α和β的值，减少LDA中的平滑效应，这将带来更加决定性的主题关联，Θ和Φ将变得更加稀疏。\n",
    "    Φ的稀疏性由β控制，意味着模型更喜欢给每个主题分配少一些的词项，这将影响模型的主题数。\n",
    "    Θ的稀疏由α控制，意味着模型更喜欢用少一些主题描述文档。\n",
    "    α的估计是文档在它们的（隐性）语义方面不同程度的指标，β的估计表明普遍共现词组的大小。\n",
    "    事实上，学习狄利克雷参数的最好方法将是使用（collapsed）Gibbs采样器已经可用的信息，如：主题相关的计数统计信息而不是积分掉的多项式参数Θ和Φ。\n",
    "    \n",
    "    \n",
    "    增大beta会减小单个文档中主题的数量，减少beta能够产生更多的主题[11]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LDA 原始论文"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/lda1.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/lda2.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LDA应用：\n",
    "\n",
    "1、文本分类\n",
    "\n",
    "2、协同过滤--推荐系统"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/lda4.png\">\n",
    "    \n",
    "    lda的各个参数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/lda6.png\">\n",
    "\n",
    "        β超参的影响\n",
    "        \n",
    "<img src=\"img/lda7.png\">    \n",
    "    \n",
    "        α超参影响"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 参数解释\n",
    "\n",
    "Dirichlet hyperparameters generally have a smoothing effect on multinomial parameters. \n",
    "\n",
    "Reducing this smoothing effect in LDA by lowering the values of α and β will result in more decisive topic associations, thus Θ and Φ will become sparser. \n",
    "减小α和β的值，那么Θ和Φ 矩阵会变得稀疏。\n",
    "\n",
    "\n",
    "Sparsity of Φ, controlled by β, means that the model prefers to assign few terms to each topic, which again may influence the number of topics that the model assumes to be inherent in the data. \n",
    "Φ月稀疏意味着，每个topic下的独有的word会减少，其实是好事。 \n",
    "\n",
    "\n",
    "This is related to how “similar” words need to be (that is, how often they need to co-occur across different contexts26) to find themselves assigned to the same topic. \n",
    "\n",
    "That is, for sparse topics, the model will fit better to the data if K is set higher because the model is reluctant to assign several topics to a given term. \n",
    "越稀疏的矩阵，那我们最好就是选更大的主题。\n",
    "\n",
    "\n",
    "This is one reason why in models that learn K, such as non-parametric Bayesian approaches [TJB+06], K strongly depends on the hyperparameters. \n",
    "\n",
    "Sparsity of Θ, controlled by α, means that the model prefers to characterise documents by few topics.\n",
    "\n",
    "As the relationship between hyperparameters, topic number and model behaviour is a mutual one, it can be used for synthesis of models with specific properties, as well as for analysis of features inherent in the data.\n",
    "\n",
    "Heuristically, good model quality (see next section for analysis methods) has been reported for α = 50/K and β = 0.01.\n",
    "\n",
    "On the other hand, learning α and β from the data can be used to increase model quality (w.r.t. to the objective of the estimation method), given the number of topics K. \n",
    "\n",
    "Further, hyperparameter estimates may reveal specific properties of the data set modelled.\n",
    "\n",
    "The estimate for α is an indicator of how different documents are in terms of their(latent) semantics, and the estimate for β suggests how large the groups of commonlyco-occurring words are. However, the interpretation of estimated hyperparameters is not always simple, and the influence of specific constellations of document content has not yet been thoroughly investigated. \n",
    "\n",
    "In the following, we consider estimation of α, which is analogous to that of β."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### α的影响\n",
    "\n",
    "<img src=\"img/lda8.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "参数alpha和beta的选择\n",
    "\n",
    "alpha是一个对称的Dirichlet分布的参数，值越大意味着越平滑（更正规化）。When a is less than 1, the prior distribution is peaked, with most topics receiving low probability in any given document.a =1 represents a uniform prior, and a > 1 penalizes distributions that assign a high probability to any one topic in a specific document.\n",
    "\n",
    "\n",
    "guide1:Appropriate values for ALPHA and BETA depend on the number of topics and the number of words in vocabulary. For most applications, good results can be obtained by setting ALPHA = 50 / T and BETA = 200 / W\n",
    "\n",
    "lz注：alpha一般默认 = 50/k + 1，50/k是LDA中的通用设置，+1是根据on smoothing and inference for topic models文中的推荐值。\n",
    "\n",
    "\n",
    "guide2:I suppose it has no advantage of Dirichlet parameters greater than 1 on topic model.I always choose parameters as small as possible, e.g. ALPHA=0.01/T and so on.我认为在主题模型中Dirichlet参数大于1没有什么优势，我总是选择尽可能小的参数，例如Alpha=0.01/T等等。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 主题数目的选择\n",
    "\n",
    "有时主题数目的选择都相当随意，如果我们仅仅把主题抽取作为处理的中间环节，那么对大多数用户来说，主题数量对最终结果并无影响，也就是说，只要你抽取了足够多的主题，最终结果并无区别。\n",
    "\n",
    "然而（lz注）主题数目应该代表模型的复杂度，如果主题数目太小，模型描述数据的能力就会受限；当主题数目设置超过某个threshold时，模型已经足够来处理数据了，这里增加主题数目没用，同时还会增加模型训练的时间。\n",
    "\n",
    "lz建议使用交叉验证，使用不同的主题数目值，测试参数的sensitivity，通过最终结果的准确率来实现主题数目的选择。\n",
    "\n",
    "HDP\n",
    "\n",
    "不过即使这样，你有时候仍然需要去确定需要抽取多少主题，通过垂直狄利克莱过程的方法，它在Gensim中有所实现。\n",
    "   hdp = gensim.models.hdpmodel.HdpModel(mm,id2word)\n",
    "\n",
    "剩余流程和使用LDA一样，不过使用这种方法花费的时间更长一些。\n",
    "\n",
    "不用LDP\n",
    "\n",
    "hierarchical(分层的) Dirichlet process learn the number of topics from the data, In practice however, the inferred(推论的) topic counts and resulting topics are often not what’s expected. Theoptimal(最佳的) number of topics from the structural/syntactic(句法的) point of view isn’t necessarily optimal from  thesemantic(语义的) point of view. \n",
    "\n",
    "Thus in practice,  running LDA with a number of different topic counts, evaluating the learned topics, and deciding whether the number topics should be increased or decreased usually gives better results.\n",
    "\n",
    "[Gensim中ndarray、vector的用法及LDA的使用、主题数目的选择]\n",
    "\n",
    "后验分布的更新实践[AMC]\n",
    "\n",
    "在burn in phase结束后再在每个sample lag进行posterior distributions更新。\n",
    "\n",
    "数据量很小时(e.g., 100 reviews)我们只保留Markov chain最后的状态 (i.e., sampleLag = -1). The reason is that it avoids the topics being dominated by the most frequent words.\n",
    "\n",
    "数据量不是很小时(e.g., 1000 reviews)，我们可以设置sampleLag为20。\n",
    "@Option(name = \"-slag\", usage = \"Specify the length of interval to sample for calculating posterior distribution\")\n",
    "    public int sampleLag = -1; // Subject to change given the size of the data.When the data is very small (e.g., 100 reviews), we only retain the last Markov chain status (i.e., sampleLag = -1). The reason is that it avoids the topics being dominated by the most frequent words. When the data is not very small (e.g., 1000 reviews), we should set sampleLag as 20.\n",
    "\n",
    "[随机采样和随机模拟：吉布斯采样Gibbs Sampling 吉布斯采样的相关性和独立性得到近似的独立样本]\n",
    "\n",
    "皮皮blog\n",
    "\n",
    "\n",
    "每次运行LDA产生的主题都不同\n",
    "\n",
    "由于LDA使用随机性的训练和推理步骤。\n",
    "\n",
    "怎么产生稳定的主题？\n",
    "\n",
    "通过重置numpy.random种子一样\n",
    "SOME_FIXED_SEED = 42\n",
    "\n",
    "before training/inference:\n",
    "\n",
    "np.random.seed(SOME_FIXED_SEED)\n",
    "\n",
    "Note: trying lots of random seeds until you hit the right model (as tested on a validation set) is a pretty standard trick.\n",
    "\n",
    "[lda-model-generates-different-topics-everytime-i-train-on-the-same-corpus]\n",
    "\n",
    "[http://blog.csdn.net/pipisorry/article/details/42129099]\n",
    "\n",
    "皮皮blog\n",
    "\n",
    "\n",
    "Naming the topics命名主题\n",
    "\n",
    "1> The LDA topics are distributions over words, which naturally lends itself  as a naming scheme: just take a number (for example 5-10) of most probable words in the distribution as the topicdescriptor(描述符号). This typically works quite well.\n",
    "\n",
    "2> There are interesting approaches on how to improve topic naming, for example taking into account wordcentrality(中心) in the word network in thecorpus(语料库) etc. \n",
    "\n",
    "from:http://blog.csdn.net/pipisorry/article/details/42129099"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
